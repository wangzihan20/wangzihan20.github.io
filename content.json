{"pages":[{"title":"入门网站归纳","text":"入门各类编程语言的网站 -菜鸟教程","link":"/test1/cainiao.html"},{"title":"about","text":"eeeeqwewqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq","link":"/about/index.html"},{"title":"记录一下","text":"关于font-awesome图标使用 1234567891011font-awesome是一种把图表当成字体的工具下载：npm install font-awesome --save格式要求：&lt;i class=\"fa 从官网上找图标复制的东西\"&gt;&lt;/i&gt;例子：&lt;i class=\"fa fa-car\" &gt;&lt;/i&gt;&lt;i class=\"fa fa-area-chart\"&gt;&lt;/i&gt;&lt;i class=\"fa fa-bluetooth\"&gt;&lt;/i&gt; 运行效果： 1234567891011121314151617181920212223颜色以及字体改变：&lt;font face=\"黑体\"&gt;我是黑体字&lt;/font&gt;&lt;font face=\"微软雅黑\"&gt;我是微软雅黑&lt;/font&gt;&lt;font face=\"STCAIYUN\"&gt;我是华文彩云&lt;/font&gt;&lt;font color=#0099ff size=7 face=\"黑体\"&gt;color=#0099ff size=72 face=\"黑体\"&lt;/font&gt;&lt;font color=#00ffff size=72&gt;color=#00ffff&lt;/font&gt;&lt;font color=gray size=72&gt;color=gray&lt;/font&gt;常用颜色十六进制代码深蓝：#0099ff浅蓝：#00ffff黑色：#000000灰色：#808080银色：#c0c0c0红色：#FF0000 浅红：#FF4040 橙色：#FF8000绿色：#80DF20 浅绿：#9FFF40 青色：#00FF80 亮青：#00FFFF白色：#f0f8ff #f8f8ff 全部颜色十六进制代码对照表 运行效果： 我是黑体字 我是微软雅黑 我是华文彩云 color=#0099ff font-awesome官网 Hexoz主题更改 123456789101112131415161718192021222324252627282930313233343536373839404142菜单示例：menus: 首页: { path: /, fa: fa-fort-awesome faa-shake } 归档: { path: /archives, fa: fa-archive faa-shake, submenus: { 技术: {path: /categories/技术/, fa: fa-code }, 生活: {path: /categories/生活/, fa: fa-file-text-o }, 资源: {path: /categories/资源/, fa: fa-cloud-download }, 随想: {path: /categories/随想/, fa: fa-commenting-o }, 转载: {path: /categories/转载/, fa: fa-book } } } 清单: { path: javascript:;, fa: fa-list-ul faa-vertical, submenus: { 书单: {path: /tags/悦读/, fa: fa-th-list faa-bounce }, 番组: {path: /bangumi/, fa: fa-film faa-vertical }, 歌单: {path: /music/, fa: fa-headphones }, 图集: {path: /tags/图集/, fa: fa-photo } } } 留言板: { path: /comment/, fa: fa-pencil-square-o faa-tada } 友人帐: { path: /links/, fa: fa-link faa-shake } 赞赏: { path: /donate/, fa: fa-heart faa-pulse } 关于: { path: /, fa: fa-leaf faa-wrench , submenus: { 我？: {path: /about/, fa: fa-meetup}, 主题: {path: /theme-sakura/, fa: iconfont icon-sakura }, Lab: {path: /lab/, fa: fa-cogs }, } } 客户端: { path: /client/, fa: fa-android faa-vertical } RSS: { path: /atom.xml, fa: fa-rss faa-pulse }## menumenu:- page: home url: / icon: fa-home- page: Java基础 url: /categories/javase/ icon: - page: 博客教程 url: /categories/blog/ icon: - page: 虚拟化 url: /categories/虚拟化/ icon: 数学公式 多行公式 1234567891011$$\\begin{equation}{s.t.}=\\left\\{ \\begin{array}{ll} A·x\\leq b,\\\\ Aeq·=beq，\\\\ lb\\leq x\\leq ub。 \\end{array}\\right.\\end{equation}$$ 运行效果 ，。","link":"/test1/index.html"},{"title":"王梓涵的个人简历","text":"王梓涵 12345678912 · zs123@126.com · GitHub · My Blog 个人信息 男，2001 年，武汉，中共党员 求职意向：**岗位 教育经历 硕士，清华大学，**专业，2018.09~至今. 学士，北京大学，**专业，2014.09~2018.07. 通过了 CET6 英语等级考试，计算机四级认证，软考中级认证(网络工程师). 校外实习 ##公司，##部门，##岗位，2018.7~2018.11 项目描述：##################################################### 工作描述：##################################################### 个人项目 项目一：######################################################### 项目二：######################################################### 项目二：########### 项目三：######################################################### 职场技能 精通############################################################ 精通############################################################ 熟悉############################################################ 熟悉############################################################ 了解############################################################ 了解############################################################ 获奖情况 自我评价 感谢抽空阅读","link":"/%E4%B8%AA%E4%BA%BA%E7%AE%80%E5%8E%86/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"有关Github的介绍","text":"Github是一个面向开源的私有软件托管平台，因为只支持Git作为唯一的版本库格式进行托管，所以叫Github。它于2008年4月10日正式上线，它的开发者也是linux之父：“林纳斯·本纳第克特·托瓦兹”，作为一个分布式的版本控制系统，Github的功能除了 Git 代码仓库托管及基本的 Web 管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。目前，在 GitHub 上托管的版本数量非常之多，其中不乏知名开源项目 Ruby on Rails、jQuery、python 等。Github的仓库是他独有的特征，你大可以理解为一个无限容量且没有传输速度限制的网上云盘，但是这个云盘是可以设置公共与私密空间的，在这个开源的时代，你可以通过这个平台看到世界上许多其他大佬的程序作品，并且学习他们的编程思想，而且这很容易获得。 Github的注册与登录 1.首先来到Github的官网，或者你可以直接点击这里的超链接：Github，随后你会来到官网首页界面 国内访问Github可能会有些慢，或者你可以使用一些加速器，也可以使用国内的平台Gitee（码云），它的操作与Github一样，只是部分功能需要付费才能解锁。 2.点击右上角sign up登录，再点击create a new account创建账户 随后按照提示操作完成注册并且登录，值得注意的是:这里的注册邮箱可以是虚拟的，但是为了方便以后找回账号以及其他操作，建议使用自己的邮箱。至于Username，建议取一个具有标识特征的名称，之后登录就是使用Username和设置的Password。 3.随后他会询问你是否，以后可以通过这个邮箱给你发送一些最新的推送，选择之后做一个简单的真人验证就可以了 4.接下来一路无脑next操作你就完成了注册 Github的个人主页 登录之后你会看到这样的界面： 有关仓库的建立等会再进行介绍，点击右上角的个人头像的用户名，出现如下图： 标注 1：Edit profile，修改个人简介； 标注 2：Overview，个人主页概览； 标注 3：Repositories，仓库； 标注 4：Star，点星记录； 标注 5：Followers，粉丝； 标注 6：Following，关注的 GitHub 账号； 标注 7：个人贡献历史记录。 如上图所示: 标注 1 表示的为Edit profile，这个选项当我们修改完个人信息之后，就会自动消失； 标注 2 表示的为Overview，展示了我们账号的主要内容，包括仓库和贡献等； 标记 3 表示的为Repositories，是我们建立的仓库，包括Fork来的项目，GitHub 也会自动为我们创建一个仓库； 标注 4 表示为Star，收藏了我们的“点星”，或者说是“点赞”过的项目； 标注 7 表示的为我们最近一年来的contribution，用实心的小方格标记，小方格的颜色越深，表示我们的contribution越多。在这里，我们点击Edit profile，编辑个人简历，这个简历以后甚至可以作为找工作的招牌之一…… Github仓库建立 上个环节中，我们介绍了基本界面，其中就包括了标记3：Repositories，repo（即Repositories）是Github的核心要素——库，接下来，我们就尝试创建自己的 GitHub 仓库。 点击New创建一个新的repo。 标注 1：Repository name，仓库名称； 标注 2：Description，可选描述，也就是写不写都可以； 标注 3：Public，默认的仓库类型； 标注 4：Initialize this repository with a README，初始化仓库的信息文件，建议勾选。 如上图所示，这是创建 GitHub 仓库的核心页面，里面包含了众多信息。为了方便演示，我已经把各种所需的信息都填写完啦！接下来，点击绿色Create repository按钮即可，随后来到了仓库的界面 标注 1：Code，代码部分； 标注 2：Issues，文章部分； 标注 3：Pull request，拉取请求； 标注 4：Action，点星记录； 标注 5：Project，项目； 标注 6：Settings，设置； 其中的一些功能，因为涉及到Git操作，将在我的其他博客上介绍。 那么该怎么删除一个仓库呢？ 首先点击Settings,然后划到最底下出现Delete this repository按钮即是删除, 点击之后会让你验证，需要你重新输入一下你库的名字，这一步是保证你删除的库没有删错（可能你的库太多了之后，设置的名字有相似的） 这样输入之后，点击下方的I understand……之后，你就彻底删除一个库啦！","link":"/2022/01/03/Github/"},{"title":"Git操作与仓库创建","text":"Git简介 首先了解一个概念:版本控制,简单来说就是如果你做文案工作，每次提交之后，你的领导会让你修改，一篇稿子可能修改十几次，但是最后定稿的很可能不是最新修改的那一稿，所以就需要有个版本控制的方法，可以回溯到你所修改的任何一版，并且可以拿出来使用。 目前来说，版本控制主要分为：集中式版本控制（Centralized Version Control Systems，简称 CVCS）和分布式版本控制，（Distributed Version Control System，简称 DVCS）。 集中式版本控制类似于中央集权，多个终端与一个服务器进行交互，缺点明显，如果服务器损毁，则所有终端都不能拿到最新版本。 分布式系统（distributed system）是建立在网络之上的软件系统。正是因为软件的特性，所以分布式系统具有高度的内聚性和透明性。因此，网络和分布式系统之间的区别更多的在于高层软件（特别是操作系统），而不是硬件。 下图就是分布式版本控制系统的图解： 想更多的了解这两者的优缺点，请点击这里 CVCS的代表主要有CVS、SVN 以及Perforce等； DVCS主要有 Git、Mercurial、Bazaar 以及 Darcs 等。我们平时比较常用的就是SVN和Git。 Git是世界上目前最先进的版本控制系统，Git的开发者也是Linux操作系统的创始人：“林纳斯·本纳第克特·托瓦兹”，他开发这个Git只用了短短两周，而关于这个Git开发的故事，也有一段奇闻，想了解的请点击深入 git 必看：git 是如何被创造的？讲述 git 的诞生史、核心思想及其父：Linus Torvalds 如想了解SVN的简单使用，可以查看：SVN的介绍与使用流程。 接下就开始介绍Git的简单操作使用。以下主要对官方文档以及其他文档的总结，会使用git作为版本控制工具来完成团队协作。因此，对基本的git操作指令进行总结是十分有必要的，本文会对基本的理论与命令做归纳总结。 git的通用操作流程如下图 主要涉及到四个关键点： 工作区：本地电脑存放项目文件的地方，比如learnGitProject文件夹； 暂存区（Index/Stage）：在使用git管理项目文件的时候，其本地的项目文件会多出一个.git的文件夹，将这个.git文件夹称之为版本库。其中.git文件夹中包含了两个部分，一个是暂存区（Index或者Stage）,顾名思义就是暂时存放文件的地方，通常使用add命令将工作区的文件添加到暂存区里； 本地仓库：.git文件夹里还包括git自动创建的master分支，并且将HEAD指针指向master分支。使用commit命令可以将暂存区中的文件添加到本地仓库中； 远程仓库：不是在本地仓库中，项目代码在远程git服务器上，比如项目放在github上，就是一个远程仓库，通常使用clone命令将远程仓库拷贝到本地仓库中，开发后推送到远程仓库中即可； 因此，经过这样的分析，git命令可以分为这样的逻辑进行理解和记忆： 1.git管理配置的命令； 2.几个核心存储区的交互命令： 3.工作区与暂存区的交互； 4.暂存区与本地仓库（分支）上的交互； 5.本地仓库与远程仓库的交互。 Git下载与环境配置 Git下载安装以及环境配置请参考Git下载、安装与环境配置，这里不再赘述。 git配置命令 Git操作需要在一个文件夹下生成，安装完成之后，在桌面或者文件夹下，点击右键，出现下图： Git GUI 是图像化显示，与Git Bash功能一样 Git Bash常用此方式创建一个仓库，点击打开Git控制台 以下所有命令均在Git控制台上运行：如下图 1$表示提示输入 查询配置信息 12341.列出当前配置：git config --list;2.列出repository配置：git config --local --list;3.列出全局配置：git config --global --list;4.列出系统配置：git config --system --list; 第一次使用git，配置用户信息 121.配置用户名：git config --global user.name &quot;your name&quot;;2.配置用户邮箱：git config --global user.email &quot;youremail@github.com&quot;; 其他配置 1231.配置解决冲突时使用哪种差异分析工具，比如要使用vimdiff：git config --global merge.tool vimdiff;2.配置git命令输出为彩色的：git config --global color.ui auto;3.配置git使用的文本编辑器：git config --global core.editor vi; 工作区上的操作命令 12345新建仓库1.将工作区中的项目文件使用git进行管理，即创建一个新的本地仓库：git init；(初始化)2.从远程git仓库复制项目：git clone &lt;url&gt;，如：git clone git://github.com/wasd/example.git;、3.克隆项目时如果想定义新的项目名，可以在clone命令后指定新的项目名：git clone git://github.com/wasd/example.git mygit； 查看文件状态 12345$ git status显示工作目录的状态，不带参数执行，输出内容很详细。并且根据文件是否暂存，会预示下一步的指令操作。如果想简洁一点，那么加个--short （-s）参数：git status -s 移除文件 12345$ git rm 从git中将已跟踪的文件从工作目录、暂存区移除，注意是已跟踪的。如果该文件又是已修改的，可以使用参数 -f 强制删除。如果移除未跟踪的文件，或者只在工作目录移除，在暂存区继续保留，那么可以执行： 提交 1231.提交工作区所有文件到暂存区：git add .('.'的前面还有个空格！)2.提交工作区中指定文件到暂存区：git add &lt;file1&gt; &lt;file2&gt; ...;3.提交工作区中某个文件夹中所有文件到暂存区：git add [dir]; 暂存区上的操作命令 123456提交文件到版本库1.将暂存区中的文件提交到本地仓库中，即打上新版本：git commit -m &quot;commit_info&quot;;2.将所有已经使用git管理过的文件暂存后一并提交，跳过add到暂存区的过程：git commit -a -m &quot;commit_info&quot;;3.提交文件时，发现漏掉几个文件，或者注释写错了，可以撤销上一次提交：git commit --amend;4.修改commit信息git commit --ammend,修改，保存; 删除git远程仓库地址 12git remote -v //查看git remote rm origin 分支管理 123456789101112131.创建分支：git branch &lt;branch-name&gt;，如git branch testing；2.从当前所处的分支切换到其他分支：git checkout &lt;branch-name&gt;，如git checkout testing；3.新建并切换到新建分支上：git checkout -b &lt;branch-name&gt;;4.删除分支：git branch -d &lt;branch-name&gt;；5.将当前分支与指定分支进行合并：git merge &lt;branch-name&gt;;6.显示本地仓库的所有分支：git branch;7.查看各个分支最后一个提交对象的信息：git branch -v;8.查看哪些分支已经合并到当前分支：git branch --merged;9.查看当前哪些分支还没有合并到当前分支：git branch --no-merged;10.把远程分支合并到当前分支：git merge &lt;remote-name&gt;/&lt;branch-name&gt;，如git merge origin/serverfix；如果是单线的历史分支不存在任何需要解决的分歧，只是简单的将HEAD指针前移，所以这种合并过程可以称为快进（Fast forward），而如果是历史分支是分叉的，会以当前分叉的两个分支作为两个祖先，创建新的提交对象；如果在合并分支时，遇到合并冲突需要人工解决后，再才能提交；11.在远程分支的基础上创建新的本地分支：git checkout -b &lt;branch-name&gt; &lt;remote-name&gt;/&lt;branch-name&gt;，如git checkout -b serverfix origin/serverfix;12.从远程分支checkout出来的本地分支，称之为跟踪分支。在跟踪分支上向远程分支上推送内容：git push。该命令会自动判断应该向远程仓库中的哪个分支推送数据；在跟踪分支上合并远程分支：git pull；13.将一个分支里提交的改变移到基底分支上重放一遍：git rebase &lt;rebase-branch&gt; &lt;branch-name&gt;，如git rebase master server，将特性分支server提交的改变在基底分支master上重演一遍；使用rebase操作最大的好处是像在单个分支上操作的，提交的修改历史也是一根线；如果想把基于一个特性分支上的另一个特性分支变基到其他分支上，可以使用--onto操作：git rebase --onto &lt;rebase-branch&gt; &lt;feature branch&gt; &lt;sub-feature-branch&gt;，如git rebase --onto master server client；使用rebase操作应该遵循的原则是：一旦分支中的提交对象发布到公共仓库，就千万不要对该分支进行rebase操作； 忽略文件.gitignore 一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。如下例： 12345678910111213# 此为注释 – 将被 Git 忽略# 忽略所有 .a 结尾的文件*.a# 但 lib.a 除外!lib.a# 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO/TODO# 忽略 build/ 目录下的所有文件build/# 会忽略 doc/notes.txt 但不包括 doc/server/arch.txtdoc/*.txt# 忽略 doc/ 目录下所有扩展名为 txt 的文件doc/**/*.txt 查看历史命令 12$ histroy该命令可以按顺序查看你之前输入过的所有的git命令 完整创建一个仓库的流程 1234567cd mall-clientgit inittouch README.mdgit add README.mdgit commit -m &quot;first commit&quot;git remote add origin https://gitee.com/onewj830/mall-client.gitgit push -u origin master 最常用的Git命令 123456789git init \\\\初始化git add . \\\\添加到缓存区git commit \\\\提交到本地仓库git push \\\\复制到远端仓库git clone &lt;url&gt; \\\\克隆库git status \\\\查看当前状态history \\\\查看历史命令git branch &lt;branch-name&gt; \\\\创建分支git branch -d &lt;branch-name&gt; \\\\删除分支 结语 在学习中总结，也有很多不足的地方，总结不是很到位，望指出。 记得很早开始开始接触的时候，也是很懵，看着一堆英文的界面啥也不懂，虽然我看的懂部分英文解释说明，但是对这个系统的工作流程与工作方式是一知半解的，分布式版本控制系统我觉得有时候也像是区块链的思想，我感觉我很有动力去学这些东西，特别是写自己的博客，我觉得很开心，甚至再探索的时候时间不知不觉就来到了凌晨，我非要解决了眼下这个问题才肯休息……从入门到现在，一点心得就是，初学不需明白那些非常高深的理论知识，只要知道git是个版本控制器就行，知道怎么创建仓库，怎么利用别人的仓库的开源项目提升自己，和SVN的区别也不需特别的去理会，一边学习一边应用，效率才是最高的。使用过，才知道他是个什么，以前那些原理也就恍然大悟了，可以举一反三！即初学少研究，多实践，最后事倍功半！","link":"/2022/01/05/Git/"},{"title":"Java学习笔记|Java简介","text":"Java的特性1. Java是简单的:-Java语法和C++很接近，却没有C++中一些少使用、难理解的特性，比如：操作符重载、多继承、自动的强制类型转换。尤其是Java不使用指针，而是使用引用，并提供了自动分配和回收的内存空间，不需要为内存管理而担忧。 2. Java语言是面向对象的：Java 语言提供类、接口和继承等面向对象的特性，为了简单起见，只支持类之间的单继承，但支持接口之间的多继承，并支持类与接口之间的实现机制（关键字为 implements）。Java 语言全面支持动态绑定，而 C++语言只对虚函数使用动态绑定。总之，Java语言是一个纯的面向对象程序设计语言。 3. Java语言是分布式的：Java 语言支持 Internet 应用的开发，在基本的 Java 应用编程接口中有一个网络应用编程接口（java net），它提供了用于网络应用编程的类库，包括 URL、URLConnection、Socket、ServerSocket 等。 4. Java语言是安全的：Java通常被用在网络环境中，为此，Java 提供了一个安全机制以防恶意代码的攻击。除了Java 语言具有的许多安全特性以外，Java 对通过网络下载的类具有一个安全防范机制（类 ClassLoader），如分配不同的名字空间以防替代本地的同名类、字节代码检查，并提供安全管理机制（类 SecurityManager）让 Java 应用设置安全哨兵。 5. Java 语言是体系结构中立的：Java 程序（后缀为 java 的文件）在 Java 平台上被编译为体系结构中立的字节码格式（后缀为 class 的文件），然后可以在实现这个 Java 平台的任何系统中运行。这种途径适合于异构的网络环境和软件的分发。 6.Java 语言是可移植的：这种可移植性来源于体系结构中立性，另外，Java 还严格规定了各个基本数据类型的长度。 7. Java 是高性能的：与那些解释型的高级脚本语言相比，Java 的确是高性能的。事实上，Java 的运行速度随着 JIT(Just-In-Time）编译器技术的发展越来越接近于 C++。 8. Java 语言是多线程的：在 Java 语言中，线程是一种特殊的对象，它必须由 Thread 类或其子（孙）类来创建。通常有两种方法来创建线程：其一，使用型构为 Thread(Runnable) 的构造子类将一个实现了 Runnable 接口的对象包装成一个线程，其二，从 Thread 类派生出子类并重写 run 方法，使用该子类创建的对象即为线程。值得注意的是 Thread 类已经实现了 Runnable 接口，因此，任何一个线程均有它的 run 方法，而 run 方法中包含了线程所要运行的代码。线程的活动由一组方法来控制。Java 语言支持多个线程的同时执行，并提供多线程之间的同步机制（关键字为 synchronized）。 开发环境配置参考Java 开发环境配置 参考菜鸟教程:Java 基础语法","link":"/2022/01/07/Java1/"},{"title":"入门网站归纳","text":"入门各类编程语言的网站 -菜鸟教程","link":"/2022/01/07/cainiao/"},{"title":"数学建模|规划问题","text":"线性规划 定义： 可行解：满足约束条件的解，使目标函数达到最大的可行解称为最优解。 可行域：所有可行解构成的集合。 MATLAB中线性规划的标准形式 ，。 其中的为列向量，为价值向量，为资源向量；，为矩阵。 MATLAB的求解线性规划的命令： 123[x,fval]=linprog(f,A,b)[x,fval]=linprog(f,A,b,Aeq,beq)[x,fval]=linprog(f,A,b,Aeq,beq,lb,ub) x返回决策向量的取值，fval返回目标函数的最优值，A和b对应线性不等式约束；Aeq和beq对应线性等式约束；lb和ub分别对应决策向量的下界向量和上界向量。 例子 相关应用 投资收益和风险 可以参考博客：投资的收益与风险的数学建模 整数规划 数学规划中变量限制为整数时，称为整数规划。线性规划模型中，变量限制为整数，则称为整数线性规划。目前流行的求解整数规划的方法，大多只适用于整数线性规划。 整数规划有两大类： 变量全限制为整数，纯整数规划 变量部分限制为整数，混合整数规划 整数规划特点： 1.原线性规划有最优解，当自变量限制为整数后，其整数规划解出现下述情况： 原线性规划最优解全是整数，则整数规划最优 解与线性规划最优解一致。 整数规划无可行解 求解方法分类 有可行解（当然就存在最优解），但最优解值变差。 2.整数规划最优解不能按照实数最优解简单取整而获得。 求解方法分类： 1.分枝定界法—可求纯或混合整数线性规划。 2.割平面法—可求纯或混合整数线性规划。 3.隐枚举法—求解“0-1”整数规划。 过滤隐枚举法； 分枝隐枚举法。 4.匈牙利法—解决指派问题（“0-1”规划特殊情形）。 5.蒙特卡洛法—求解各种类型规划。 0-1整数规划 相互排斥的约束条件 0-1型整数规划是整数规划中的特殊情形，它的变量仅取0或1。这时称为0-1变量，或称二进制变量。仅取值0 或 1 这个条件可由下述约束条件: ，且为整数 所代替，是和一般整数规划的约束条件形式一致的。 固定费用问题 指派问题 拟分配人去做项工作,每人做且仅做一项工作，若分配第人去做第项工作，需花费单位时间，问应如何分配工作才能使工人花费的总时间最少？ 要给出一个指派问题的实例，只需给出矩阵,被称为指派问题的系数矩阵。 引入0-1变量： 第人做第项工作第人做第项工作 。 上述指派问题的数学模型： ，，或。 上述指派问题的可行解可以用一个矩阵表示，每行、每列均有且只有一个元素为1，其余元素均为0。（因为一个人只能做一项工作） 蒙特卡洛（随机取样） 蒙特卡洛方法也称为计算机随机模拟方法，它源于世界 著名的赌城—摩纳哥的 Monte Carlo（蒙特卡洛）。它是基于 对大量事件的统计结果来实现一些确定性问题的计算。 蒙特卡洛方法可分为两类： 所求解的问题本身具有内在的随机性，借助计算机的 运算能力可以直接模拟这种随机的过程。 所求解问题可以转化为某种随机分布的特征数，比如 随机事件出现的概率，或者随机变量的期望值。用于求解复杂的多维积分问题。 蒙特卡洛法必须使用计算机生成相关分布的随机数，Matlab给出了生成各种随机数的命令。 例子1 若想求得图中不规则阴影部分的面积： 可以在规定的矩形区域内生成随机点，设点在不规则图形内部的数量为,全部的随机点为，矩阵面积为，则不规则阴影面积可以近似为： 例子2 可以参考蒙特卡洛算法的MATLAB实现的例子 非线性规划 非线性规划模型 定义：目标函数或约束条件中包含非线性函数 一般形式： 与线性规划区别：线性规划的最优解只能在可行域的边界达到，而非线性规划的最优解可能在可行域上的任意一点。 matlab中非线性规划的数学模型标准型： 其中部分变量与线下规划相同;为非线性向量函数。 例子 例子求解 代码实现： 1234567891011121314%目标函数function f=fun1(x);f=sum(x.^2)+8;%定义非线性约束条件function [g,h]=fun2(x);g=[-x(1)^2+x(2)-x(3)^2x(1)+x(2)^2+x(3)^3-20]; %非线性不等式约束h=[-x(1)-x(2)^2+2x(2)+2*x(3)^2-3]; %非线性等式约束%主程序，利用函数fmincon[x,y]=fmincon('fun1',rand(3,1),[],[],[],[],zeros(3,1),[],'fun2') 无约束问题 符号解 编写的matlab程序如下 12345678910111213141516171819202122clc, clearsyms x yf=x^3-y^3+3*x^2+3*y^2-9*x;%目标函数df=jacobian(f); %求一阶偏导数d2f=jacobian(df); %求Hessian阵（二阶导数阵）[xx,yy]=solve(df) %求驻点xx=double(xx);yy=double(yy); %转化成双精度浮点型数据，下面判断特征值的正负，必须是数值型的数据for i=1:length(xx) a=subs(d2f,{x,y},{xx(i),yy(i)}); b=eig(a); %求矩阵的特征值 f=subs(f,{x,y},{xx(i),yy(i)}); f=double(f); if all(b&gt;0) fprintf('(%f,%f)是极小值点，对应的极小值为%f\\n',xx(i),yy(i),f); elseif all(b&lt;0) fprintf('(%f,%f)是极大值点，对应的极大值为%f\\n',xx(i),yy(i),f); elseif any(b&gt;0) &amp; any(b&lt;0) fprintf('(%f,%f)不是极值点\\n',xx(i),yy(i)); else fprintf(无法判断(%f,%f)是否是极值点\\n',xx(i),yy(i)); endend 数值解 例子1 求解多元函数：的极值 1234567clc, clearf=@(x) x(1)^3-x(2)^3+3*x(1)^2+3*x(2)^2-9*x(1); %定义匿名函数g=@(x) -f(x);[xy1,z1]=fminunc(f, rand(2,1)) %求极小值点[xy2,z2]=fminsearch(g,rand(2,1)); %求极大值点，只能求出初值附近的一个极小值点xy2, z2=-z2 极小值点:,极小值;极大值点,极大值: 例子2 求函数的极小值 使用函数梯度，编写M函数fun3.m如下： 1234567function[f,g]=fun3(x);f=100*(x(2)-x(1)^2)^2+(1-x(1))^2;g=[-400*x(1)*(x(2)-x(1)^2)^2-2*(1-x(1));200*(x(2)-x(1)^2)];%g返回的是梯度向量%编写主程序文件：options=optimset('GrandObj','on');[x,y]=fminunc('fun3',rand(1,2),options) 求极值时，利用二阶导数（利用Hessian求解，加入优化参数） 123456789%目标函数的Hessian阵function [f,df,d2f]=fun4(x);f=100*(x(2)-x(1)^2)^2+(1-x(1))^2;df=[-400*x(1)*(x(2)-x(1)^2)-2*(1-x(1));200*(x(2)-x(1)^2)];d2f=[-400*x(2)+1200*x(1)^2+2,-400*x(1) -400*x(1),200];%编写主程序文件options = optimset('GradObj','on','Hessian','on');[x,y]=fminunc('fun4',rand(1,2),options) 函数的零点和方程组的解 求多项式 Matlab程序如下： 1234clc, clearxishu=[1 -1 2 -3];%多项式是用向量定义的，系数从高次幂到低次幂排列x0=roots(xishu) 求得多项式的全部零点为-0.1378和1.2757。 使用符号求解如下 123syms xx0=solve(x^3-x^2+2*x-3) %求函数零点的符号解x0=vpa(x0,5) %化成小数格式的数据 数值解 12y=@(x) x^3-x^2+2*x-3;x=fsolve(y,rand) %只能求给定初始值附近的一个零点 约束极值问题 二次规划 若某非线性规划的目标函数为自变量的二次函数，约束条件又全是线性的，就称其为二次规划。 例子： 12345h=[4,-4;-4,8];%实对称矩阵f=[-6;-3];a=[1,1;4,1];b=[3;9];[x,value]=quadprog(h,f,a,b,[],[],zeros(2,1)) 求得,,。 罚函数法 序列无约束最小化技术：将非线性规划问题转化为无约束极值问题。 利用问题中的约束函数作出适当的罚函数，由此构造出带参数的增广目标函数，把问题转化为无约束非线性规划问题。 对如上问题，取一个充分大的数，构造函数 例子 Matlab程序 1234567function g=test3(x);M=50000;f=x(1)^2+x(2)^2+8;g=f-M*min(min(x),0)-M*min(x(1)^2-x(2),0)+M*(-x(1)-x(2)^2+2)^2;%求增广目标函数的极小值[x,y]=fminsearch('test3',rand(2,1)) 飞行管理问题 参考博客【数学建模学习④】飞行管理问题 参考博客 ··············································································· matlab——整数规划 数学建模—整数规划（笔记） 蒙特卡洛算法的MATLAB实现 第三章：非线性规划 【数学建模学习④】飞行管理问题","link":"/2022/01/10/jm1/"},{"title":"笔记|统计学习方法：感知机模型","text":"12345678(由于我的markdown文件是使用VSC写的，在此顺便记录一下markdown语法)Tips：需要在Git中使用 npm install hexo-math --save 命令来安装数学公式环境VSC中预览：先按住ctrl+k,松开后按vctrl B 粗体ctrl l 斜体ctrl shift ] == #ctrl m 标记数学公式环境** 感知机（perception）是一个二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1与-1二值 1.感知机模型 假设输入空间（特征空间）是 ，输出空间是。输入 表示实例的特征向量，对应输出空间（特征空间）的点；输出 表示实例的类别,由输入空间到输出空间的如下函数 称为感知机。其中,和b为感知机模型参数, 叫作权值（weight）或权值向量（weight vector）,叫作偏置（bias）,表示和x的内积。sign是符号函数，即： 感知机是一种线性分类模型，属于判别模型。感知机的假设空间的定义是在特征空间中的所有线性分类模型或者线性分类器，即函数集合。 2.感知机的几何解释 线性方程 对应特征空间中的一个超平面，是超平面的法向量，b是超平面的截距，则这条线将超平面分离成正负两类。 感知机学习，由训练数据集 其中， ,,就得到了感知机模型，即求得模型参数，b。 3.感知机的学习策略 数据集的线性可分 对于给定的数据集 其中， , 能将数据集的正负实例点完全正确地划分到超平面的两侧。可如此划分则称为线性可分数据集，否则称为数据集不可分。 4.损失函数 由点到平面距离公式可得,一个错误分类的点到超平面的距离为： 对于分类错误的点，一定有： 则所有错误分类的点到超平面的总距离： 则感知机的损失函数为： 所有样本都分类正确时，损失函数为0，错误越少，损失函数越小，分类错误样本离超平面距离越近，则损失函数越小。因此感知机的学习目标就是最小化该损失函数。 5.梯度下降法 通俗的讲话，梯度就是导数和偏导数，梯度下降法的思想是：梯度方向是目标函数值下降最快的方向，因此沿着梯度下降的方向优化能最快寻找到目标函数的极小值。 参数、b的更新可以表示为： 在感知机中采用随机梯度下降法，即每次随机选择一个分类错误的样本计算，进行和b的更新，即： 其过程直观理解为：当一个样本被当前超平面划分到分类错误一类时，利用此样本调整超平面的参数，使超平面向靠近该样本的方向移动，则该样本距离超平面的距离减小，从而降低损失函数，直到超平面移动至使该样本被正确划分为止。 6.例子 7.python代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import numpy as np;import pandas as pd;import matplotlib.pyplot as plt; df=pd.read_excel(\"D:\\pythondata\\perceptrondata.xls\");#读取原题数据df=pd.DataFrame(df);x=df.values[:,:-1];#x代表所有行的第一列到倒数第二列的数据，即分类实例的所有特征向量y=df.values[:,-1];#y代表df的倒数第一列数据，即分类实例的所有类别a=x.shape;n=a[0];#n代表x特征向量的行数m=a[1];#m代表x特征向量的列数w=[];#创建空列表，w代表分离超平面中的法向量for i in range(m): w.append(0);#列表的长度与特征向量的特征个数相同#若直接命令w[0]=0,w[1]=0时，会报错，直接按照索引向列表内添加东西时，因为空的列表不能直接指定其位置。b=0;#b代表分离超平面的截距k=1;#用来判别是否找到最优的超平面。假设值为1，即未找到while (k==1): k=0; for i in range(n): t=np.dot(w,x[i]);#用来计算w*x if (y[i]*(t+b)&lt;=0): w=w+np.dot(y[i],x[i]); b=b+y[i]; k=1;print(\"分离超平面的法向量w={0},截距b={1}\".format(w,b));########可视化结果#def plot_and_scatter(df=None,w=0,b=0): xmin=df.values[:,:-1].min(); xmax=df.values[:,:-1].max(); xdiff=(xmax-xmin)*0.5; xx=np.linspace((xmin-xdiff),(xmax+xdiff),100); yy=-b-w[1]*xx; plt.figure(); plt.xlabel(\"X(1)\"); plt.ylabel(\"X(2)\");#设置坐标轴的文字标签 ax=plt.gca();# get current axis 获得坐标轴对象 ax.spines[\"right\"].set_color(\"none\"); ax.spines[\"top\"].set_color(\"none\"); # 将右边 上边的两条边颜色设置为空 其实就相当于抹掉这两条边 ax.xaxis.set_ticks_position(\"bottom\"); ax.yaxis.set_ticks_position(\"left\"); ax.spines[\"bottom\"].set_position((\"data\",0)); ax.spines[\"left\"].set_position((\"data\",0));#指定 data设置的bottom(也就是指定的x轴)绑定到y轴的0这个点上 plt.plot(xx,yy,\"r\"); color_list=[\"blue\",\"green\",\"black\",\"pink\",\"orange\"]; y=df.values[:,-1]; a=set(y); a=list(a); y_num=len(a); t=0; for j in range(y_num): tt=a[j]; y_index=[i for i,y in enumerate(y) if y==tt]; x_group1=df.values[y_index,0]; x_group2=df.values[y_index,1]; plt.scatter(x_group1,x_group2); t=t+1; plot_and_scatter(df,w,b);plt.show();","link":"/2022/01/05/mathrecord1/"},{"title":"数学建模|图与网络模型","text":"图论的基本介绍 由于博主专业，在此不再赘述图论的基本知识算法。 想了解基础知识，请参考图论（一）基本概念与邻接矩阵 顺带一提： - 对于有向图，只要写出邻接矩阵，直接使用Matlab命令：sparse，可以将邻接矩阵转化为稀疏矩阵的表示方式。 - 对于无向图，由于邻接矩阵是对称的，Matlab中只需要使用邻接矩阵的下三角元素 - 稀疏矩阵可以使用full命令变成普通矩阵。 最短路径问题 两个顶点之间的最短问题 Dijkstra算法 基本步骤： 首先，定义一个数组 D，D[v] 表示从源点 s 到顶点 v 的边的权值，如果没有边则将 D[v] 置为无穷大。 把图的顶点集合划分为两个集合 S 和 V-S。第一个集合 S 表示距源点最短距离已经确定的顶点集，即一个顶点如果属于集合 S 则说明从源点 s 到该顶点的最短路径已知。其余的顶点放在另一个集合 V-S 中。 每次从尚未确定最短路径长度的集合 V-S 中取出一个最短特殊路径长度最小的顶点 u，将 u 加入集合 S，同时修改数组 D 中由 s 可达的最短路径长度。若加入集合 S 的 u 作为中间顶点时，vi 的最短路特殊路径长度变短，则修改 vi 的距离值（即当D[u] + W[u, vi] &lt; D[vi]时，令D[vi] = D[u] + W[u, vi]）。 重复第 3 步的操作，一旦 S 包含了所有 V 中的顶点，D 中各顶点的距离值就记录了从源点 s 到该顶点的最短路径长度。 Matlab代码及例子 Matlab代码实现 12345678910111213141516171819202122232425clc,clear alla=zeros(6);a(1,2)=50;a(1,4)=40;a(1,5)=25;a(1,6)=10; a(2,3)=15;a(2,4)=20;a(2,6)=25;a(3,4)=10;a(3,5)=20;a(4,5)=10;a(4,6)=25;a(5,6)=55;a=a+a' a(find(a==0))=inf %将a=0的数全部替换为无强大 pb(1:length(a))=0;pb(1)=1; %当一个点已经求出到原点的最短距离时，其下标i对应的pb(i)赋1index1=1; %存放存入S集合的顺序index2=ones(1,length(a)); %存放始点到第i点最短通路中第i顶点前一顶点的序号d(1:length(a))=inf;d(1)=0; %存放由始点到第i点最短通路的值temp=1; %temp表示c1,算c1到其它点的最短路。while sum(pb)&lt;length(a) %看是否所有的点都标记为P标号tb=find(pb==0); %找到标号为0的所有点,即找到还没有存入S的点d(tb)=min(d(tb),d(temp)+a(temp,tb));%计算标号为0的点的最短路，或者是从原点直接到这个点，又或者是原点经过r1,间接到达这个点tmpb=find(d(tb)==min(d(tb))); %求d[tb]序列最小值的下标temp=tb(tmpb(1));%可能有多条路径同时到达最小值，却其中一个，temp也从原点变为下一个点pb(temp)=1;%找到最小路径的表对应的pb(i)=1index1=[index1,temp]; %存放存入S集合的顺序temp2=find(d(index1)==d(temp)-a(temp,index1));index2(temp)=index1(temp2(1)); %记录标号索引endd, index1, index2 Java代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public class DijkstraAlgorithm { private final static int N = 10000; // 约定 10000 代表距离无穷大 public static void main(String[] args) { char[] vertexes = { 'A', 'B', 'C', 'D', 'E', 'F', 'G' }; // 顶点 int[][] weight = { // 图的邻接矩阵 /*A*//*B*//*C*//*D*//*E*//*F*//*G*/ /*A*/{0, 5, 7, N, N, N, 2}, /*B*/{5, 0, N, 9, N, N, 3}, /*C*/{7, N, 0, N, 8, N, N}, /*D*/{N, 9, N, 0, N, 4, N}, /*E*/{N, N, 8, N, 0, 5, 4}, /*F*/{N, N, N, 4, 5, 0, 6}, /*G*/{2, 3, N, N, 4, 6, 0} }; int source = 6; // 源点下标 int[] dis = dijkstra(source, vertexes, weight); // 使用迪杰斯特拉查找最短路径 // 输出最短路径长度 for (int i=0; i&lt;dis.length; i++){ System.out.println(vertexes[source] + \"-&gt;\" + vertexes[i] + \" = \" + dis[i]); } } /** * 迪杰斯特拉算法求解最短路径问题 * @param source 源点下标 * @param vertexes 顶点集合 * @param weight 邻接矩阵 * @return int[] 源点到各顶点最短路径距离 */ public static int[] dijkstra(int source, char[] vertexes, int[][] weight){ int[] dis; // 记录源点到各顶点的最短路径长度,如 dis[2] 表示源点到下标为 2 的顶点的最短路径长度 ArrayList&lt;Character&gt; S = new ArrayList&lt;&gt;(); // 存储已经求出到源点最短路径的顶点，即算法步骤中的 S 集合。 /* 初始化源点 */ dis = weight[source]; S.add(vertexes[source]); /* 当 S 集合元素个数等于顶点个数时，说明最短路径查找完毕 */ while(S.size() != vertexes.length){ int min = N; int index = -1; // 记录已经求出最短路径的顶点下标 /* 从 V-S 的集合中找距离源点最近的顶点 */ for (int j=0; j&lt;weight.length; j++){ if (!S.contains(vertexes[j]) &amp;&amp; dis[j] &lt; min){ min = weight[source][j]; index = j; } } dis[index] = min; // 更新源点到该顶点的最短路径长度 S.add(vertexes[index]); // 将顶点加入到 S 集合中，即表明该顶点已经求出到源点的最小路径 /* 更新源点经过下标为 index 的顶点到其它各顶点的最短路径 */ for (int m=0; m&lt;weight.length; m++){ if (!S.contains(vertexes[m]) &amp;&amp; dis[index] + weight[index][m] &lt; dis[m]){ dis[m] = dis[index] + weight[index][m]; } } } return dis; }} 每个顶点之间的最短路径 迭代使用n-1次Dijkstra算法 Floyd算法 Floyd算法是一个经典的动态规划算法。用通俗的语言来描述的话，首先我们的目标是寻找从点i到点j的最短路径。从动态规划的角度看问题，我们需要为这个目标重新做一个诠释（这个诠释正是动态规划最富创造力的精华所在）。 从任意节点i到任意节点j的最短路径不外乎两种可能： 一是直接从i到j， 二是从i经过若干个节点k到j。 所以，我们假设Dis(i,j)为节点u到节点v的最短路径的距离，对于每一个节点k，我们检查Dis(i,k) + Dis(k,j) &lt; Dis(i,j)是否成立。 如果成立，证明从i到k再到j的路径比i直接到j的路径短，我们便设置Dis(i,j) = Dis(i,k) + Dis(k,j)，这样一来，当我们遍历完所有节点k，Dis(i,j)中记录的便是i到j的最短路径的距离。 c语言实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;stack&gt;#include &lt;queue&gt;using namespace std;const int MAX = 65535;class Graph{ private: int** G; // 邻接矩阵 int** dist; // 距离数组 int** path; // 路径数组 int Nv; // 顶点数 public: //构造函数 Graph(int nv, int ne){ this-&gt;Nv = nv; G = new int*[nv+1]; dist = new int*[nv+1]; path = new int*[nv+1]; for(int i = 0 ; i &lt; nv+1 ; i++){ G[i] = new int[nv+1]; dist[i] = new int[nv+1]; path[i] = new int[nv+1]; memset(path[i],-1,sizeof(path[0][0])*(nv+1)); for(int j = 0 ; j &lt; nv+1 ; j++){ this-&gt;G[i][j] = this-&gt;dist[i][j] = MAX; } this-&gt;G[i][i] = this-&gt;dist[i][i] = 0; } cout&lt;&lt;\"请输入边与权重:\"&lt;&lt;endl; for( i = 0 ; i &lt; ne ; i++){ int v1,v2,weight; cin&gt;&gt;v1&gt;&gt;v2&gt;&gt;weight; this-&gt;G[v1][v2] = this-&gt;G[v2][v1] = weight; this-&gt;dist[v1][v2] = this-&gt;dist[v2][v1] = weight; } } //Floyd算法(多源最短路径算法) bool Floyd(){ for(int k = 1 ; k &lt; this-&gt;Nv+1 ; k++){ //k代表中间顶点 for(int i = 1 ; i &lt; this-&gt;Nv+1 ; i++){//i代表起始顶点 for(int j = 1 ; j &lt; this-&gt;Nv+1 ; j++){//j代表终点 if(this-&gt;dist[i][k] + this-&gt;dist[k][j] &lt; this-&gt;dist[i][j]){ this-&gt;dist[i][j] = this-&gt;dist[i][k] + this-&gt;dist[k][j]; if(i == j &amp;&amp; this-&gt;dist[i][j] &lt; 0){//发现了负值圈 return false; } this-&gt;path[i][j] = k; } } } } return true; } // 分治法寻找start到end最短路径的中间结点 void Find(queue&lt;int&gt; &amp;q ,int start,int end){ int mid = this-&gt;path[start][end]; if(mid == -1){ return; } Find(q,start,mid); q.push(mid); Find(q,mid,end); } //打印start顶点到end顶点的路径 void Print_Path(int start,int end){ queue&lt;int&gt; queue; queue.push(start); this-&gt;Find(queue,start,end); queue.push(end); cout&lt;&lt;queue.front(); queue.pop(); while(!queue.empty()){ cout&lt;&lt;\"-&gt;\"&lt;&lt;queue.front(); queue.pop(); } cout&lt;&lt;endl; } void Print_Floyd(){ int i,j,k; for( i = 1 ; i &lt; this-&gt;Nv+1 ; i++){ for(int j = 1 ; j &lt; this-&gt;Nv+1 ; j++){ cout&lt;&lt;this-&gt;path[i][j]&lt;&lt;\" \"; } cout&lt;&lt;endl; } cout&lt;&lt;\" length path\"&lt;&lt;endl; for(i = 1 ; i &lt; this-&gt;Nv+1 ; i++){ for(j = i+1 ; j &lt; this-&gt;Nv+1 ; j++){ cout&lt;&lt;i&lt;&lt;\"-&gt;\"&lt;&lt;j&lt;&lt;\" \"; cout&lt;&lt;this-&gt;dist[i][j]&lt;&lt;\" \"; this-&gt;Print_Path(i,j); } cout&lt;&lt;endl; } }};int main(){ cout&lt;&lt;\"请输入顶点数与边长数:\"&lt;&lt;endl; int nv,ne; cin&gt;&gt;nv&gt;&gt;ne; Graph graph(nv,ne); if(graph.Floyd()){ cout&lt;&lt;\"各个顶点的最短路径为：\"&lt;&lt;endl; graph.Print_Floyd(); } return 0; } 运行结果： 最小生成树 连通的无圈图叫作树，度为1的叫作叶子结点。 应用问题例子：欲修筑连接个城市的铁路，已知城与城之间的铁路造价为,设计一个路线图，使得总造价最低。 Prim算法（以点为主，每次选择下一个中最小权重） 基本思想 例子： Matlab实现： 12345678910111213141516171819clc;clear;a=zeros(7);a(1,2)=50;a(1,3)=60;a(2,4)=65;a(2,5)=40;a(3,4)=52;a(3,7)=45;a(4,5)=50;a(4,6)=30;a(4,7)=42;a(5,6)=70;a=a+a';a(find(a==0))=inf;result=[];p=1; %起点为1tb=2:length(a);while length(result)~=length(a)-1 temp=a(p,tb);temp=temp(:); d=min(temp); [jb,kb]=find(a(p,tb)==d); j=p(jb(1));k=tb(kb(1)); result=[result,[j;k;d]];p=[p,k];tb(find(tb==k))=[];endresult Kruskal算法(以边为主，每次选最小) 基本思想 将图的n个顶点看作n个分离的部分树，每个树具有一个顶点，算法的每一步就是选择连接两个分离树的具有最小权值的边，将两个树合二为一，直到只有一个树为止（进行n-1步）得到最小生成树。 例子： 我们使用边权矩阵进行存储数据，边权矩阵就是按列写入，每列由出发顶点、接收顶点和边的权值组成，如下所示： 1234567891011121314151617181920212223242526272829%边权矩阵，每一列都表示一条边，从上到下分别为两个顶点以及它们边的权值b = [1 1 1 2 2 3 3 4; 2 4 5 3 5 4 5 5; 8 1 5 6 7 9 10 3];%sortrows函数对某一列进行比较排序，所以我们先转置b矩阵，然后对第三列也就是权值进行排序[B,i]=sortrows(b',3);%再将其转置回来B=B';%m为边的条数，n为点的个数m=size(b,2);n=5;%t数组用来标记选中的边，k用来计数，T矩阵用来存储选中的边，c计算最小生成树的总长度t=1:n;k=0;T=[];c=0;for i=1:m if t(B(1,i))~=t(B(2,i)) k=k+1;T(k,1:2)=B(1:2,i),c=c+B(3,i); tmin=min(t(B(1,i)),t(B(2,i))); tmax=max(t(B(1,i)),t(B(2,i))); for j=1:n if t(j)==tmax t(j)=tmin; end end end if k==n-1 break; endendT,c, 网络最大流 何为最大流问题？ 参考博客网络流——最大流（全） Matlab实现 最大流问题(maximum flow problem)，一种组合最优化问题，就是要讨论如何充分利用装置的能力，使得运输的流量最大，以取得最好的效果。管道网络中每条边的最大通过能力（容量）是有限的，实际流量不超过容量。 在数学建模的过程中时长会遇到这种问题，存在专门的算法去解决这一问题，但其实现较为复杂。 MATLAB提供了解决这一问题所使用的函数，即maxflow函数。 语法: 1234mf = maxflow(G,s,t)mf = maxflow(G,s,t,algorithm)[mf,GF] = maxflow(___)[mf,GF,cs,ct] = maxflow(___) 符号说明： mf = maxflow(G,s,t) 返回节点 s 和 t 之间的最大流。如果图 G 未加权（即 G.Edges 不包含变量 Weight），则 maxflow 将所有图边的权重视为 1。 mf = maxflow(G,s,t,algorithm) 指定要使用的最大流算法。此语法仅在 G 为有向图时可用。 [mf,GF] = maxflow(___) 还使用上述语法中的任何输入参数返回有向图对象 GF。GF 仅使用 G 中具有非零流值的边构造。 [mf,GF,cs,ct] = maxflow(___) 还返回源和目标节点 ID cs 和 ct，表示与最大流相关联的最小割。 创建并绘制一个加权图。加权边表示流量。 12345s = [1 1 2 2 3 4 4 4 5 5];t = [2 3 3 4 5 3 5 6 4 6];weights = [0.77 0.44 0.67 0.75 0.89 0.90 2 0.76 1 1];G = digraph(s,t,weights);plot(G,'EdgeLabel',G.Edges.Weight,'Layout','layered'); 确定节点1到6的最大流: 1mf = maxflow(G,1,6) 结果： 1234mf = 1.2100 参考博客图的最大流问题（含matlab函数使用方法） Matlab图论工具箱","link":"/2022/01/11/jm2/"},{"title":"笔记|统计学习方法：朴素贝叶斯","text":"联合概率 朴素贝叶斯是生成模型，由训练数据学习联合分布概率,求得后验概率为:。联合概率分布为： 概率估计方法是极大似然法，或者贝叶斯估计。 基本假设：条件独立性 确定x的类别 1. 计算先验概率以及条件概率 ， 2. 对于给定的实例,计算 3. 确定实例的类 确定x的类的例子： 利用贝叶斯定理与联合概率进行分类预测 贝叶斯定理： 将输入x分到后验概率最大的类y 后验概率最大等价于0-1损失函数时的期望风险最小化 贝叶斯估计 条件概率： 的取值有个 称作极大似然估计 称作拉普拉斯平滑 先验概率： 贝叶斯估计例子","link":"/2022/01/07/mathrecord3/"},{"title":"笔记|统计学习方法：k近邻算法","text":"k近邻算法(k-NN)是一种基本分类与回归方法，它有三个基本要素，本文将介绍k近邻算法的模型与kd树。 k近邻算法 给定一个训练数据集，对于新输入的实例，在训练数据集中找到与该实例最临近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。 数学实现 输入:训练数据集 其中， ,为实例的类别,;实例特征向量 输出：实例x所属的类y。 1.根据给定的距离度量，在训练集中找出与最近邻的个点，涵盖这个点的的领域记作。 2.在中根据分类决策规则(如多数表决)决定的类别。 式中为指示函数，即当时为，否则为0. 在时称为最近邻算法，k近邻法没有显式的学习过程。 模型建立 模型三要素为：距离度量，的大小和分类规则。 距离度量 闵可夫斯基距离(Minkowski Distance) 其中。 当时，是欧式距离。 当时，是曼哈顿距离。 的选择 的选择会对结果产生重大影响。 的值过小，极端情况下，测试实例只和最接近的一个样本有关，训练误差很小，但是如果这个样本恰好是噪点，预测就会出错，即产生了过拟合。 如果值过大，极端情况，则会产生欠拟合。 姑通常采用交叉验证法来选取合适的。 分类规则 近邻的分类决策通常是多数表决：由测试样本的个临近样本的多数类决定测试样本的类别。有如下规则： 给定测试样本，其最临近的个训练示例构成的集合，分类损失函数为型损失，如果涵盖区域的类别为，则分类误差率为： 要使得分类误差率最小，就是要使最大，所以多数表决规则等价于误分类绿最小。 实现：树 树算法有三步： 构造树 搜索近邻 预测 树的构建 选取为坐标轴，以训练集中的所有数据坐标中的中位数作为切分点，将超矩形区域切割成两个子区域。将该切分点作为根结点，由根结点生出深度为1的左右子结点，左节点对应坐标小于切分点，右结点对应坐标大于切分点。 对深度为的结点，选择为切分坐标轴，，以该结点区域中训练数据坐标的中位数作为切分点，将区域分为两个子区域，且生成深度为的左、右子结点。左节点对应坐标小于切分点，右结点对应坐标大于切分点 重复2，直到两个子区域没有数据时停止。 实例参考KNN算法和kd树详解（例子+图示） 的搜索 输入：已构造的树，目标点 输出：的最近邻 在kd树中找出包含目标点的叶结点；从根结点，递归地向下访问kd树，若目标点当前纬的坐标小于分切点，则移动到左子结点，直到子结点为叶子结点为止。 此叶子结点为\"当前最近点\"。 递归地向上回退，在每个结点都进行如下操作： 123456(a)如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”(b)当前最近点一定存在于该结点一个子结点对应的区域，检查该子结点的父结点的另一个子结点对应的区域是否有更近的点。具体的，检查另一子结点对应的区域是否与以目标点为球心，以目标点与“当前最近点”间的距离为半径的超球体相交。(c)如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索；如果不相交，则向上退回。 当退回到根结点时，搜索结束。最后一个“当前最近点”即为的最近邻点。 实例参考KNN算法和kd树详解（例子+图示） Python代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import numpy as npclass Node: def __init__(self, data, lchild = None, rchild = None): self.data = data self.lchild = lchild self.rchild = rchildclass KdTree: def __init__(self): self.kdTree = None def create(self, dataSet, depth): #创建kd树，返回根结点 if (len(dataSet) &gt; 0): m, n = np.shape(dataSet) #求出样本行，列 midIndex = int(m / 2) #中间数的索引位置 axis = depth % n #判断以哪个轴划分数据 sortedDataSet = self.sort(dataSet, axis) #进行排序 node = Node(sortedDataSet[midIndex]) #将节点数据域设置为中位数，具体参考下书本 # print sortedDataSet[midIndex] leftDataSet = sortedDataSet[: midIndex] #将中位数的左边创建2改副本 rightDataSet = sortedDataSet[midIndex+1 :] print(leftDataSet) print(rightDataSet) node.lchild = self.create(leftDataSet, depth+1) #将中位数左边样本传入来递归创建树 node.rchild = self.create(rightDataSet, depth+1) return node else: return None def sort(self, dataSet, axis): #采用冒泡排序，利用aixs作为轴进行划分 sortDataSet = dataSet[:] #由于不能破坏原样本，此处建立一个副本 m, n = np.shape(sortDataSet) for i in range(m): for j in range(0, m - i - 1): if (sortDataSet[j][axis] &gt; sortDataSet[j+1][axis]): temp = sortDataSet[j] sortDataSet[j] = sortDataSet[j+1] sortDataSet[j+1] = temp print(sortDataSet) return sortDataSet def preOrder(self, node): if node != None: print(\"tttt-&gt;%s\" % node.data) self.preOrder(node.lchild) self.preOrder(node.rchild) # def search(self, tree, x): # node = tree # depth = 0 # while (node != None): # print node.data # n = len(x) #特征数 # axis = depth % n # if x[axis] &lt; node.data[axis]: # node = node.lchild # else: # node = node.rchild # depth += 1 def search(self, tree, x): self.nearestPoint = None #保存最近的点 self.nearestValue = 0 #保存最近的值 def travel(node, depth = 0): #递归搜索 if node != None: #递归终止条件 n = len(x) #特征数 axis = depth % n #计算轴 if x[axis] &lt; node.data[axis]: #如果数据小于结点，则往左结点找 travel(node.lchild, depth+1) else: travel(node.rchild, depth+1) #以下是递归完毕后，往父结点方向回朔 distNodeAndX = self.dist(x, node.data) #目标和节点的距离判断 if (self.nearestPoint == None): #确定当前点，更新最近的点和最近的值 self.nearestPoint = node.data self.nearestValue = distNodeAndX elif (self.nearestValue &gt; distNodeAndX): self.nearestPoint = node.data self.nearestValue = distNodeAndX print(node.data, depth, self.nearestValue, node.data[axis], x[axis]) if (abs(x[axis] - node.data[axis]) &lt;= self.nearestValue): #确定是否需要去子节点的区域去找（圆的判断） if x[axis] &lt; node.data[axis]: travel(node.rchild, depth+1) else: travel(node.lchild, depth + 1) travel(tree) return self.nearestPoint def dist(self, x1, x2): #欧式距离的计算 return ((np.array(x1) - np.array(x2)) ** 2).sum() ** 0.5##运行示例：#初始值设定dataSet = [[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]x = [5, 3]#调用函数kdtree = KdTree()tree = kdtree.create(dataSet, 0)kdtree.preOrder(tree)#输出结果print(kdtree.search(tree, x))","link":"/2022/01/06/mathrecord2/"},{"title":"笔记|统计学习方法：决策树(一)","text":"决策树的基本概念 决策树就是一棵树。 叶结点对应于决策结果，其他每个结点则对应于一个属性测试； 每个结点包含的样本集合根据属性测试的结果被划分到子结点中； 根结点包含样本全集，从根结点到每个叶子结点的路径对应了一个判定测试序列。 示例： 决策树学习的关键在于如何选择最优的划分属性，所谓的最优划分属性，对于二元分类而言，就是尽量使划分的样本属于同一类别，即“纯度”最高的属性。那么如何来度量特征（features）的纯度，这时候就要用到“经验熵（information entropy）”。 经验熵 先来看看信息熵的定义：假如当前样本集D中第k类样本所占的比例为为类别的总数（对于二元分类来说,i=2）。则样本集的信息熵为： 的值越小，则D的纯度越高。 信息增益算法 输入：训练数据集和特征； 输出：特征对训练数据的信息增益 信息增益表示了得知特征X的信息而使得类Y的信息的不确定性性减少的程度。 经验条件熵 计算特征对数据集的经验条件熵 的意思是在某一特征下的特征样本中满足训练目标的个数 指的是该特征的样本容量 信息增益为： 信息增益=经验熵-经验条件熵 一般而言，信息增益越大，则表示使用特征 对数据集划分所获得的“纯度提升”越大。所以信息增益可以用于决策树划分属性的选择，其实就是选择信息增益最大的属性，ID3算法就是采用的信息增益来划分属性。 信息增益比： 信息增益比为信息增益与训练数据集关于特征的值熵值之比： 其中： 是特征取值的个数。 ID3算法决策树生成 输入：数据集，特征集,阈值 输出：决策树 先对特征集求信息增益，选取最大的作为根结点的特征。 看特征集对训练数据集D的划分成几个子集（例如该特征集会划分 是或否具有该特征） 若某个子集只具有同一个类的的样本点，成为一个叶节点 否则，对子集从剩下的特征中选择新的特征，求得信息增益 如此递归操作，直到所有的特征分类，生成决策树。 Python代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108from math import logimport operatordef calcShannonent(dataSet): #计算数据的熵（entropy） numEntries = len(dataSet) #数据条数 labelCounts = {} for featVec in dataSet: currentLabel = featVec[-1] #每行数据的最后一个字（类别） if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 #统计又多少个类以及每个类的数量 shannonEnt = 0 for key in labelCounts: prob = float(labelCounts[key])/numEntries #计算单个类的熵值 shannonEnt -= prob*log(prob, 2) #累加每个类的熵值 return shannonEntdef createDataSet_temp(): #创造实例数据 labels = ['头发', '声音'] with open('TreeGrowth_ID3.txt', 'r', encoding='UTF-8') as f: #改了一个文件读写 dataSet = [[] for i in range(9)] value = ['长', '短', '粗', '细', '男', '女'] num_line = 0 for line in f: # re.split(r'(\\s{8}\\[\\')|(\\', \\')|(\\'\\],)|(\\'\\])', line) #尝试使用正则划分，失败了 for i in line: if (i in value): dataSet[num_line].append(i) num_line += 1 del(dataSet[0]) ''' dataSet = [ ['长', '粗', '男'], ['短', '粗', '男'], ['短', '粗', '男'], ['长', '细', '女'], ['短', '细', '女'], ['短', '粗', '女'], ['长', '粗', '女'], ['长', '粗', '女'] ] ''' return dataSet, labelsdef splitDataSet(dataSet, axis, value): #按某个特征分类后的数据 retDataSet = [] for featVec in dataSet: if featVec[axis] == value: #axis表示指定属性在label中的标号，value是该分类中属性的目标值 reducedFeatVec = featVec[:axis] #对于每一条记录，在分完类后，都要把之前使用过的属性删除 reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSetdef chooseBestFeatureToSplit(dataSet): #选择最优的分类特征 numFeatures = len(dataSet[0])-1 #特征的个数 baseEntropy = calcShannonent(dataSet) #原始熵 bestInfoGain = 0 bestFeature = -1 for i in range(numFeatures): #循环每一个特征 featList = [example[i] for example in dataSet]#读取每一条记录取出其中第i个属性的值，并新建一个列表 uniqueVals = set(featList) #set()创建一个无序不重复元素集，可进行关系测试，删除重复元素，进行交差并集运算 newEntropy = 0 for value in uniqueVals: #对于第i个属性值列表中每一个值 subDataSet = splitDataSet(dataSet, i, value) #subDataSet是去掉了第i个属性值是value的列表 prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob*calcShannonent(subDataSet) #按特征分类后的熵 infoGain = baseEntropy - newEntropy #计算信息增益 if(infoGain &gt; bestInfoGain): #若按某特征划分后，熵值减少的最大，则次特征为最优分类特征 bestInfoGain = infoGain bestFeature = i return bestFeaturedef majorityCnt(classList): #多数表决排序，如：最后分类为2男1女则判断为男 classCount = {} for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) #将items返回的可遍历键值对数组，根据键值对的第二个域(值)，进行降序排列 return sortedClassCount[0][0] #返回数量最大的类名def createTree(dataSet,labels): classList = [example[-1] for example in dataSet]#类别：男或女 对于记录集中的每一条都取其最后一个值形成列表 if classList.count(classList[0]==len(classList)):#~~~~~~~~~~~~~~~~~~~当该记录集只有一个特征时 return classList[0] if len(dataSet[0])==1: #当最后一个属性分类完成，记录的长为一 return majorityCnt((classList)) #直接返回数量最多的值 bestFeat = chooseBestFeatureToSplit(dataSet) #找到最优特征的标号 if(bestFeat == -1): #自己添加的，不然报错 return classList[0] bestFeatLabel = labels[bestFeat] #找到最优特征 myTree = {bestFeatLabel:{}} #分类结果以字典形式保存，得到树的当前层 del(labels[bestFeat]) #删除labels中当前最优特征 featValues = [example[bestFeat] for example in dataSet]#将记录集中当前最优特征的值形成列表 uniqueVals = set(featValues) #对值集去重 for value in uniqueVals: #对于每个值 subLabels = labels[:] #subLabels为去除当前最优特征的特征集 myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),subLabels) #使用递归，创造下一层树，~~~~~~~~~~~~~~~~~~~~~~ return myTreeif __name__=='__main__': dataSet, labels = createDataSet_temp() #创造示例数据 print(createTree(dataSet, labels)) #输出决策树模型结果 参考博客： - 决策树（decision tree）(一)——构造决策树方法 - 决策树原理实例（python代码实现）","link":"/2022/01/07/mathrecord4/"},{"title":"笔记|统计学习方法：决策树(二)","text":"决策树的剪枝 由于决策树的生成算法是递归实现的，所以对已知数据的分类十分准确，但对未知数据的预测就不那么准确，就产生了过拟合的现象。 所以就产生了一种将已生成的树进行简化的过程，称为：“剪枝”。 剪枝算法定义 决策树的剪枝往往通过极小化决策树整体的损失函数来实现。 设 - 树的叶子结点个数为 - 是树的叶子结点 - 该叶子结点有个样本点，其中类的样本点有个 - 为叶子结点上的经验熵 - 为参数 则损失函数定义为： 其中经验熵为： 将记作,代入： 这时有： 式中： - 表示模型对训练数据的预测误差（模型与训练数据的拟合程度） - 表示模型复杂度 - 参数控制着两者之间的联系，参数较大的促使选择较为简单的模型，参数较小则选择复杂的模型，只考虑训练数据的拟合程度，不考虑模型复杂度。 ##### 剪枝 剪枝就是当确定时，选择损失函数最小的模型，即损失函数最小的子树。 决策树的生成只考虑了通过提高信息增益对训练数据的更好拟合，而剪枝通过优化损失函数还考虑减小模型复杂度。 树剪枝算法使用 输入：生成算法产生的整个树，参数； 输出：修剪后的子树 计算每个节点的经验熵 递归得从树的叶子结点向上回缩 设一组叶节点回缩到其父结点之前与之后的整体树分别为与，其对应的损失函数为与 。如果 返回第二步，直到不能继续位置，得到损失函数最小的子树。 Python代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import mathimport numpy as np # 创建数据集 备注 李航《统计学习方法》中表5.1 贷款申请数据数据def createDataLH(): data = np.array([['青年', '否', '否', '一般']]) data = np.append(data, [['青年', '否', '否', '好']], axis = 0) data = np.append(data, [['青年', '是', '否', '好'] , ['青年', '是', '是', '一般'] , ['青年', '否', '否', '一般'] , ['中年', '否', '否', '一般'] , ['中年', '否', '否', '好'] , ['中年', '是', '是', '好'] , ['中年', '否', '是', '非常好'] , ['中年', '否', '是', '非常好'] , ['老年', '否', '是', '非常好'] , ['老年', '否', '是', '好'] , ['老年', '是', '否', '好'] , ['老年', '是', '否', '非常好'] , ['老年', '否', '否', '一般'] ], axis = 0) label = np.array(['否', '否', '是', '是', '否', '否', '否', '是', '是', '是', '是', '是', '是', '是', '否']) name = np.array(['年龄', '有工作', '有房子', '信贷情况']) return data, label, name# 创建西瓜书数据集2.0def createDataXG20(): data = np.array([['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'] , ['乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑'] , ['乌黑', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'] , ['青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑'] , ['浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'] , ['青绿', '稍蜷', '浊响', '清晰', '稍凹', '软粘'] , ['乌黑', '稍蜷', '浊响', '稍糊', '稍凹', '软粘'] , ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '硬滑'] , ['乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑'] , ['青绿', '硬挺', '清脆', '清晰', '平坦', '软粘'] , ['浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑'] , ['浅白', '蜷缩', '浊响', '模糊', '平坦', '软粘'] , ['青绿', '稍蜷', '浊响', '稍糊', '凹陷', '硬滑'] , ['浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑'] , ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '软粘'] , ['浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑'] , ['青绿', '蜷缩', '沉闷', '稍糊', '稍凹', '硬滑']]) label = np.array(['是', '是', '是', '是', '是', '是', '是', '是', '否', '否', '否', '否', '否', '否', '否', '否', '否']) name = np.array(['色泽', '根蒂', '敲声', '纹理', '脐部', '触感']) return data, label, namedef splitXgData20(xgData, xgLabel): xgDataTrain = xgData[[0, 1, 2, 5, 6, 9, 13, 14, 15, 16],:] xgDataTest = xgData[[3, 4, 7, 8, 10, 11, 12],:] xgLabelTrain = xgLabel[[0, 1, 2, 5, 6, 9, 13, 14, 15, 16]] xgLabelTest = xgLabel[[3, 4, 7, 8, 10, 11, 12]] return xgDataTrain, xgLabelTrain, xgDataTest, xgLabelTest# 定义一个常用函数 用来求numpy array中数值等于某值的元素数量equalNums = lambda x,y: 0 if x is None else x[x==y].size# 定义计算信息熵的函数def singleEntropy(x): \"\"\"计算一个输入序列的信息熵\"\"\" # 转换为 numpy 矩阵 x = np.asarray(x) # 取所有不同值 xValues = set(x) # 计算熵值 entropy = 0 for xValue in xValues: p = equalNums(x, xValue) / x.size entropy -= p * math.log(p, 2) return entropy # 定义计算条件信息熵的函数def conditionnalEntropy(feature, y): \"\"\"计算 某特征feature 条件下y的信息熵\"\"\" # 转换为numpy feature = np.asarray(feature) y = np.asarray(y) # 取特征的不同值 featureValues = set(feature) # 计算熵值 entropy = 0 for feat in featureValues: # 解释：feature == feat 是得到取feature中所有元素值等于feat的元素的索引（类似这样理解） # y[feature == feat] 是取y中 feature元素值等于feat的元素索引的 y的元素的子集 p = equalNums(feature, feat) / feature.size entropy += p * singleEntropy(y[feature == feat]) return entropy # 定义信息增益def infoGain(feature, y): return singleEntropy(y) - conditionnalEntropy(feature, y)# 定义信息增益率def infoGainRatio(feature, y): return 0 if singleEntropy(feature) == 0 else infoGain(feature, y) / singleEntropy(feature)# 使用李航数据测试函数 p62lhData, lhLabel, lhName = createDataLH()print(\"书中H(D)为0.971，函数结果：\" + str(round(singleEntropy(lhLabel), 3))) print(\"书中g(D, A1)为0.083，函数结果：\" + str(round(infoGain(lhData[:,0] ,lhLabel), 3))) print(\"书中g(D, A2)为0.324，函数结果：\" + str(round(infoGain(lhData[:,1] ,lhLabel), 3))) print(\"书中g(D, A3)为0.420，函数结果：\" + str(round(infoGain(lhData[:,2] ,lhLabel), 3))) print(\"书中g(D, A4)为0.363，函数结果：\" + str(round(infoGain(lhData[:,3] ,lhLabel), 3))) # 测试正常，与书中结果一致 运行结果： 12345书中H(D)为0.971，函数结果：0.971书中g(D, A1)为0.083，函数结果：0.083书中g(D, A2)为0.324，函数结果：0.324书中g(D, A3)为0.420，函数结果：0.42书中g(D, A4)为0.363，函数结果：0.363 预剪枝： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 创建预剪枝决策树def createTreePrePruning(dataTrain, labelTrain, dataTest, labelTest, names, method = 'id3'): \"\"\" 预剪枝 需要使用测试数据对每次的划分进行评估 策略说明：原本如果某节点划分前后的测试结果没有提升，根据奥卡姆剃刀原则将不进行划分（即执行剪枝），但考虑到这种策略容易造成欠拟合， 且不能排除后续划分有进一步提升的可能，因此，没有提升仍保留划分，即不剪枝 另外：周志华的书上评估的是某一个节点划分前后对该层所有数据综合评估，如评估对脐部 凹陷下色泽是否划分， 书上取的色泽划分前的精度是71.4%(5/7)，划分后的精度是57.1%(4/7)，都是脐部下三个特征（凹陷，稍凹，平坦）所有的数据的精度，计算也不易 而我觉得实际计算时，只对当前节点下的数据划分前后进行评估即可，如脐部凹陷时有三个测试样本， 三个样本色泽划分前的精度是2/3=66.7%，色泽划分后的精度是1/3=33.3%，因此判断不划分 \"\"\" trainData = np.asarray(dataTrain) labelTrain = np.asarray(labelTrain) testData = np.asarray(dataTest) labelTest = np.asarray(labelTest) names = np.asarray(names) # 如果结果为单一结果 if len(set(labelTrain)) == 1: return labelTrain[0] # 如果没有待分类特征 elif trainData.size == 0: return voteLabel(labelTrain) # 其他情况则选取特征 bestFeat, bestEnt = bestFeature(dataTrain, labelTrain, method = method) # 取特征名称 bestFeatName = names[bestFeat] # 从特征名称列表删除已取得特征名称 names = np.delete(names, [bestFeat]) # 根据最优特征进行分割 dataTrainSet, labelTrainSet = splitFeatureData(dataTrain, labelTrain, bestFeat) # 预剪枝评估 # 划分前的分类标签 labelTrainLabelPre = voteLabel(labelTrain) labelTrainRatioPre = equalNums(labelTrain, labelTrainLabelPre) / labelTrain.size # 划分后的精度计算 if dataTest is not None: dataTestSet, labelTestSet = splitFeatureData(dataTest, labelTest, bestFeat) # 划分前的测试标签正确比例 labelTestRatioPre = equalNums(labelTest, labelTrainLabelPre) / labelTest.size # 划分后 每个特征值的分类标签正确的数量 labelTrainEqNumPost = 0 for val in labelTrainSet.keys(): labelTrainEqNumPost += equalNums(labelTestSet.get(val), voteLabel(labelTrainSet.get(val))) + 0.0 # 划分后 正确的比例 labelTestRatioPost = labelTrainEqNumPost / labelTest.size # 如果没有评估数据 但划分前的精度等于最小值0.5 则继续划分 if dataTest is None and labelTrainRatioPre == 0.5: decisionTree = {bestFeatName: {}} for featValue in dataTrainSet.keys(): decisionTree[bestFeatName][featValue] = createTreePrePruning(dataTrainSet.get(featValue), labelTrainSet.get(featValue) , None, None, names, method) elif dataTest is None: return labelTrainLabelPre # 如果划分后的精度相比划分前的精度下降, 则直接作为叶子节点返回 elif labelTestRatioPost &lt; labelTestRatioPre: return labelTrainLabelPre else : # 根据选取的特征名称创建树节点 decisionTree = {bestFeatName: {}} # 对最优特征的每个特征值所分的数据子集进行计算 for featValue in dataTrainSet.keys(): decisionTree[bestFeatName][featValue] = createTreePrePruning(dataTrainSet.get(featValue), labelTrainSet.get(featValue) , dataTestSet.get(featValue), labelTestSet.get(featValue) , names, method) return decisionTree 预剪枝测试： 12345678910111213# 将西瓜数据2.0分割为测试集和训练集xgDataTrain, xgLabelTrain, xgDataTest, xgLabelTest = splitXgData20(xgData, xgLabel)# 生成不剪枝的树xgTreeTrain = createTree(xgDataTrain, xgLabelTrain, xgName, method = 'id3')# 生成预剪枝的树xgTreePrePruning = createTreePrePruning(xgDataTrain, xgLabelTrain, xgDataTest, xgLabelTest, xgName, method = 'id3')# 画剪枝前的树print(\"剪枝前的树\")createPlot(xgTreeTrain)# 画剪枝后的树print(\"剪枝后的树\")createPlot(xgTreePrePruning) 后剪枝 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 创建决策树 带预划分标签def createTreeWithLabel(data, labels, names, method = 'id3'): data = np.asarray(data) labels = np.asarray(labels) names = np.asarray(names) # 如果不划分的标签为 votedLabel = voteLabel(labels) # 如果结果为单一结果 if len(set(labels)) == 1: return votedLabel # 如果没有待分类特征 elif data.size == 0: return votedLabel # 其他情况则选取特征 bestFeat, bestEnt = bestFeature(data, labels, method = method) # 取特征名称 bestFeatName = names[bestFeat] # 从特征名称列表删除已取得特征名称 names = np.delete(names, [bestFeat]) # 根据选取的特征名称创建树节点 划分前的标签votedPreDivisionLabel=_vpdl decisionTree = {bestFeatName: {\"_vpdl\": votedLabel}} # 根据最优特征进行分割 dataSet, labelSet = splitFeatureData(data, labels, bestFeat) # 对最优特征的每个特征值所分的数据子集进行计算 for featValue in dataSet.keys(): decisionTree[bestFeatName][featValue] = createTreeWithLabel(dataSet.get(featValue), labelSet.get(featValue), names, method) return decisionTree # 将带预划分标签的tree转化为常规的tree# 函数中进行的copy操作，原因见有道笔记 【YL20190621】关于Python中字典存储修改的思考def convertTree(labeledTree): labeledTreeNew = labeledTree.copy() nodeName = list(labeledTree.keys())[0] labeledTreeNew[nodeName] = labeledTree[nodeName].copy() for val in list(labeledTree[nodeName].keys()): if val == \"_vpdl\": labeledTreeNew[nodeName].pop(val) elif type(labeledTree[nodeName][val]) == dict: labeledTreeNew[nodeName][val] = convertTree(labeledTree[nodeName][val]) return labeledTreeNew# 后剪枝 训练完成后决策节点进行替换评估 这里可以直接对xgTreeTrain进行操作def treePostPruning(labeledTree, dataTest, labelTest, names): newTree = labeledTree.copy() dataTest = np.asarray(dataTest) labelTest = np.asarray(labelTest) names = np.asarray(names) # 取决策节点的名称 即特征的名称 featName = list(labeledTree.keys())[0] # print(\"\\n当前节点：\" + featName) # 取特征的列 featCol = np.argwhere(names==featName)[0][0] names = np.delete(names, [featCol]) # print(\"当前节点划分的数据维度：\" + str(names)) # print(\"当前节点划分的数据：\" ) # print(dataTest) # print(labelTest) # 该特征下所有值的字典 newTree[featName] = labeledTree[featName].copy() featValueDict = newTree[featName] featPreLabel = featValueDict.pop(\"_vpdl\") # print(\"当前节点预划分标签：\" + featPreLabel) # 是否为子树的标记 subTreeFlag = 0 # 分割测试数据 如果有数据 则进行测试或递归调用 np的array我不知道怎么判断是否None, 用is None是错的 dataFlag = 1 if sum(dataTest.shape) &gt; 0 else 0 if dataFlag == 1: # print(\"当前节点有划分数据！\") dataTestSet, labelTestSet = splitFeatureData(dataTest, labelTest, featCol) for featValue in featValueDict.keys(): # print(\"当前节点属性 {0} 的子节点：{1}\".format(featValue ,str(featValueDict[featValue]))) if dataFlag == 1 and type(featValueDict[featValue]) == dict: subTreeFlag = 1 # 如果是子树则递归 newTree[featName][featValue] = treePostPruning(featValueDict[featValue], dataTestSet.get(featValue), labelTestSet.get(featValue), names) # 如果递归后为叶子 则后续进行评估 if type(featValueDict[featValue]) != dict: subTreeFlag = 0 # 如果没有数据 则转换子树 if dataFlag == 0 and type(featValueDict[featValue]) == dict: subTreeFlag = 1 # print(\"当前节点无划分数据！直接转换树：\"+str(featValueDict[featValue])) newTree[featName][featValue] = convertTree(featValueDict[featValue]) # print(\"转换结果：\" + str(convertTree(featValueDict[featValue]))) # 如果全为叶子节点， 评估需要划分前的标签，这里思考两种方法， # 一是，不改变原来的训练函数，评估时使用训练数据对划分前的节点标签重新打标 # 二是，改进训练函数，在训练的同时为每个节点增加划分前的标签，这样可以保证评估时只使用测试数据，避免再次使用大量的训练数据 # 这里考虑第二种方法 写新的函数 createTreeWithLabel，当然也可以修改createTree来添加参数实现 if subTreeFlag == 0: ratioPreDivision = equalNums(labelTest, featPreLabel) / labelTest.size equalNum = 0 for val in labelTestSet.keys(): equalNum += equalNums(labelTestSet[val], featValueDict[val]) ratioAfterDivision = equalNum / labelTest.size # print(\"当前节点预划分标签的准确率：\" + str(ratioPreDivision)) # print(\"当前节点划分后的准确率：\" + str(ratioAfterDivision)) # 如果划分后的测试数据准确率低于划分前的，则划分无效，进行剪枝，即使节点等于预划分标签 # 注意这里取的是小于，如果有需要 也可以取 小于等于 if ratioAfterDivision &lt; ratioPreDivision: newTree = featPreLabel return newTree 测试： 12345678910111213141516# 书中的树结构 p81 p83xgTreeBeforePostPruning = {\"脐部\": {\"_vpdl\": \"是\" , '凹陷': {'色泽':{\"_vpdl\": \"是\", '青绿': '是', '乌黑': '是', '浅白': '否'}} , '稍凹': {'根蒂':{\"_vpdl\": \"是\" , '稍蜷': {'色泽': {\"_vpdl\": \"是\" , '青绿': '是' , '乌黑': {'纹理': {\"_vpdl\": \"是\" , '稍糊': '是', '清晰': '否', '模糊': '是'}} , '浅白': '是'}} , '蜷缩': '否' , '硬挺': '是'}} , '平坦': '否'}}xgTreePostPruning = treePostPruning(xgTreeBeforePostPruning, xgDataTest, xgLabelTest, xgName)createPlot(convertTree(xgTreeBeforePostPruning))createPlot(xgTreePostPruning) 代码参考博客： 决策树python源码实现（含预剪枝和后剪枝）","link":"/2022/01/09/mathrecord5/"},{"title":"有关抛射角度对铅球射程的影响的研究","text":"假设——以水平面为参考系 假设一定高度的人以一定的角度抛出一铅球求该铅球的最远射程 设运动员出手高度为h 出手角度为 出手速度为v 铅球达到最高点时间 从最高点下落到水平面的时间为 总时间 水平方向路程 忽略空气阻力 不考虑展臂动作带来的误差 求解过程 1.将初速度分解： 2.铅球从开始抛出到最高点时间： 3.铅球最高点处到抛出位置的垂直高度： 4.铅球从最高点落到水平面的时间： 5.铅球水平方向经过的路程： 联立以上方程求得与出手速度、出手角度、出手高度的函数关系式： 此时的情况是对出手高度没有要求。 给定出手高度，对于不同的出手速度，确定最佳的角度 对求导： 使得，则 化简： 姑在给定高度，对于不同速度，最佳角度 结论 铅球抛出点与落地点等高时，最佳抛射角度为。，不等高时小于。，具体的最佳抛射角度与抛出点和落地点的高度差以及抛出时的速度有关。理论值大约在。~。. matlab实现","link":"/2022/01/06/wzhppydshecheng/"},{"title":"笔记|统计学习方法：CART算法","text":"待写","link":"/2022/01/09/mathrecord6/"},{"title":"笔记|统计学习方法：逻辑斯蒂回归与最大熵","text":"","link":"/2022/01/12/mathrecord7/"},{"title":"数学建模|回归分析","text":"什么是回归分析 人们关心的因变量受自变量的关联性(非因果性)的影响，并且存在众多随机因素，难以用机理分析方法找出它们之间的关系；需要建立这些变量的数学模型，使得能够根据自变量的数值预测因变量的大小，或者解释因变量的变化。 换句话说：回归分析是一种类相关性分析，就是通过分析已知数据和其造成的影响，来预测未知数据造成的影响。 一般来说，回归分析的主要步骤： 收集一组包含因变量和自变量的数据 选定因变量与自变量之间的模型，利用数据按照最小二乘准则计算模型中的系数； 利用统计分析方法对不同的模型进行比较，找出与数据拟合得最好的模型； 判断得到的模型是否适合于这组数据, 诊断有无不适合回归模型的异常数据； 利用模型对因变量作出预测或解释。 线性回归 一元线性回归 模型为： 其中：为自变量，,为回归系数，是随机变量(影响的随机因素的总和)。 由于是非随机的，可以视作某个常数，故也可以理解为 相当于正态总体的参数估计问题。 模型假设 独立：对于不同的相互独立 线性：的期望是的线性函数 齐次：对于不同的的方差是常数 正态：对于给定的服从正态分布 是相互独立的、期望为、方差为，正态分布的随机变量即：,称为随机误差。 回归系数的最小二乘估计 将数据带入，则对 随后对求偏导，得出： 误差方差估计 残差： 则的无偏估计： 的自由度=数据容量-模型中含有的参数个数 剩余方差(样本方差)，剩余标准差（样本标准差） 回归系数的区间估计和假设检验 模型的有效性检验 利用一元线性回归模型进行预测 Matlab实现 12b=regress(y,X)[b,bint,r,rint,s]=regress(y,X,alpha) 输入： y:因变量（列向量） X:1与自变量组成的矩阵 alpha：显著性水平（若无值，则设为0.05） 输出： - - bint：的置信区间 - r:残差 - rint:残差的置信区间 - s:决定系数 - 值，分布的分位数大于值的概率，当时，模型有效。 多元线性回归 标准方程 保证可逆只需要保证满秩即可，是因为观测y与变量x是有区别的，不然容易混淆。 误差方差估计 归回系数区间估计和假设检验 模型有效性检测 预测 例子 多元线性回归模型：已知某湖八年来湖水中COD浓度实测值(y)与影响因素湖区工业产值(x1)、总人口数(x2)、捕鱼量(x3)、降水量(x4)资料，建立污染物y的水质分析模型。 1234567891011%输入数据x1=[1.376, 1.375, 1.387, 1.401, 1.412, 1.428, 1.445, 1.477]x2=[0.450, 0.475, 0.485, 0.500, 0.535, 0.545, 0.550, 0.575]x3=[2.170 ,2.554, 2.676, 2.713, 2.823, 3.088, 3.122, 3.262]x4=[0.8922, 1.1610 ,0.5346, 0.9589, 1.0239, 1.0499, 1.1065, 1.1387]y=[5.19, 5.30, 5.60,5.82,6.00, 6.06,6.45, 6.95] x=[ones(8,1),x1' x2' x3' x4'][b,bint,r,rint,stats] = regress(y',x) 运行结果： 取得其中的结果： 且 所以, 通过查表可知，代表决定系数（代表相关系数），它的值很接近与1，说明此方程是高度线性相关的； (这里使用的是F检验) 检验值为远大于，可见，检验结果是显著的。 非线性回归 什么是非线性回归 对于非线性回归分析，需要根据实际情况来确定函数类型，再根据已知的数据来估计非线性函数中的参数。常见的回归函数有幂函数、指数函数、对数函数、S型曲线函数，S型曲线函数中常见的是Logistic回归模型，其函数表达式为： 对于非线性回归，往往需要根据已知的数据绘制散点图，以此分析出数据的变化趋势，进而确定回归模型。 函数nlinfit语法： 1[beta,r,j] = nlinfit(x,y,@function,b0) 其中： x表示自变量 y表示因变量 function表示回归函数的函数名 b0表示回归函数中参数的初值 beta表示回归参数的最优值 r表示残差 j表示雅克比矩阵 常见非线性函数模型：matlab 万能实用的非线性曲线拟合方法 例子请参照博客：非线性回归分析及其Matlab实现 参考博客： 数学建模——回归分析（上） 数学建模常用模型22：回归模型 非线性回归分析及其Matlab实现","link":"/2022/01/12/jm3/"},{"title":"Java学习笔记|基础语法","text":"基础语法一个 Java 程序可以认为是一系列对象的集合，而这些对象通过调用彼此的方法来协同工作。下面简要介绍下类、对象、方法和实例变量的概念。 对象：对象是类的一个实例，有状态和行为。例如，一条狗是一个对象，它的状态有：颜色、名字、品种；行为有：摇尾巴、叫、吃等。 类：类是一个模板，它描述一类对象的行为和状态。 方法：方法就是行为，一个类可以有很多方法。逻辑运算、数据修改以及所有动作都是在方法中完成的。 实例变量：每个对象都有独特的实例变量，对象的状态由这些实例变量的值决定。 程序实例123456789101112131415public class project2 { public static void main(String[] args) { System.out.println(&quot;Hello World&quot;); }} /* 第一个Java程序 * 它将输出字符串 Hello World * public是访问修饰符 * project2是java文件的名 * static是关键字 * void是返回类型 * main是方法名 * String[]是String类 * args是字符串数组*/ 注意的点 大小写敏感：Java 是大小写敏感的，这就意味着标识符 Hello 与 hello 是不同的。 类名：对于所有的类来说，类名的首字母应该大写。如果类名由若干单词组成，那么每个单词的首字母应该大写，例如 MyFirstJavaClass 。 方法名：所有的方法名都应该以小写字母开头。如果方法名含有若干单词，则后面的每个单词首字母大写。 源文件名：源文件名必须和类名相同。当保存文件的时候，你应该使用类名作为文件名保存（切记 Java 是大小写敏感的），文件名的后缀为 .java。（如果文件名和类名不相同则会导致编译错误）。 主方法入口：所有的 Java 程序由 public static void main(String[] args) 方法开始执行。 Java标识符 标识符要以字母、$、或者下划线开始 首字符之后可以是字母、$、下划线或者数字的任意组合 关键字不能做标识符 大小写敏感 Java修饰符 访问控制修饰符 : default, public , protected, private 非访问控制修饰符 : final, abstract, static, synchronized Java变量 局部变量 类变量（静态） 成员变量（非静态变量） Java注释12345678/* 这是第一个Java程序* 它将输出 Hello World* 这是一个多行注释的示例*///单行/*单行 */ 继承在 Java 中，一个类可以由其他类派生。如果你要创建一个类，而且已经存在一个类具有你所需要的属性或方法，那么你可以将新创建的类继承该类。 利用继承的方法，可以重用已存在类的方法和属性，而不用重写这些代码。被继承的类称为超类（super class），派生类称为子类（sub class）。 Java对象和类Java作为一种面向对象语言。支持以下基本概念： 多态 继承 封装 抽象 类 对象 实例 方法 重载 以下图为例，图中汽车就是一个“类”，而具体的每辆车为该汽车类的对象，对象包含了的他的各种属性：颜色、品牌、名称…… 对象现在让我们深入了解什么是对象。看看周围真实的世界，会发现身边有很多对象，车，狗，人等等。所有这些对象都有自己的状态和行为。 拿一条狗来举例，它的状态有：名字、品种、颜色，行为有：叫、摇尾巴和跑。 对比现实对象和软件对象，它们之间十分相似。 软件对象也有状态和行为。软件对象的状态就是属性，行为通过方法体现。 在软件开发中，方法操作对象内部状态的改变，对象的相互调用也是通过方法来完成。 类类可以看成是创建 Java 对象的模板 代码示例： 123456789101112131415161718public class Dog { String breed; int size; String colour; int age; void eat() { } void run() { } void sleep(){ } void name(){ }} 一个类可以包含以下类型变量： 局部变量：在方法、构造方法或者语句块中定义的变量被称为局部变量。变量声明和初始化都是在方法中，方法结束后，变量就会自动销毁。 成员变量：成员变量是定义在类中，方法体之外的变量。这种变量在创建对象的时候实例化。成员变量可以被类中方法、构造方法和特定类的语句块访问。 类变量：类变量也声明在类中，方法体之外，但必须声明为 static 类型。 一个类可以拥有多个方法，在上面的例子中：eat()、run()、sleep() 和 name() 都是 Dog 类的方法。 创建对象对象是根据类创建的。在Java中，使用关键字 new 来创建一个新的对象。创建对象需要以下三步： 声明：声明一个对象，包括对象名称和对象类型。 实例化：使用关键字 new 来创建一个对象。 初始化：使用 new 创建对象时，会调用构造方法初始化对象。 代码实例： 12345678910public class Puppy{ public Puppy(String name){ //这个构造器仅有一个参数：name System.out.println(&quot;小狗的名字是 : &quot; + name ); } public static void main(String[] args){ // 下面的语句将创建一个Puppy对象 Puppy myPuppy = new Puppy( &quot;tommy&quot; ); }} 运行结果： 1小狗的名字是 : tommy 访问实例变量和方法通过已创建的对象来访问成员变量和成员方法，如下所示： 123456/* 实例化对象 */Object referenceVariable = new Constructor();/* 访问类中的变量 */referenceVariable.variableName;/* 访问类中的方法 */referenceVariable.methodName(); 展示如何访问实例变量和调用成员方法例子123456789101112131415161718192021222324252627public class Puppy{ int puppyAge; public Puppy(String name){ // 这个构造器仅有一个参数：name System.out.println(&quot;小狗的名字是 : &quot; + name ); } public void setAge( int age ){ puppyAge = age; } public int getAge( ){ System.out.println(&quot;小狗的年龄为 : &quot; + puppyAge ); return puppyAge; } public static void main(String[] args){ /* 创建对象 */ Puppy myPuppy = new Puppy( &quot;tommy&quot; ); /* 通过方法来设定age */ myPuppy.setAge( 2 ); /* 调用另一个方法获取age */ myPuppy.getAge( ); /*你也可以像下面这样访问成员变量 */ System.out.println(&quot;变量值 : &quot; + myPuppy.puppyAge ); }} 运行结果： 123小狗的名字是 : tommy小狗的年龄为 : 2变量值 : 2 源文件声明规则 一个源文件中只能有一个 public 类 一个源文件可以有多个非 public 类 源文件的名称应该和 public 类的类名保持一致。例如：源文件中 public 类的类名是 Employee，那么源文件应该命名为Employee.java。 如果一个类定义在某个包中，那么 package 语句应该在源文件的首行。 如果源文件包含 import 语句，那么应该放在 package 语句和类定义之间。如果没有 package 语句，那么 import 语句应该在源文件中最前面。 import 语句和 package 语句对源文件中定义的所有类都有效。在同一源文件中，不能给不同的类不同的包声明。 import语句在 Java 中，如果给出一个完整的限定名，包括包名、类名，那么 Java 编译器就可以很容易地定位到源代码或者类。import 语句就是用来提供一个合理的路径，使得编译器可以找到某个类。 参考博客 Java 对象和类","link":"/2022/01/15/java2/"},{"title":"数学建模|多元分析（一）","text":"多元分析是多变量的统计分析方法。 聚类分析聚类分析一般分为Q型聚类分析和R型聚类分析。 Q型聚类分析是指对样品进行聚类分析 R型聚类分析是指对变量进行聚类。 根据处理方法的不同聚类分析又分为系统聚类法、有序样品聚类法、动态聚类法、模糊聚类法、图论聚类法。 聚类的一般过程： 数据预处理（标准化） 构造关系矩阵（亲疏关系的描述） 聚类（根据不同方法进行分类） 确定最佳分类（类别数） Q型聚类分析样本相似性度量要用数量化的方法对事物进行分类，就必须用数量化的方法描述事物之间的相似程度。一个事物常常需要用多个变量来刻画。如果对于一群有待分类的样本点需用个变量描述，则每个样本点可以看成是 空间中的一个点。因此，很自然地想到可以用 距离来度量样本点间的相似程度。 记是样本点集，距离是的一个函数，满足条件： (1) d(x,y)\\geq0,x,y\\in\\Omega\\\\ (2) d(x,y)=0当且仅当x=y (3) d(x,y)=d(y,x),x,y\\in\\Omega (4) d(x,y)\\leq d(x,z)+d(z,y),x,y,z\\in\\Omega这一距离的定义是我们所熟知的，它满足正定性，对称性和三角不等式。在聚类分析中，对于定量变量，常用的闵氏(Minkowski)距离、绝对值距离、欧氏距离、切比雪夫距离。 闵氏距离 d_q(x,y)=[\\sum_{k=1}^p|x_k-y_k|^q]^{\\frac{1}{q}},q>0 在 Minkowski 距离中，常用的是欧氏距离，它的主要优点是当坐标轴进行正交旋转时，欧氏距离是保持不变的。因此，如果对原坐标系进行平移和旋转变换，则变换 后样本点间的距离和变换前完全相同。值得注意的是在采用 Minkowski距离时，一定要采用相同量纲的变量。如果变量的量纲不同，测量值变异范围相差悬殊时，建议首先进行数据的标准化处理，然后再计算距离。在采用 Minkowski 距离时，还应尽可能地避免变量的多重相关性(multicollinearity)。多重相关性所造成的信息重叠，会片面强调某些变量的重要性。 由于 Minkowski 距离的这些缺点，一种改进的距离就是马氏距离，定义如下: 马氏距离 d(x,y)=\\sqrt{(x-y)^T\\Sigma^{-1}(x-y)}其中为来自维总体的样本观测值；为的协方差矩阵，实际中往往是不知道的，常常需要用样本协方差来估计。马氏距离对一切线性变换是不变的，故不受量纲的影响。 类与类间的相似性度量 如果有两个样本类和，可以用下面的一系列方法度量它们间的距离： (1)最短距离法 (2)最长距离法 (3)重心法 (4)类平均法 (5)离差平方和法 事实上，若$G1,G_2内部点与点距离很小，则它们能很好地各自聚为一类，并且这两类又能够充分分离（即D{12}很大），这时必然有D=D_{12}-D_1-D_2很大。因此，按定义可以认为，两类G_1,G_2$之间的距离很大。离差平方和法初是由 Ward 在 1936 年提出，后经 Orloci 等人 1976 年发展起来的，故又称为 Ward 方法。 聚类图Q型聚类结果可由一个聚类图展示出来： 生成聚类图 案例 Q型聚类例子参考博客：matlab Q型聚类分析例题Q型聚类分析 R型聚类变量相似性度量对变量进行聚类分析，首先要确定变量的相似性度量，常用的变量相似性度量有两张： 相关系数 对变量进行聚类分析的时候，利用相关系数矩阵是最多的。 夹角余弦 各种定义的相似度量均应具有以下两个性质： 变量聚类法类似样本集合聚类分析中最常用的最短距离法、最长距离法，变量聚类法采用了与系统聚类法相同的思路和过程。在变量聚类问题中，常用的有最长距离法、最短距离法。 最长距离法在最长距离法中，定义两类变量的距离为： R(G_1，G_2)=max_{x_j\\in G_1,x_k\\in G_2}|d_{jk}|式子中：$d{jk}=1-|r{jk}|或者d{jk}^2=1-r{jk}^2，这时，R(G_1，G_2)$与两类中相似性最小的两变量间的相似性度量值有关。 最短距离法在最短距离法中，定义两类变量的距离为： R(G_1，G_2)=min_{x_j\\in G_1,x_k\\in G_2}|d_{jk}|式子中：$d{jk}=1-|r{jk}|或者d{jk}^2=1-r{jk}^2，这时，R(G_1，G_2)$与两类中相似性最大的两变量间的相似性度量值有关。 案例 R型聚类分析 聚类算法分类： 主成分分析(PCA)主成分分析的主要目的是希望用较少的变量去解释原来资料中的大部分变异，将我们手中许多相关性很高的变量转化成彼此相互独立或不相关的变量。从数学的角度来说，是一种降维处理技术，降维可以去除噪声和不重要的特征，可以提升数据处理的速度。 基本思想如果用表示门课程，表示各门课程的权重，那么加权之和是： s=c_1x_1+c_2x_2+···+c_px_p我们希望选择合适的权重能更好的区分学生的成绩。每个学生都对应一个综合成绩，记为为学生人数，如果这些值很分散，则表明区分的很好，但是现实情况往往是不能很好的分散，所以需要找到一个加权方式来使得尽可能的分散。有如下的统计定义：设表示以为样本观测值的随机变量，如果能找到一组权重3，使得 Var(c_1X_1+c_2X_2+···+c_pX_p)的值达到最大，则由于方差反应了数据差异的程度，也就表明了我们抓住了个变量的最大变异。当然，该式子也必须加上某种限制，不然权值无限大就没有意义了，一般来说规定 c_1^2+c_2^2+···+c_p^2=1在此约束条件下，求得的最优解是纬空间的一个单位向量，它所表示的方向，就是主成分的方向。实际运用中，一个主成分往往不能代表原来的变量，所以我们需要寻找第二个、第三个……，且新寻找的主成分不再包含旧主成分的信息。设表示第个成分，,则可以表示为： \\begin{equation} \\left\\{ \\begin{array}{ll} Z_1=c_{11}X_1+c_{12}X_2+···+c_{1p}X_p\\\\ Z_2=c_{21}X_1+c_{22}X_2+···+c_{2p}X_p\\\\\\\\ ·····\\\\\\\\ Z_p=c_{p1}X_1+c_{p2}X_2+···+c_{pp}X_p\\\\ \\end{array}\\right. \\end{equation}且对每个，均有$c{i1}^2+c{i2}^2+···+c_{ip}^2=1$ 基本方法假设有n个样本，p个指标，则可以构成大小为的矩阵 x={\\left[ {\\begin{matrix} x_{11}&x_{12}&···&x_{1p}\\\\ x_{21}&x_{22}&···&x_{2p}\\\\···\\\\ x_{n1}&x_{n2}&···&x_{np}\\\\ \\end{matrix}} \\right]=(x_1,x_2,···,x_p)}1. 进行标准化处理按列计算均值$\\bar{xj}=\\frac{1}{n}\\sum{i=1}^nx_{ij}$和标准差 S_j=\\sqrt{\\frac{\\sum_{i=1}^n(x_{ij}-\\bar{x_j})}{n-1}}再标准化数据$X{ij}=\\frac{x{ij}-\\bar{x_j}}{S_j}$，原始样本矩阵经过标准化为： X={\\left[ {\\begin{matrix} X_{11}&X_{12}&···&X_{1p}\\\\ X_{21}&X_{22}&···&X_{2p}\\\\···\\\\ X_{n1}&X_{n2}&···&X_{np}\\\\ \\end{matrix}} \\right]=(X_1,X_2,···,X_p)}2. 计算标准化样本的协方差矩阵 R={\\left[ {\\begin{matrix} r_{11}&r_{12}&···&r_{1p}\\\\ r_{21}&r_{22}&···&r_{2p}\\\\···\\\\ r_{n1}&r_{n2}&···&r_{np}\\\\ \\end{matrix}} \\right]}其中 r_{ij}=\\frac{1}{n-1}\\sum_{k=1}^n(X_{ki}-\\bar{X_i})(X_{kj}-\\bar{X_j})=\\frac{1}{n-1}\\sum_{k=1}^nX_{ki}X_{kj}其实这两步可以合成一步， R=\\frac{\\sum_{k=1}^n(x_{ki}-\\bar{x_i})(x_{kj}-\\bar{x_j})}{\\sqrt{\\sum_{k=1}^n(x_{ki}-\\bar{x_i})^2\\sum_{k=1}^n(x_{kj}-\\bar{x_j})^2}}3. 计算的特征值和特征向量特征值： $\\lambda1\\geq\\lambda_2\\geq···\\geq\\lambda_p\\geq 0R是半定矩阵，且tr(R)=\\sum{k=1}^p\\lambda_k=p$) 特征向量： a_1={\\left[ {\\begin{matrix} a_{11}\\\\ a_{21}\\\\···\\\\ a_{p1} \\end{matrix}} \\right],a_2={\\left[ {\\begin{matrix} a_{12}\\\\ a_{22}\\\\···\\\\ a_{p2} \\end{matrix}} \\right],···,a_p={\\left[ {\\begin{matrix} a_{1p}\\\\ a_{2p}\\\\···\\\\ a_{pp} \\end{matrix}} \\right]}}}Matlab中计算特征值和特征向量的函数： 4. 计算主成分贡献率以及累计贡献率贡献率： \\frac{\\lambda_i}{\\sum_{k=1}^p},(i=1,2,···,p)累计贡献率： \\frac{\\sum_{k=1}^i\\lambda_k}{\\sum_{k=1}^p\\lambda_k},(i=1,2,···,p)5.写出主成分一般取累计贡献率超过的特征值所对应的第一、第二、……、第m个主成分,第个主成分： F_i=a_{1i}X_1+a_{2i}X_2+···+a_{pi}X_p,(i=1,2,···,m)6. 根据系数分析主成分代表的意义简而言之，系数越大，影响越大 主成分分析回归主成分可用于聚类分析、回归分析。 例如Hald水泥问题，参考这里：水泥问题及matlab代码在文章尾部 案例 数学建模常用模型08 ：主成分分析 数学建模—主成分分析 数学建模算法 一 简述（4）主成分分析（PCA） 因子分析(FA)因子分析可以看做是主成分分析的推广，也是利用降维的思想，由研究原始变量相关矩阵或协方差矩阵的内部依赖关系出发，把一些具有错综复杂关系的多个变量归 结为少数几个综合因子的一种多元统计分析方法。 基本思想把每个研究变量分解为几个影响因素变量，将每个原始变量分解成两部分因素，一部分是由所有变量共同具有的少数几个公共因子组成的，另一部分是每个变量独自具有的因素，即特殊因子。 与主成分分析的区别 主成分分析模型是原始变量的线性组合，是将原始变量加以综合、归纳，仅仅是变量变换；而因子分析是将原始变量加以分解，描述原始变量协方差矩阵结构的模型；只有当提取的公因子个数等于原始变量个数时，因子分析才对应变量变换。 主成分分析中每个主成分对应的系数是唯一确定的；因子分析中每个因子的相应系数即因子载荷不是唯一的。 因子分析中因子载荷的不唯一性有利于对公因子进行有效解释； 而主成分分析对提取的主成分的解释能力有限。 因子分析模型 因子载荷矩阵中的几个统计性质 因子载荷的统计意义因子载荷是第个变量与个公共因子的相关系数，反映了第个变量与第个公共因子的相关重要性，绝对值越大，相关的密切程度越高。 变量共同度的统计意义变量$Xi的共同度是因子载荷矩阵的第i行的元素的平方和，记为h_i^2=\\sum{j=1}^ma_{ij}^2$,对两边求方差，有 Var(X_i)=\\alpha_{i1}^2Var(F_1)+···+\\alpha_{im}^2Var(F_m)+Var(\\varepsilon_i)即 1=\\sum_{j=1}^m\\alpha_{ij}^2+\\sigma_i^2 可以看出所有的公共因子对特殊因子对变量的贡献为，如果非常接近，说明从原变量空间到公共因子空间的转化效果好。 公共因子方差贡献的统计意义因子载荷矩阵中各列元素的平方和S_j=\\sum_{i=1}^p\\alpha_{ij}^2\\则称对所有的的方差贡献和，用于衡量的相对重要性。 因子载荷矩阵的估计方法因子分析的一个基本问题是如何估计因子载荷，即如何求解因子模型式，下面介绍常用的因子载荷矩阵的估计方法。 主成分分析法设为样本相关系数矩阵的特征值，为相应的标准正交化特征向量，设，则因子的荷载矩阵为 实例参考（含matlab代码）：因子分析—建立载荷矩阵 主因子法主因子法是对主成分法的修正，首先对变量标准化 R=\\Lambda \\Lambda^T+D,D=diag\\{\\sigma_1^2,···,\\sigma_m^2\\}。记 R^*=\\Lambda\\Lambda^T=R-D式中，$R^为约相关系数矩阵，R^对角线上的元素是h_i^2$。在实际情况中对特殊因子的方差一般是未知的，可以通过一组样本来估计，估计的方法如下： 取，在这种情况下主因子解与主成分解等价 取$\\hat{hi^2}=max{j\\neq i}|r_{ij}|这意味着X_i与其余的X_j$的简单相关系数的绝对值最大者。记作 直接求$R^的前p个特征值\\lambda_1^\\geq\\lambda_2^\\geq···\\geq\\lambda_p^\\geq \\lambda_p和对应的正交特征向量u_1^,u_2^,··u_p^*$。得到如下的因子载荷矩阵: 极大似然估计Matlab工具箱求因子载荷矩阵使用的是最大似然估计法，命令是 案例数学建模常用模型14 ：因子分析 因子旋转（正交变换）为什么要旋转因子？建立了因子分析数学目的不仅仅要找出公共因子以及对变量进行分组，更重要的要知道每个公共因子的意义，以便进行进一步的分析，如果每个公共因子的含义不清，则不便于进行实际背景的解释。由于因子载荷阵是不唯一的，所以应该对因子载荷阵进行旋转。目的是使每个变量在尽可能少的因子上有比较高的载荷，让某个变量在某个因子上的载荷趋于1，而在其他因子上的载荷趋于0。即：使载荷矩阵每列或行的元素平方值向0和1两极分化。旋转的方法有：正交旋转；斜交旋转 正交旋转由初始载荷矩阵A左乘一正交矩阵得到；目的是新的载 荷系数尽可能的接近于0或尽可能的远离0；只是在旋 转后的新的公因子仍保持独立性。 方差最大法： 方差最大法从简化因子载荷矩阵的每一列出发，使和每个因子有关的载荷的平方的方差最大。当只有少数几个变量在某个因子上有较高的载荷时，对因子的解释最简单。方差最大的直观意义是希望通过因子旋转后，使每个因子上的载荷尽量拉开距离，一部分的载荷趋于 1，另一部分趋于0。 四次方最大旋转： 四次方最大旋转是从简化载荷矩阵的行出发，通过旋转 初始因子，使每个变量只在一个因子上有较高的载荷， 而在其它的因子上尽可能低的载荷。如果每个变量只在 一个因子上有非零的载荷，这时的因子解释是最简单的。 四次方最大法通过使因子载荷矩阵中每一行的因子载 荷平方的方差达到最大。 等量最大法： 等量最大法把四次方最大法和方差最大法结合起来求行和列因子载荷平方的方差的加权平均最大。 斜交旋转目的是新的载荷系数尽可能的接近于0或尽可能的远离0；只是在旋转时，放弃了因子之间彼此独立的限制，旋转后的新公因子更容易解释。主要有以下的方法： 直接斜交旋转。允许因子之间具有相关性； 斜交旋转方法。允许因子之间具有相关性； 具体算法过程参考因子分析法之因子旋转 因子得分积分因子的概念前面我们主要解决了用公共因子的线性组合来表示一组观测变量的有关问题。如果我们要使用这些因子做其他的研究，比如把得到的因子作为自变量来做回归分析，对样 本进行分类或评价，这就需要我们对公共因子进行测度，即给出公共因子的值。 因子分析数学模型 原变量被表示为公共因子的线性组合，当载荷矩阵旋转之后，公共因子可以做出解释，通常的情况下，我们还想反过来把公共因子表示为原变量的线性组合。 得分因子函数： 可见，要求得每个因子的得分，必须求得分函数的系数，而由于 p &gt; m，所以不能得到精确的得分，只能通过估计。 巴特莱特因子得分（加权最小二乘法）把看作因变量，把因子载荷矩阵 还可参考以下博客： 因子分析 factor analysis (五) ： 因子得分 因子分析——因子得分 案例因子分析 factor analysis (六) ：用因子分析法进行综合评价 因子分析步骤与主成分分析对比相同： 指标标准化 相关系数矩阵及其特征值和特征向量 用累计贡献率确定主成分和因子个数m 单个主成分与综合主成分的分析评价、单因子与综合因子的分析评价步骤 不同之处： 参考博客 参考博客： 多元统计分析 （一）：聚类分析 数学建模—主成分分析 数学建模常用模型14 ：因子分析","link":"/2022/01/13/jm4/"},{"title":"数学建模|多元分析（二）","text":"判别分析判别问题用统计的语言来表达，就是已有个总体，它们的分布函数分别为,每个都是维函数。对于给定的样本，要判断它来自哪一个总体。当然，应该要求判别准则在某种意义下是优的，例如错判的概率小或错判的损失小等。我们仅介绍基本的几种判别方法，即距离判别，Bayes判别和Fisher判别。 距离判别Mahalanobis 距离的概念通常我们定义的距离是Euclid距离（欧式距离），但是在统计分析里就不适用了。 Mahalanobis 距离（马氏距离）的定义 距离判别的判别准则在这里讨论两个总体的距离判别，分协方差相同和协方差不同两种进行讨论。 两总体距离的判别函数 待测样本的判别函数与判别准则 Fisher判别Fisher判别的基本思想就是投影，将表面不易分类的数据通过投影到某个方向上使得投影类与类之间得以分离的一种判别方法。 当总体的参数未知时，我们用样本对 $\\mu{1} ,\\mu {2}及\\Sigma$进行估计，注意到这里的 Fisher 判别与距离判别一样,不需要知道总体的分布类型，但两总体的均值向量必须有显著的差 异才行，否则判别无意义。 Bayes判别Bayes判别和Bayes估计的思想方法是一样的，即假定对研究的对象已经有一定的认识，这种认识常用先验概率来描述，当我们取得一个样本后，就可以用样本来修正已有的先验概率分布，得出后验概率分布，再通过后验概率分布进行各种统计推断。 误判概率与误判损失设有两个总体和，根据某一个判别规则，将实际上为的个体判为 或者将实际上为的个体判为的概率就是误判概率，一个好的判别规则应该使误判概率最小。除此之外还有一个误判损失问题或者说误判产生的花费（Cost）问题，如把的个体误判到 的损失比的个体误判到严重得多，则人们在作前一种判断时就要特别谨慎。譬如在药品检验中把有毒的样品判为无毒后果比无毒样品判为有毒严重得多，因此一个好的判别规则还必须使误判损失最小。 总体的Bayes判别 建立 Anderson 线性判别函数 Matlab解决123456789101112131415161718clc,cleara=[24.8 24.1 26.6 23.5 25.5 27.4-2.0 -2.4 -3.0 -1.9 -2.1 -3.1]';b=[22.1 21.6 22.0 22.8 22.7 21.5 22.1 21.4-0.7 -1.4 -0.8 -1.6 -1.5 -1.0 -1.2 -1.3]';n1=6;n2=8;mu1=mean(a);mu2=mean(b);mu1=mu1',mu2=mu2's1=(n1-1)*cov(a),s2=(n2-1)*cov(b)sigma2=(s1+s2)/(n1+n2-2)beta=log(8/6)syms x1 x2x=[x1;x2];wx=(x-0.5*(mu1+mu2)).'*inv(sigma2)*(mu1-mu2);digits(6),wx=vpa(wx)ahat=subs(wx,{x1,x2},{a(:,1),a(:,2)})bhat=subs(wx,{x1,x2},{b(:,1),b(:,2)}) 以下是Σ1 ≠ Σ2 情形下的 MATLAB 程序：123456789101112131415161718192021clc,clearp1=6/14;p2=8/14;a=[24.8 24.1 26.6 23.5 25.5 27.4-2.0 -2.4 -3.0 -1.9 -2.1 -3.1]';b=[22.1 21.6 22.0 22.8 22.7 21.5 22.1 21.4-0.7 -1.4 -0.8 -1.6 -1.5 -1.0 -1.2 -1.3]'; n1=6;n2=8;mu1=mean(a);mu2=mean(b);mu1=mu1',mu2=mu2'cov1=cov(a),cov2=cov(b)k=log(p2/p1)+0.5*log(det(cov1)/det(cov2))+0.5*(mu1'*inv(cov1)*mu1-mu2'*inv(cov2)*mu2)syms x1 x2x=[x1;x2];wx=-0.5*x.'*(inv(cov1)-inv(cov2))*x+(mu1'*inv(cov1)-mu2'*inv(cov2))*x;digits(6),wx=vpa(wx);wx=simple(wx)ahat=subs(wx,{x1,x2},{a(:,1),a(:,2)})bhat=subs(wx,{x1,x2},{b(:,1),b(:,2)})ahat&gt;=k,bhat&lt;k 典型相关分析通常情况下，为了研究两组变量: 的相关关系，可以用最原始的方法，分别计算两组变量之间的全部相关系数，一共有p × q p\\times qp×q个简单相关系数，这样又烦琐又不能抓住问题的本质。如果能够采用类似于主成分的思想，分别找出两组变量的各自的某个线性组合，讨论线性组合之间的相关关系，则更简捷。 因此，典型相关分析是分析两组变量之间的相关性的一种统计方法，它包含了简单的Pearson相关分析（两组均只含一个变量）和复相关（一组只含一个变量，另一个组含多个变量）这两种特殊情况。 典型相关分析的基本思想和主成分分析的基本思想相似，它将一组变量与另一组变量之间单变量的多重线性相关性研究，转换为少数几对综合变量之间的简单线性相关性的研究，并且这少数几对变量所包含的线性相关性的信息几乎覆盖了原变量组所包含的全部相应信息。 基本思想假设所研究的两组变量为X组和Y组，其中X组有p个变量 X=(x_1,x_2,···,x_p)Y组有q个变量 Y=(y_1,y_2,···,y_q)则分别对这两组变量做线性组合后，再计算它们的加权和的简单相关系数，以这个简单相关系数当做这两组变量之间相关性的度量指标，即 其中，u 和 v 分别是由 x 变量和 y 变量的线性组合产生的综合逐步变量。显然，对任意的一组系数 都可以通过上式求出一对典型变量 u 和 v，在典型相关分析中称之为典型变量。进而可以求出典型变量 u 和 v 的简单相关系数，称之为典型相关系数。那么，问题来了，怎么进行组合呢？ 首先，分别在每组变量中找出第一对线性组合 使其具有最大相关性，即使得对应的典型变量和的相关系数为最大。假设这个最大的 p_1=p(u_1,v_1)则称为第1典型相关系数，且称具有最大相关系数的这对典型变量和为第1典型变量。 然后再次估计组合系数，在每组变量中找出第二对线性组合，使其分别与本组内的第一线性组合不相关，第二对本身具有次大的相关性 假设这个次大的相关系数是 p_2=p(u_2,v_2)则称为第2典型相关系数，且称这对典型变量和为第2典型变量。 其中，和与和相互独立，但和相关。如此继续下去，直至进行到 r 步，两组变量的相关性被提取完为止。 r\\leq min(p,q)可以得到 r 组变量。 从上述分析的过程可以看出，第1对典型变量的第1典型相关系数描述了两个组中变量之间的相关程度，且它提取的有关这两组变量相关性的信息景最多。 第2对典型变量的第2典型相关系数也描述了两个组中变量之间的相关程度，但它提取的有关这两组变量相关性的信总量次多。 依次类推，可以得知，由上述方法得到的一系列典型变量的典型相关系数，所包含的有关原变量组之间相关程度的信息一个比一个少，如果少数几对典型变量就能够解释原数据的主要信息，特别是如果一对典型变量就能够反映出原数据的主要信息，那么，对两个变量组之间相关程度的分析就可以转化为对少数几对或者是一对典型变量的简单相关分析，这就是典型相关分析的主要目的。 典型相关分析的理论以及基本假设考虑两组变量的向量 典型相关分析就是寻找 x 组 的线性组合$u1=a{11}x1+a{21}x2+···+a{p1}xp与组的线性组合v_1=b{11}y1+b{21}y2+···+b{q1}y_q，使得u_1和v_1$之间的简单相关系数为最大，其中 设，x 组与 y 组的协方差阵为 典型相关系数的检验那么，要选择多少组典型变量呢？ 在做两组变量，的典型相关分析之前，首先应该检验两组变量是否相关，如果不相关，则讨论两组变量的典型相关就毫无意义. 最多可以选取组，可经由卡方检验决定要选取多少组典型变量。先检验最大的典型根，然后再一个接一个对各个根进行检验，只保留有统计显著性（就是拒绝原假设）的根。 1. 提出假设 2. 当上述原假设被拒绝时，接着做 3. 当上述原假设被拒绝时,接着做 H_0:\\lambda _3=···=\\lambda _r=0···· 4. 依此类推 案例 典型相关分析方法及案例介绍 相关性分析｜典型相关性分析 对应分析对应分析原理案例参考博客 判别分析 （ distinguish analysis）(一)：距离判别 典型相关分析（canonical correlation analysis，CCA）","link":"/2022/01/14/jm5/"},{"title":"Java学习笔记|数据类型","text":"数据类型如下图，java的数据类型 Java基本数据类型其中： 整数类型：byte，1字节，8位，最大存储数据量是255，存放的数据范围是-128~127之间。 整数类型：short，2字节，16位，最大数据存储量是65536，数据范围是-32768~32767之间。 整数类型：int，4字节，32位，最大数据存储容量是2的32次方减1，数据范围是负的2的31次方到正的2的31次方减1。 整数类型：long，8字节，64位，最大数据存储容量是2的64次方减1，数据范围为负的2的63次方到正的2的63次方减1。 浮点类型：float，4字节，32位，数据范围在3.4e-45~1.4e38，直接赋值时必须在数字后加上f或F。 浮点类型：double，8字节，64位，数据范围在4.9e-324~1.8e308，赋值时可以加d或D也可以不加。 字符型：char，2字节，16位，存储Unicode码，用单引号赋值。 布尔型：boolean，只有true和false两个取值 1 byte=8 bit，bit是最小单位，1 B=8 bit Java基本数据类型封装器类图 代码示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class PrimitiveTypeTest { public static void main(String[] args) { // byte System.out.println(&quot;基本类型：byte 二进制位数：&quot; + Byte.SIZE); System.out.println(&quot;包装类：java.lang.Byte&quot;); System.out.println(&quot;最小值：Byte.MIN_VALUE=&quot; + Byte.MIN_VALUE); System.out.println(&quot;最大值：Byte.MAX_VALUE=&quot; + Byte.MAX_VALUE); System.out.println(); // short System.out.println(&quot;基本类型：short 二进制位数：&quot; + Short.SIZE); System.out.println(&quot;包装类：java.lang.Short&quot;); System.out.println(&quot;最小值：Short.MIN_VALUE=&quot; + Short.MIN_VALUE); System.out.println(&quot;最大值：Short.MAX_VALUE=&quot; + Short.MAX_VALUE); System.out.println(); // int System.out.println(&quot;基本类型：int 二进制位数：&quot; + Integer.SIZE); System.out.println(&quot;包装类：java.lang.Integer&quot;); System.out.println(&quot;最小值：Integer.MIN_VALUE=&quot; + Integer.MIN_VALUE); System.out.println(&quot;最大值：Integer.MAX_VALUE=&quot; + Integer.MAX_VALUE); System.out.println(); // long System.out.println(&quot;基本类型：long 二进制位数：&quot; + Long.SIZE); System.out.println(&quot;包装类：java.lang.Long&quot;); System.out.println(&quot;最小值：Long.MIN_VALUE=&quot; + Long.MIN_VALUE); System.out.println(&quot;最大值：Long.MAX_VALUE=&quot; + Long.MAX_VALUE); System.out.println(); // float System.out.println(&quot;基本类型：float 二进制位数：&quot; + Float.SIZE); System.out.println(&quot;包装类：java.lang.Float&quot;); System.out.println(&quot;最小值：Float.MIN_VALUE=&quot; + Float.MIN_VALUE); System.out.println(&quot;最大值：Float.MAX_VALUE=&quot; + Float.MAX_VALUE); System.out.println(); // double System.out.println(&quot;基本类型：double 二进制位数：&quot; + Double.SIZE); System.out.println(&quot;包装类：java.lang.Double&quot;); System.out.println(&quot;最小值：Double.MIN_VALUE=&quot; + Double.MIN_VALUE); System.out.println(&quot;最大值：Double.MAX_VALUE=&quot; + Double.MAX_VALUE); System.out.println(); // char System.out.println(&quot;基本类型：char 二进制位数：&quot; + Character.SIZE); System.out.println(&quot;包装类：java.lang.Character&quot;); // 以数值形式而不是字符形式将Character.MIN_VALUE输出到控制台 System.out.println(&quot;最小值：Character.MIN_VALUE=&quot; + (int) Character.MIN_VALUE); // 以数值形式而不是字符形式将Character.MAX_VALUE输出到控制台 System.out.println(&quot;最大值：Character.MAX_VALUE=&quot; + (int) Character.MAX_VALUE); } } 运行结果： 12345678910111213141516171819202122232425262728293031323334基本类型：byte 二进制位数：8包装类：java.lang.Byte最小值：Byte.MIN_VALUE=-128最大值：Byte.MAX_VALUE=127基本类型：short 二进制位数：16包装类：java.lang.Short最小值：Short.MIN_VALUE=-32768最大值：Short.MAX_VALUE=32767基本类型：int 二进制位数：32包装类：java.lang.Integer最小值：Integer.MIN_VALUE=-2147483648最大值：Integer.MAX_VALUE=2147483647基本类型：long 二进制位数：64包装类：java.lang.Long最小值：Long.MIN_VALUE=-9223372036854775808最大值：Long.MAX_VALUE=9223372036854775807基本类型：float 二进制位数：32包装类：java.lang.Float最小值：Float.MIN_VALUE=1.4E-45最大值：Float.MAX_VALUE=3.4028235E38基本类型：double 二进制位数：64包装类：java.lang.Double最小值：Double.MIN_VALUE=4.9E-324最大值：Double.MAX_VALUE=1.7976931348623157E308基本类型：char 二进制位数：16包装类：java.lang.Character最小值：Character.MIN_VALUE=0最大值：Character.MAX_VALUE=65535 引用数据类型 在Java中，引用类型的变量非常类似于C/C++的指针。引用类型指向一个对象，指向对象的变量是引用变量。这些变量在声明时被指定为一个特定的类型，比如 Employee、Puppy 等。变量一旦声明后，类型就不能被改变了。 对象、数组都是引用数据类型。 所有引用类型的默认值都是null。 一个引用变量可以用来引用任何与之兼容的类型。 例子：Site site = new Site(“Runoob”)。 强制数据转换1. 条件是转换的数据类型必须是兼容的。 2. 格式：(type)value type是要强制类型转换后的数据类型。 示例： 1234567public class QiangZhiZhuanHuan{ public static void main(String[] args){ int i1 = 123; byte b = (byte)i1;//强制类型转换为byte System.out.println(&quot;int强制类型转换为byte后的值等于&quot;+b); }} 结果示例： 1int强制类型转换为byte后的值等于123","link":"/2022/01/16/java3/"},{"title":"Java学习笔记|变量","text":"在java中，每个变量都有一个类型，声明变量类型时，变量的类型位于变量名前： 示例： 1234double salary;int days;long population;boolean done; 声明变量的基本格式： 1type identifier [ = value][, identifier [= value]...] 其中type为Java数据类型，identifier是变量名。 支持的变量类型 局部变量：类的方法中的变量 实例变量：独立于方法之外的变量，不过没有static修饰。 类变量：独立于方法之外的变量，用static修饰 实例： 1234567public class Variable{ static int allClicks=0;//类变量 String str='hello world';//实例变量 public void method(){ int i = 0;//局部变量 }} 局部变量 局部变量声明在方法、构造方法或者语句块中； 局部变量在方法、构造方法、或者语句块被执行的时候创建，当它们执行完成后，变量将会被销毁； 访问修饰符不能用于局部变量； 局部变量只在声明它的方法、构造方法或者语句块中可见； 局部变量是在栈上分配的； 局部变量没有默认值，所以局部变量被声明后，必须经过初始化，才可以使用。 实例： 在以下实例中 age 是一个局部变量。定义在 pupAge() 方法中，它的作用域就限制在这个方法中。 123456789101112public class Test{ public void pupAge(){ int age = 0; age = age + 7; System.out.println(&quot;Puppy age is : &quot; + age); } public static void main(String args[]){ Test test = new Test(); test.pupAge(); }} 以上实例编译运行结果如下： 1Puppy age is: 7 实例变量 实例变量声明在一个类中，但在方法、构造方法和语句块之外； 当一个对象被实例化之后，每个实例变量的值就跟着确定； 实例变量在对象创建的时候创建，在对象被销毁的时候销毁； 实例变量的值应该至少被一个方法、构造方法或者语句块引用，使得外部能够通过这些方式获取实例变量信息； 实例变量可以声明在使用前或者使用后； 访问修饰符可以修饰实例变量； 实例变量对于类中的方法、构造方法或者语句块是可见的。一般情况下应该把实例变量设为私有。通过使用访问修饰符可以使实例变量对子类可见； 实例变量具有默认值。数值型变量的默认值是0，布尔型变量的默认值是 false，引用类型变量的默认值是 null。变量的值可以在声明时指定，也可以在构造方法中指定； 实例变量可以直接通过变量名访问。但在静态方法以及其他类中，就应该使用完全限定名：ObejectReference.VariableName。 实例： 123456789101112131415161718192021222324252627import java.io.*;public class Employee{ // 这个成员变量对子类可见 public String name; // 私有变量，仅在该类可见 private double salary; //在构造器中对name赋值 public Employee (String empName){ name = empName; } //设定salary的值 public void setSalary(double empSal){ salary = empSal; } // 打印信息 public void printEmp(){ System.out.println(&quot;name : &quot; + name ); System.out.println(&quot;salary :&quot; + salary); } public static void main(String args[]){ Employee empOne = new Employee(&quot;Ransika&quot;); empOne.setSalary(1000); empOne.printEmp(); }} 以上实例编译运行结果如下: 12name : Ransikasalary :1000.0 类变量（静态变量） 类变量也称为静态变量，在类中以 static 关键字声明，但必须在方法、构造方法和语句块之外。 无论一个类创建了多少个对象，类只拥有类变量的一份拷贝。 静态变量除了被声明为常量外很少使用。常量是指声明为 public/private，final 和 static 类型的变量。常量初始化后不可改变。 静态变量储存在静态存储区。经常被声明为常量，很少单独使用 static 声明变量。 静态变量在程序开始时创建，在程序结束时销毁。 与实例变量具有相似的可见性。但为了对类的使用者可见，大多数静态变量声明为 public 类型。 默认值和实例变量相似。数值型变量默认值是0，布尔型默认值是 false，引用类型默认值是 null。变量的值可以在声明的时候指定，也可以在构造方法中指定。此外，静态变量还可以在静态语句块中初始化。 静态变量可以通过：ClassName.VariableName 的方式访问。 类变量被声明为 public static final 类型时，类变量名称必须使用大写字母。如果静态变量不是 public 和 final 类型，其命名方式与实例变量以及局部变量的命名方式一致。 实例： 1234567891011import java.io.*;public class Employee{ //salary是静态的私有变量 private static double salary; // DEPARTMENT是一个常量 public static final String DEPARTMENT = &quot;Development &quot;; public static void main(String args[]){ salary = 1000; System.out.println(DEPARTMENT+&quot;average salary:&quot;+salary); }} 以上实例编译运行结果如下: 1Development average salary:1000 注意：如果其他类想要访问该变量，可以这样访问：Employee.DEPARTMENT。 变量命名规则 必须以字母开头并且由数字和字母构成的序列，不能包含空格与java保留关键字‘ 命名的时候注意使得命名有意义，养成英文命名习惯，也可以参照驼峰命名法 在C里区别变量的声明和定义，但是在java中不区分声明和定义 实例 12int i=10;//是一个定义extern int i;//是一个声明","link":"/2022/01/18/java4/"},{"title":"数学建模|预测方法：插值与拟合","text":"","link":"/2022/01/18/jm10/"},{"title":"数学建模|预测方法：神经元网络","text":"","link":"/2022/01/19/jm11/"},{"title":"数学建模|预测方法：微分方程","text":"微分方程预测特征适用范围适用于基于相关原理的因果预测模型，大多是物理或几何方面的典型问题，假设条件，用数学符号表示规律，列出方程，求解的结果就是问题的答案。 优点优点是短、中、长期的预测都适合。如：传染病的预测模型、经济增长（或人口）的预测模型、Lanchester战争预测模型。 缺点反应事物内部规律及其内在关系，但由于方程的建立是以局部规律的独立性假定为基础，当作为长期预测时，误差较大，且微分方程的解比较难以得到。 常见案例传染病的预测模型、经济增长（或人口）的预测模型、Lanchester战争预测模型、药物在体内的分布与排除预测模型、烟雾的扩散与消失模型 常用方法直接列方程 利用所学过的公式对某些实际问题列出微分方程。 微元分析法与任意区域上取积分的方法 利用已知的规律建立一些变量（自变量与未知函数）的微元之间的关系式。 然后再通过取极限的方法得到微分方程，或等价地通过任意区域上取积分的方法来建立微分方程。 Matlab求解dsolve()函数1[y1,y2,?,yn]=dsolve(eqns,conds,name,value) 其中：eqns为符号微分方程（组）；conds为初值条件或边值条件；name，value为可选的成对参数。 自变量名可以省略，默认变量名‘t’ 123y1=dsolve('Dy=1+y^2','y(0)=1','x')[x,y]=dsolve('Dx=y,D2y-Dy=0','x(0)=2,y(0)=1,Dy(0)=1','t') ode函数还有大量的常微分方程，虽然从理论上讲，其解是存在的，但我们却无法求出其解析解，此时，我们需要寻求方程的数值解。 123456789101112131415function testode45tspan=[3.9 4.0]; %求解区间y0=[8 2]; %初值[t,x]=ode45(@odefun,tspan,y0);plot(t,x(:,1),'-o',t,x(:,2),'-*')legend('y1','y2')title('y'' ''=-t*y + e^t*y'' +3sin2t')xlabel('t')ylabel('y')function y=odefun(t,x)y=zeros(2,1); % 列向量y(1)=x(2);y(2)=-t*x(1)+exp(t)*x(2)+3*sin(2*t); %常微分方程公式endend 案例 数学建模【微分方程模型(介绍、分析方法、数值模拟、传染病问题的建模和分析、经济增长模型、人口增长预测和控制模型)】 微分方程建模——以传染病模型为例","link":"/2022/01/18/jm6/"},{"title":"Java学习笔记|修饰符和运算符","text":"Java修饰符 Java运算符","link":"/2022/01/19/java5/"},{"title":"数学建模|预测方法：马尔科夫预测","text":"马尔可夫链的定义现实世界中有很多这样的现象：某一个系统在已知现在的条件下，系统未来时刻的情况只与现在有关，而与过去的历史无关，比如，研究一个商店的累计销售额，如果现在时刻的累计销售额已知，则未来某一时刻的累计销售额与现在时刻以前的任一时刻累计销售额无关。描述这类随机现象的数学模型称为马尔可夫模型。 数学表达设是一个随机序列，状态空间为有限或可列集，对于任意的正整数，若有 P\\{\\xi_{n+m}=j|\\xi_n=i,\\xi_{n-1}=i_{n-1},···,\\xi_1=i_1\\}=P\\{\\xi_{n+m}=j|\\xi_n=i\\}则称为一个马尔可夫链。 事实上，证明该等式对于成立，则它对于任意的正整数也成立，则只要当时等式成立，则可以称为马尔可夫链。 设是一个马尔可夫链，则上述等式右边的条件概率与无关，即 P\\{\\xi_{n+m}=j|\\xi_n=i\\}=p_{ij}(m)则称${\\xin,n=1,2,···}为时齐的马尔可夫链，称p{ij}(m)为系统由状态i经过m个时间间隔转移到状态j$的转移概率。 转移概率与转移概率矩阵对于一个马尔可夫链${\\xin,n=1,2,···}，称以m步转移概率p{ij}(m)为元素的矩阵P(m)=(p_{ij}(m))为马尔可夫链的m步转移矩阵。当m=1时，记P(1)=P$称为马尔可夫链的一步转移矩阵 它有如下三条性质： 对一切 对于一切$i\\in E,\\sum{j\\in E}p{ij}(m)=1$ 对一切 \\begin{equation} p_{ij}(0)=\\delta_{ij} =\\left\\{ \\begin{array}{ll} 1,当i=j时\\\\ 0,当i\\not ={j}时\\\\ \\end{array}\\right. \\end{equation} 当实际问题可以用马尔可夫链来描述时，首先要确定它的状态空间及参数集合，然后确定它的一步转移概率，关于这一概率的确定，可以由问题的内在规律得到，也可以由过去经验给出，还可以根据观测数据来估计。 案例 数学建模常用模型23：马尔可夫预测方法 数学建模|马尔科夫链模型 一个马尔科夫链实例","link":"/2022/01/18/jm9/"},{"title":"数学建模|预测方法：差分方程","text":"差分方程有关于什么是差分方程，以及差分方程的建立与求解，参考差分方程的建立及经典解法 差分方程的求解规定只取非负整数。记为变量在点的取值，则称为的一阶向前差分，简称差分，称为的二阶差分。类似地，可以定义的阶差分。由、及的差分给出的方程称为的差分方程，其中含的最高阶差分的阶数称为该差分方程的阶。差分方程也可以写成不显含差分的形式。例如，二阶差分方程，也可改写成。满足一差分方程的序列称为差分方程的解。 示例 参照司守奎的数学建模书例子假设不与成线性相关，而是按前一个或前两个的一定比例增长。如 y_t=a_1y_{t-1}+a_2 y_t=a_1y_{t-1}+a_2y_{t-2}+a_3如果点不在线上，我们一般采用最小二乘法 然而这本书的代码并没有用最小二乘法，还是照着矩阵运算 123y0=[11 12 13 15 16]';y=y0(3:5);x=[y0(2:4),y0(1:3),ones(3,1)];z=x\\y 例子1商品销售量预测 以上的两部分代码都是为了求解一个商品5年第一季度的销售量与时间的方程。但是在此如果每个季度都列一个五年的方程与事实不符。所以试试全体数据拟合。 1234567891011y0=[11 16 25 12 12 ... 18 26 14 13 20 ... 27 15 15 24 30 ... 15 16 25 32 17]';y=y0(9:20);x=[y0(5:16),y0(1:12),ones(12,1)];z=x\\yfor t=21:25y0(t)=z(1)*y0(t-4)+z(2)*y0(t-8)+z(3)endyhat=y0(21:25)%提取预测t=21,...,25时的预测值 同样的，这里也是普通的矩阵运算解方程组，没有涉及最小二乘法。 例子2养老保险 Matlab程序如下:1234clc,clearM=600;N=420;p=200;q=2282;eq=@(x)x^M-(1+q/p)*x^(M-N)+q/p;x=fzero(eq,[1.0001,1.5]) 参考博客 学习笔记17 差分方程 数学建模（五） 微分方程，常微分方程，差分方程模型","link":"/2022/01/18/jm8/"},{"title":"数学建模|预测方法：灰色预测模型","text":"简介灰色系统理论是由华中理工大学邓聚龙教授于1982年提出并加以发展的。二十几年来，引起了不少国内外学者的关注，得到了长足的发展。目前，在我国已经成为社会、经济、科学技术在等诸多领域进行预测、决策、评估、规划控制、系统分析与建模的重要方法之一。特别是它对时间序列短、统计数据少、信息不完全系统的分析与建模，具有独特的功效，因此得到了广泛的应用. 适用范围该模型使用的不是原始数据的序列，而是生成的数据序列。核心体系是Grey Model，即对原始数据作累加生成（或其他处理生成）得到近似的指数规律再进行建模的方法。 优点在处理较少的特征值数据，不需要数据的样本空间足够大，就能解决历史数据少、序列的完整性以及可靠性低的问题，能将无规律的原始数据进行生成得到规律较强的生成序列。 缺点只适用于中短期的预测，只适合近似于指数增长的预测。 灰色系统灰色系统是黑箱概念的一种推广。我们把既含有已知信息又含有未知信息的系统称为灰色系统，作为两个极端，我们将称信息完全未确定的系统为黑色系统;称信息完全确定的系统为白色系统。区别白色系统与黑色系统的重标志是系统各因素之间是否具有确定的关系。 特点 用灰色数学处理不确定量，使之量化. 充分利用已知信息寻求系统的运动规律. 灰色系统理论能处理贫信息系统. 灰色生成将原始数据列中的数据，按某种要求作数据处理称为生成。客观世界尽管复杂，表述其行为的数据可能是杂乱无章的，然而它必然是有序的，都存在着某种内在规律，不过这些规律被纷繁复杂的现象所掩盖，人们很难直接从原始数据中找到某种内在的规律。对原始数据的生成就是企图从杂乱无章的现象中去发现内在规律。 常用灰色系统生成方式 累加生成 累减生成 均值生成 级比生成 …… 累加生成累加生成，即通过数列间各时刻数据的依个累加以得到新的数据与数列。累加前的数列称原始数列，累加后的数列称为生成数列。累加生成是使灰色过程由灰变白的一种方法，它在灰色系统理论中占有极其重要地位，通过累加生成可以看出灰量积累过程的发展态势,使离乱的原始数据中蕴含的积分特性或规律加以显化。累加生成是对原始数据列中各时刻的数据依次累加，从而生成新的序列的一种手。 GM(1,1)预测模型推导 精度检验模型选定之后，一定要经过检验才能判定其是否合理，只有通过检验的模型才能用来作预测。灰色模型的精度检验一般有三种方法: 相对误差大小检验法 关联度检验法 后验差检验法 下面主要介绍后验差检验法 精度检验等级参照表 算法总结主要步骤 累加生成 建立GM（1，1）模型 检验预测值 Matlab代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374function []=greymodel(y)% 本程序主要用来计算根据灰色理论建立的模型的预测值。% 应用的数学模型是 GM(1,1)。% 原始数据的处理方法是一次累加法。y=input('请输入数据 ');n=length(y);yy=ones(n,1);yy(1)=y(1);for i=2:n yy(i)=yy(i-1)+y(i);endB=ones(n-1,2);for i=1:(n-1) B(i,1)=-(yy(i)+yy(i+1))/2; B(i,2)=1;endBT=B';for j=1:n-1 YN(j)=y(j+1);endYN=YN';A=inv(BT*B)*BT*YN;a=A(1);u=A(2);t=u/a;i=1:n+2;yys(i+1)=(y(1)-t).*exp(-a.*i)+t;yys(1)=y(1);for j=n+2:-1:2 ys(j)=yys(j)-yys(j-1);endx=1:n;xs=2:n+2;yn=ys(2:n+2);plot(x,y,'^r',xs,yn,'*-b');det=0;sum1=0;sumpe=0;for i=1:n sumpe=sumpe+y(i);endpe=sumpe/n;for i=1:n; sum1=sum1+(y(i)-pe).^2;ends1=sqrt(sum1/n);sumce=0;for i=2:n sumce=sumce+(y(i)-yn(i));endce=sumce/(n-1);sum2=0;for i=2:n; sum2=sum2+(y(i)-yn(i)-ce).^2;ends2=sqrt(sum2/(n-1));c=(s2)/(s1);disp(['后验差比值为：',num2str(c)]);if c&lt;0.35 disp('系统预测精度好')else if c&lt;0.5 disp('系统预测精度合格') else if c&lt;0.65 disp('系统预测精度勉强') else disp('系统预测精度不合格') end endend disp(['下个拟合值为 ',num2str(ys(n+1))]);disp(['再下个拟合值为',num2str(ys(n+2))]); 实际案例123[724.57, 746.62, 778.27, 800.8, 827.75,871.1, 912.37, 954.28, 995.01, 1037.2][2.874,3.278,3.337,3.390,3.679] 例子 数学建模之灰色预测实例含代码(城市交通噪声的例子) GM(2,1)、DGM(2,1)模型、Verhulst模型GM(1，1)模型适用于较强指数规律的序列，只能描述单调的变化过程，对于非单调的摆动发展序列或者有饱和的S形序列，可以考虑建立GM(2，1)、DGM、Verhulst模型。 GM(2,1)模型 弱化算子对于初期增长势头过于猛烈的模型，为了提高精度，可以考虑使用弱化算子处理原始数列。 对应的，依旧是最小二乘估计参数，再对微分方程求解，得到 DGM模型建立 Verhulst预测模型Verhulst模型的定义如下 对于模型参数，使用最小二乘估计有以下结果 最终，可以求得灰色Verhulst的解为 Verhulst模型应用：道路交通事故预测 对于交通事故死亡人数统计数据，我们首先做出大体曲线变化图，以从整体上着手 可见曲线呈现S型，考虑使用verhulst建模。建模过程如下 最后我们还要进行一步模型精度检验。灰色模型有一套具体的检验标准，后注。 检验三个指标：相对误差、绝对关联度、均方差比值。利用MATLAB检验结果如下： 可见： 平均相对误差为 3.74% ，则模型精度为二级；同时算得绝对关联度 g 为 0.9845，均方差比值 C 为 0.2355，则模型精度为一级，可见模型精度较高，可用于事故预测。 Matlab程序123456789101112131415161718192021222324252627282930clc,clearx1=[4.93 5.33 5.87 6.35 6.63 7.15 7.37...7.39 7.81 8.35 9.39 10.59 10.94 10.44];n = length(x1);nian=1990:2003;plot(nian,x1,'o-');x0=diff(x1); %作累减生成x0=[x1(1),x0]z1=0.5*(x1(2:n)+x1(1:n-1)) %求紧邻均值生成序列B=[-z1',z1'.^2]Y=x0(2:end)'ab_hat=B\\Y %估计参数 a,b 的值x=dsolve('Dx+a*x=b*x^2','x(0)=x0'); %求解常微分方程x=subs(x,{'a','b','x0'},{ab_hat(1),ab_hat(2),x1(1)}); %代入参数值yuce=subs(x,'t',0:14) %计算预测值%下面显示微分方程的解，为了提高计算精度，把该语句放在计算预测值之后x=vpa(x,6)x1_all=[x1,9.92,10.71]; %加上 2004 年的两个观测值yuce(16)=yuce(15); %2004 年有两个观测值，要对应两个相同的预测值epsilon=x1_all-yuce %计算残差delta=abs(epsilon./x1_all) %计算相对误差delta_mean=mean(delta) %计算平均相对误差x1_all_0=x1_all-x1_all(1); %观测值数据列的始点零化像yuce_0=yuce-yuce(1); %预测值数据列的始点零化像s0=abs(sum(x1_all_0(2:end-1))+0.5*x1_all_0(end));s1=abs(sum(yuce_0(2:end-1))+0.5*yuce_0(end));tt=yuce_0-x1_all_0;s1_s0=abs(sum(tt(2:end-1))+0.5*tt(end));absdegree=(1+s0+s1)/(1+s0+s1+s1_s0) %计算灰色绝对关联度c=std(epsilon,1)/std(x1_all,1) %计算标准差比值 参考博客 【数学建模】灰色预测模型（预测） 【数学建模】灰色系统理论II-Verhulst建模-GM(1,N)-GM(2,1)建模","link":"/2022/01/18/jm7/"},{"title":"Python数据分析|Numpy学习","text":"","link":"/2022/01/18/py-num-pandas1/"}],"tags":[{"name":"数学建模","slug":"数学建模","link":"/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"Github","slug":"Github","link":"/tags/Github/"},{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"入门教程","slug":"入门教程","link":"/tags/%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/"},{"name":"统计学习方法","slug":"统计学习方法","link":"/tags/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/"},{"name":"小论文","slug":"小论文","link":"/tags/%E5%B0%8F%E8%AE%BA%E6%96%87/"},{"name":"Python数据分析","slug":"Python数据分析","link":"/tags/Python%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"}],"categories":[{"name":"java","slug":"java","link":"/categories/java/"}]}