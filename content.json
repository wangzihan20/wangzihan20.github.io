{"pages":[{"title":"about","text":"eeeeqwewqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqqq","link":"/about/index.html"},{"title":"记录一下","text":"关于font-awesome图标使用 1234567891011font-awesome是一种把图表当成字体的工具下载：npm install font-awesome --save格式要求：&lt;i class=&quot;fa 从官网上找图标复制的东西&quot;&gt;&lt;/i&gt;例子：&lt;i class=&quot;fa fa-car&quot; &gt;&lt;/i&gt;&lt;i class=&quot;fa fa-area-chart&quot;&gt;&lt;/i&gt;&lt;i class=&quot;fa fa-bluetooth&quot;&gt;&lt;/i&gt; 运行效果： 1234567891011121314151617181920212223颜色以及字体改变：&lt;font face=&quot;黑体&quot;&gt;我是黑体字&lt;/font&gt;&lt;font face=&quot;微软雅黑&quot;&gt;我是微软雅黑&lt;/font&gt;&lt;font face=&quot;STCAIYUN&quot;&gt;我是华文彩云&lt;/font&gt;&lt;font color=#0099ff size=7 face=&quot;黑体&quot;&gt;color=#0099ff size=72 face=&quot;黑体&quot;&lt;/font&gt;&lt;font color=#00ffff size=72&gt;color=#00ffff&lt;/font&gt;&lt;font color=gray size=72&gt;color=gray&lt;/font&gt;常用颜色十六进制代码深蓝：#0099ff浅蓝：#00ffff黑色：#000000灰色：#808080银色：#c0c0c0红色：#FF0000 浅红：#FF4040 橙色：#FF8000绿色：#80DF20 浅绿：#9FFF40 青色：#00FF80 亮青：#00FFFF白色：#f0f8ff #f8f8ff 全部颜色十六进制代码对照表 运行效果： 我是黑体字 我是微软雅黑 我是华文彩云 color=#0099ff color=#00ffff color=gray font-awesome官网 Hexoz主题更改 123456789101112131415161718192021222324252627282930313233343536373839404142菜单示例：menus: 首页: { path: /, fa: fa-fort-awesome faa-shake } 归档: { path: /archives, fa: fa-archive faa-shake, submenus: { 技术: {path: /categories/技术/, fa: fa-code }, 生活: {path: /categories/生活/, fa: fa-file-text-o }, 资源: {path: /categories/资源/, fa: fa-cloud-download }, 随想: {path: /categories/随想/, fa: fa-commenting-o }, 转载: {path: /categories/转载/, fa: fa-book } } } 清单: { path: javascript:;, fa: fa-list-ul faa-vertical, submenus: { 书单: {path: /tags/悦读/, fa: fa-th-list faa-bounce }, 番组: {path: /bangumi/, fa: fa-film faa-vertical }, 歌单: {path: /music/, fa: fa-headphones }, 图集: {path: /tags/图集/, fa: fa-photo } } } 留言板: { path: /comment/, fa: fa-pencil-square-o faa-tada } 友人帐: { path: /links/, fa: fa-link faa-shake } 赞赏: { path: /donate/, fa: fa-heart faa-pulse } 关于: { path: /, fa: fa-leaf faa-wrench , submenus: { 我？: {path: /about/, fa: fa-meetup}, 主题: {path: /theme-sakura/, fa: iconfont icon-sakura }, Lab: {path: /lab/, fa: fa-cogs }, } } 客户端: { path: /client/, fa: fa-android faa-vertical } RSS: { path: /atom.xml, fa: fa-rss faa-pulse }## menumenu:- page: home url: / icon: fa-home- page: Java基础 url: /categories/javase/ icon: - page: 博客教程 url: /categories/blog/ icon: - page: 虚拟化 url: /categories/虚拟化/ icon: 数学公式 多行公式 12345678910$$\\begin{equation}\\dot{\\boldsymbol{x}}=f(\\boldsymbol{x})=\\left\\{ \\begin{array}{ll} f_{1}(\\boldsymbol{x}) &amp; \\boldsymbol{x} \\in S_{1} \\\\ f_{2}(\\boldsymbol{x}) &amp; \\boldsymbol{x} \\in S_{2} \\end{array}\\right.\\end{equation}$$ 运行效果 \\[ \\begin{equation} \\dot{\\boldsymbol{x}}=f(\\boldsymbol{x}) =\\left\\{ \\begin{array}{ll} f_{1}(\\boldsymbol{x}) &amp; \\boldsymbol{x} \\in S_{1} \\\\ f_{2}(\\boldsymbol{x}) &amp; \\boldsymbol{x} \\in S_{2} \\end{array}\\right. \\end{equation} \\]","link":"/test1/index.html"},{"title":"入门网站归纳","text":"入门各类编程语言的网站 -菜鸟教程","link":"/test1/cainiao.html"},{"title":"tags","text":"","link":"/tags/index.html"},{"title":"王梓涵的个人简历","text":"王梓涵 12345678912 · zs123@126.com · GitHub · My Blog 个人信息 男，2001 年，武汉，中共党员 求职意向：**岗位 教育经历 硕士，清华大学，**专业，2018.09~至今. 学士，北京大学，**专业，2014.09~2018.07. 通过了 CET6 英语等级考试，计算机四级认证，软考中级认证(网络工程师). 校外实习 ##公司，##部门，##岗位，2018.7~2018.11 项目描述：##################################################### 工作描述：##################################################### 个人项目 项目一：######################################################### 项目二：######################################################### 项目二：########### 项目三：######################################################### 职场技能 精通############################################################ 精通############################################################ 熟悉############################################################ 熟悉############################################################ 了解############################################################ 了解############################################################ 获奖情况 自我评价 感谢抽空阅读","link":"/%E4%B8%AA%E4%BA%BA%E7%AE%80%E5%8E%86/index.html"}],"posts":[{"title":"Git操作与仓库创建","text":"Git简介 首先了解一个概念:版本控制,简单来说就是如果你做文案工作，每次提交之后，你的领导会让你修改，一篇稿子可能修改十几次，但是最后定稿的很可能不是最新修改的那一稿，所以就需要有个版本控制的方法，可以回溯到你所修改的任何一版，并且可以拿出来使用。 目前来说，版本控制主要分为：集中式版本控制（Centralized Version Control Systems，简称 CVCS）和分布式版本控制，（Distributed Version Control System，简称 DVCS）。 集中式版本控制类似于中央集权，多个终端与一个服务器进行交互，缺点明显，如果服务器损毁，则所有终端都不能拿到最新版本。 分布式系统（distributed system）是建立在网络之上的软件系统。正是因为软件的特性，所以分布式系统具有高度的内聚性和透明性。因此，网络和分布式系统之间的区别更多的在于高层软件（特别是操作系统），而不是硬件。 下图就是分布式版本控制系统的图解： 想更多的了解这两者的优缺点，请点击这里 CVCS的代表主要有CVS、SVN 以及Perforce等； DVCS主要有 Git、Mercurial、Bazaar 以及 Darcs 等。我们平时比较常用的就是SVN和Git。 Git是世界上目前最先进的版本控制系统，Git的开发者也是Linux操作系统的创始人：“林纳斯·本纳第克特·托瓦兹”，他开发这个Git只用了短短两周，而关于这个Git开发的故事，也有一段奇闻，想了解的请点击深入 git 必看：git 是如何被创造的？讲述 git 的诞生史、核心思想及其父：Linus Torvalds 如想了解SVN的简单使用，可以查看：SVN的介绍与使用流程。 接下就开始介绍Git的简单操作使用。以下主要对官方文档以及其他文档的总结，会使用git作为版本控制工具来完成团队协作。因此，对基本的git操作指令进行总结是十分有必要的，本文会对基本的理论与命令做归纳总结。 git的通用操作流程如下图 主要涉及到四个关键点： 工作区：本地电脑存放项目文件的地方，比如learnGitProject文件夹； 暂存区（Index/Stage）：在使用git管理项目文件的时候，其本地的项目文件会多出一个.git的文件夹，将这个.git文件夹称之为版本库。其中.git文件夹中包含了两个部分，一个是暂存区（Index或者Stage）,顾名思义就是暂时存放文件的地方，通常使用add命令将工作区的文件添加到暂存区里； 本地仓库：.git文件夹里还包括git自动创建的master分支，并且将HEAD指针指向master分支。使用commit命令可以将暂存区中的文件添加到本地仓库中； 远程仓库：不是在本地仓库中，项目代码在远程git服务器上，比如项目放在github上，就是一个远程仓库，通常使用clone命令将远程仓库拷贝到本地仓库中，开发后推送到远程仓库中即可； 因此，经过这样的分析，git命令可以分为这样的逻辑进行理解和记忆： 1.git管理配置的命令； 2.几个核心存储区的交互命令： 3.工作区与暂存区的交互； 4.暂存区与本地仓库（分支）上的交互； 5.本地仓库与远程仓库的交互。 Git下载与环境配置 Git下载安装以及环境配置请参考Git下载、安装与环境配置，这里不再赘述。 git配置命令 Git操作需要在一个文件夹下生成，安装完成之后，在桌面或者文件夹下，点击右键，出现下图： Git GUI 是图像化显示，与Git Bash功能一样 Git Bash常用此方式创建一个仓库，点击打开Git控制台 以下所有命令均在Git控制台上运行：如下图 1$表示提示输入 查询配置信息 12341.列出当前配置：git config --list;2.列出repository配置：git config --local --list;3.列出全局配置：git config --global --list;4.列出系统配置：git config --system --list; 第一次使用git，配置用户信息 121.配置用户名：git config --global user.name &quot;your name&quot;;2.配置用户邮箱：git config --global user.email &quot;youremail@github.com&quot;; 其他配置 1231.配置解决冲突时使用哪种差异分析工具，比如要使用vimdiff：git config --global merge.tool vimdiff;2.配置git命令输出为彩色的：git config --global color.ui auto;3.配置git使用的文本编辑器：git config --global core.editor vi; 工作区上的操作命令 12345新建仓库1.将工作区中的项目文件使用git进行管理，即创建一个新的本地仓库：git init；(初始化)2.从远程git仓库复制项目：git clone &lt;url&gt;，如：git clone git://github.com/wasd/example.git;、3.克隆项目时如果想定义新的项目名，可以在clone命令后指定新的项目名：git clone git://github.com/wasd/example.git mygit； 查看文件状态 12345$ git status显示工作目录的状态，不带参数执行，输出内容很详细。并且根据文件是否暂存，会预示下一步的指令操作。如果想简洁一点，那么加个--short （-s）参数：git status -s 移除文件 12345$ git rm 从git中将已跟踪的文件从工作目录、暂存区移除，注意是已跟踪的。如果该文件又是已修改的，可以使用参数 -f 强制删除。如果移除未跟踪的文件，或者只在工作目录移除，在暂存区继续保留，那么可以执行： 提交 1231.提交工作区所有文件到暂存区：git add .('.'的前面还有个空格！)2.提交工作区中指定文件到暂存区：git add &lt;file1&gt; &lt;file2&gt; ...;3.提交工作区中某个文件夹中所有文件到暂存区：git add [dir]; 暂存区上的操作命令 123456提交文件到版本库1.将暂存区中的文件提交到本地仓库中，即打上新版本：git commit -m &quot;commit_info&quot;;2.将所有已经使用git管理过的文件暂存后一并提交，跳过add到暂存区的过程：git commit -a -m &quot;commit_info&quot;;3.提交文件时，发现漏掉几个文件，或者注释写错了，可以撤销上一次提交：git commit --amend;4.修改commit信息git commit --ammend,修改，保存; 删除git远程仓库地址 12git remote -v //查看git remote rm origin 分支管理 123456789101112131.创建分支：git branch &lt;branch-name&gt;，如git branch testing；2.从当前所处的分支切换到其他分支：git checkout &lt;branch-name&gt;，如git checkout testing；3.新建并切换到新建分支上：git checkout -b &lt;branch-name&gt;;4.删除分支：git branch -d &lt;branch-name&gt;；5.将当前分支与指定分支进行合并：git merge &lt;branch-name&gt;;6.显示本地仓库的所有分支：git branch;7.查看各个分支最后一个提交对象的信息：git branch -v;8.查看哪些分支已经合并到当前分支：git branch --merged;9.查看当前哪些分支还没有合并到当前分支：git branch --no-merged;10.把远程分支合并到当前分支：git merge &lt;remote-name&gt;/&lt;branch-name&gt;，如git merge origin/serverfix；如果是单线的历史分支不存在任何需要解决的分歧，只是简单的将HEAD指针前移，所以这种合并过程可以称为快进（Fast forward），而如果是历史分支是分叉的，会以当前分叉的两个分支作为两个祖先，创建新的提交对象；如果在合并分支时，遇到合并冲突需要人工解决后，再才能提交；11.在远程分支的基础上创建新的本地分支：git checkout -b &lt;branch-name&gt; &lt;remote-name&gt;/&lt;branch-name&gt;，如git checkout -b serverfix origin/serverfix;12.从远程分支checkout出来的本地分支，称之为跟踪分支。在跟踪分支上向远程分支上推送内容：git push。该命令会自动判断应该向远程仓库中的哪个分支推送数据；在跟踪分支上合并远程分支：git pull；13.将一个分支里提交的改变移到基底分支上重放一遍：git rebase &lt;rebase-branch&gt; &lt;branch-name&gt;，如git rebase master server，将特性分支server提交的改变在基底分支master上重演一遍；使用rebase操作最大的好处是像在单个分支上操作的，提交的修改历史也是一根线；如果想把基于一个特性分支上的另一个特性分支变基到其他分支上，可以使用--onto操作：git rebase --onto &lt;rebase-branch&gt; &lt;feature branch&gt; &lt;sub-feature-branch&gt;，如git rebase --onto master server client；使用rebase操作应该遵循的原则是：一旦分支中的提交对象发布到公共仓库，就千万不要对该分支进行rebase操作； 忽略文件.gitignore 一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。如下例： 12345678910111213# 此为注释 – 将被 Git 忽略# 忽略所有 .a 结尾的文件*.a# 但 lib.a 除外!lib.a# 仅仅忽略项目根目录下的 TODO 文件，不包括 subdir/TODO/TODO# 忽略 build/ 目录下的所有文件build/# 会忽略 doc/notes.txt 但不包括 doc/server/arch.txtdoc/*.txt# 忽略 doc/ 目录下所有扩展名为 txt 的文件doc/**/*.txt 查看历史命令 12$ histroy该命令可以按顺序查看你之前输入过的所有的git命令 完整创建一个仓库的流程 1234567cd mall-clientgit inittouch README.mdgit add README.mdgit commit -m &quot;first commit&quot;git remote add origin https://gitee.com/onewj830/mall-client.gitgit push -u origin master 最常用的Git命令 123456789git init \\\\初始化git add . \\\\添加到缓存区git commit \\\\提交到本地仓库git push \\\\复制到远端仓库git clone &lt;url&gt; \\\\克隆库git status \\\\查看当前状态history \\\\查看历史命令git branch &lt;branch-name&gt; \\\\创建分支git branch -d &lt;branch-name&gt; \\\\删除分支 结语 在学习中总结，也有很多不足的地方，总结不是很到位，望指出。 记得很早开始开始接触的时候，也是很懵，看着一堆英文的界面啥也不懂，虽然我看的懂部分英文解释说明，但是对这个系统的工作流程与工作方式是一知半解的，分布式版本控制系统我觉得有时候也像是区块链的思想，我感觉我很有动力去学这些东西，特别是写自己的博客，我觉得很开心，甚至再探索的时候时间不知不觉就来到了凌晨，我非要解决了眼下这个问题才肯休息……从入门到现在，一点心得就是，初学不需明白那些非常高深的理论知识，只要知道git是个版本控制器就行，知道怎么创建仓库，怎么利用别人的仓库的开源项目提升自己，和SVN的区别也不需特别的去理会，一边学习一边应用，效率才是最高的。使用过，才知道他是个什么，以前那些原理也就恍然大悟了，可以举一反三！即初学少研究，多实践，最后事倍功半！","link":"/2022/01/05/Git/"},{"title":"Java学习笔记(一)","text":"Java的特性 1. Java是简单的: -Java语法和C++很接近，却没有C++中一些少使用、难理解的特性，比如：操作符重载、多继承、自动的强制类型转换。尤其是Java不使用指针，而是使用引用，并提供了自动分配和回收的内存空间，不需要为内存管理而担忧。 2. Java语言是面向对象的： Java 语言提供类、接口和继承等面向对象的特性，为了简单起见，只支持类之间的单继承，但支持接口之间的多继承，并支持类与接口之间的实现机制（关键字为 implements）。Java 语言全面支持动态绑定，而 C++语言只对虚函数使用动态绑定。总之，Java语言是一个纯的面向对象程序设计语言。 3. Java语言是分布式的： Java 语言支持 Internet 应用的开发，在基本的 Java 应用编程接口中有一个网络应用编程接口（java net），它提供了用于网络应用编程的类库，包括 URL、URLConnection、Socket、ServerSocket 等。 4. Java语言是安全的： Java通常被用在网络环境中，为此，Java 提供了一个安全机制以防恶意代码的攻击。除了Java 语言具有的许多安全特性以外，Java 对通过网络下载的类具有一个安全防范机制（类 ClassLoader），如分配不同的名字空间以防替代本地的同名类、字节代码检查，并提供安全管理机制（类 SecurityManager）让 Java 应用设置安全哨兵。 5. Java 语言是体系结构中立的： Java 程序（后缀为 java 的文件）在 Java 平台上被编译为体系结构中立的字节码格式（后缀为 class 的文件），然后可以在实现这个 Java 平台的任何系统中运行。这种途径适合于异构的网络环境和软件的分发。 6.Java 语言是可移植的： 这种可移植性来源于体系结构中立性，另外，Java 还严格规定了各个基本数据类型的长度。 7. Java 是高性能的： 与那些解释型的高级脚本语言相比，Java 的确是高性能的。事实上，Java 的运行速度随着 JIT(Just-In-Time）编译器技术的发展越来越接近于 C++。 8. Java 语言是多线程的： 在 Java 语言中，线程是一种特殊的对象，它必须由 Thread 类或其子（孙）类来创建。通常有两种方法来创建线程：其一，使用型构为 Thread(Runnable) 的构造子类将一个实现了 Runnable 接口的对象包装成一个线程，其二，从 Thread 类派生出子类并重写 run 方法，使用该子类创建的对象即为线程。值得注意的是 Thread 类已经实现了 Runnable 接口，因此，任何一个线程均有它的 run 方法，而 run 方法中包含了线程所要运行的代码。线程的活动由一组方法来控制。Java 语言支持多个线程的同时执行，并提供多线程之间的同步机制（关键字为 synchronized）。 开发环境配置 参考Java 开发环境配置 基础语法 一个 Java 程序可以认为是一系列对象的集合，而这些对象通过调用彼此的方法来协同工作。下面简要介绍下类、对象、方法和实例变量的概念。 对象：对象是类的一个实例，有状态和行为。例如，一条狗是一个对象，它的状态有：颜色、名字、品种；行为有：摇尾巴、叫、吃等。 类：类是一个模板，它描述一类对象的行为和状态。 方法：方法就是行为，一个类可以有很多方法。逻辑运算、数据修改以及所有动作都是在方法中完成的。 实例变量：每个对象都有独特的实例变量，对象的状态由这些实例变量的值决定。 参考菜鸟教程:Java 基础语法","link":"/2022/01/07/Java1/"},{"title":"有关Github的介绍","text":"Github是一个面向开源的私有软件托管平台，因为只支持Git作为唯一的版本库格式进行托管，所以叫Github。它于2008年4月10日正式上线，它的开发者也是linux之父：“林纳斯·本纳第克特·托瓦兹”，作为一个分布式的版本控制系统，Github的功能除了 Git 代码仓库托管及基本的 Web 管理界面以外，还提供了订阅、讨论组、文本渲染、在线文件编辑器、协作图谱（报表）、代码片段分享（Gist）等功能。目前，在 GitHub 上托管的版本数量非常之多，其中不乏知名开源项目 Ruby on Rails、jQuery、python 等。Github的仓库是他独有的特征，你大可以理解为一个无限容量且没有传输速度限制的网上云盘，但是这个云盘是可以设置公共与私密空间的，在这个开源的时代，你可以通过这个平台看到世界上许多其他大佬的程序作品，并且学习他们的编程思想，而且这很容易获得。 Github的注册与登录 1.首先来到Github的官网，或者你可以直接点击这里的超链接：Github，随后你会来到官网首页界面 国内访问Github可能会有些慢，或者你可以使用一些加速器，也可以使用国内的平台Gitee（码云），它的操作与Github一样，只是部分功能需要付费才能解锁。 2.点击右上角sign up登录，再点击create a new account创建账户 随后按照提示操作完成注册并且登录，值得注意的是:这里的注册邮箱可以是虚拟的，但是为了方便以后找回账号以及其他操作，建议使用自己的邮箱。至于Username，建议取一个具有标识特征的名称，之后登录就是使用Username和设置的Password。 3.随后他会询问你是否，以后可以通过这个邮箱给你发送一些最新的推送，选择之后做一个简单的真人验证就可以了 4.接下来一路无脑next操作你就完成了注册 Github的个人主页 登录之后你会看到这样的界面： 有关仓库的建立等会再进行介绍，点击右上角的个人头像的用户名，出现如下图： 标注 1：Edit profile，修改个人简介； 标注 2：Overview，个人主页概览； 标注 3：Repositories，仓库； 标注 4：Star，点星记录； 标注 5：Followers，粉丝； 标注 6：Following，关注的 GitHub 账号； 标注 7：个人贡献历史记录。 如上图所示: 标注 1 表示的为Edit profile，这个选项当我们修改完个人信息之后，就会自动消失； 标注 2 表示的为Overview，展示了我们账号的主要内容，包括仓库和贡献等； 标记 3 表示的为Repositories，是我们建立的仓库，包括Fork来的项目，GitHub 也会自动为我们创建一个仓库； 标注 4 表示为Star，收藏了我们的“点星”，或者说是“点赞”过的项目； 标注 7 表示的为我们最近一年来的contribution，用实心的小方格标记，小方格的颜色越深，表示我们的contribution越多。在这里，我们点击Edit profile，编辑个人简历，这个简历以后甚至可以作为找工作的招牌之一…… Github仓库建立 上个环节中，我们介绍了基本界面，其中就包括了标记3：Repositories，repo（即Repositories）是Github的核心要素——库，接下来，我们就尝试创建自己的 GitHub 仓库。 点击New创建一个新的repo。 标注 1：Repository name，仓库名称； 标注 2：Description，可选描述，也就是写不写都可以； 标注 3：Public，默认的仓库类型； 标注 4：Initialize this repository with a README，初始化仓库的信息文件，建议勾选。 如上图所示，这是创建 GitHub 仓库的核心页面，里面包含了众多信息。为了方便演示，我已经把各种所需的信息都填写完啦！接下来，点击绿色Create repository按钮即可，随后来到了仓库的界面 标注 1：Code，代码部分； 标注 2：Issues，文章部分； 标注 3：Pull request，拉取请求； 标注 4：Action，点星记录； 标注 5：Project，项目； 标注 6：Settings，设置； 其中的一些功能，因为涉及到Git操作，将在我的其他博客上介绍。 那么该怎么删除一个仓库呢？ 首先点击Settings,然后划到最底下出现Delete this repository按钮即是删除, 点击之后会让你验证，需要你重新输入一下你库的名字，这一步是保证你删除的库没有删错（可能你的库太多了之后，设置的名字有相似的） 这样输入之后，点击下方的I understand……之后，你就彻底删除一个库啦！","link":"/2022/01/03/Github/"},{"title":"规划问题","text":"线性规划 定义： 可行解：满足约束条件的解，使目标函数达到最大的可行解称为最优解。 可行域：所有可行解构成的集合。 MATLAB中线性规划的标准形式 ，。 其中的为列向量，为价值向量，为资源向量；，为矩阵。 MATLAB的求解线性规划的命令： 123[x,fval]=linprog(f,A,b)[x,fval]=linprog(f,A,b,Aeq,beq)[x,fval]=linprog(f,A,b,Aeq,beq,lb,ub) x返回决策向量的取值，fval返回目标函数的最优值，A和b对应线性不等式约束；Aeq和beq对应线性等式约束；lb和ub分别对应决策向量的下界向量和上界向量。 例子 相关应用 投资收益和风险 可以参考博客：投资的收益与风险的数学建模 整数规划 数学规划中变量限制为整数时，称为整数规划。线性规划模型中，变量限制为整数，则称为整数线性规划。目前流行的求解整数规划的方法，大多只适用于整数线性规划。 整数规划有两大类： 变量全限制为整数，纯整数规划 变量部分限制为整数，混合整数规划 整数规划特点： 1.原线性规划有最优解，当自变量限制为整数后，其整数规划解出现下述情况： 原线性规划最优解全是整数，则整数规划最优 解与线性规划最优解一致。 整数规划无可行解 求解方法分类 有可行解（当然就存在最优解），但最优解值变差。 2.整数规划最优解不能按照实数最优解简单取整而获得。 求解方法分类： 1.分枝定界法—可求纯或混合整数线性规划。 2.割平面法—可求纯或混合整数线性规划。 3.隐枚举法—求解“0-1”整数规划。 过滤隐枚举法； 分枝隐枚举法。 4.匈牙利法—解决指派问题（“0-1”规划特殊情形）。 5.蒙特卡洛法—求解各种类型规划。 0-1整数规划 相互排斥的约束条件 0-1型整数规划是整数规划中的特殊情形，它的变量仅取0或1。这时称为0-1变量，或称二进制变量。仅取值0 或 1 这个条件可由下述约束条件: ，且为整数 所代替，是和一般整数规划的约束条件形式一致的。 固定费用问题 指派问题 拟分配人去做项工作,每人做且仅做一项工作，若分配第人去做第项工作，需花费单位时间，问应如何分配工作才能使工人花费的总时间最少？ 要给出一个指派问题的实例，只需给出矩阵,被称为指派问题的系数矩阵。 引入0-1变量： 第人做第项工作第人做第项工作 。 上述指派问题的数学模型： ，，或。 上述指派问题的可行解可以用一个矩阵表示，每行、每列均有且只有一个元素为1，其余元素均为0。（因为一个人只能做一项工作） 蒙特卡洛（随机取样） 蒙特卡洛方法也称为计算机随机模拟方法，它源于世界 著名的赌城—摩纳哥的 Monte Carlo（蒙特卡洛）。它是基于 对大量事件的统计结果来实现一些确定性问题的计算。 蒙特卡洛方法可分为两类： 所求解的问题本身具有内在的随机性，借助计算机的 运算能力可以直接模拟这种随机的过程。 所求解问题可以转化为某种随机分布的特征数，比如 随机事件出现的概率，或者随机变量的期望值。用于求解复杂的多维积分问题。 蒙特卡洛法必须使用计算机生成相关分布的随机数，Matlab给出了生成各种随机数的命令。 例子1 若想求得图中不规则阴影部分的面积： 可以在规定的矩形区域内生成随机点，设点在不规则图形内部的数量为,全部的随机点为，矩阵面积为，则不规则阴影面积可以近似为： 例子2 可以参考蒙特卡洛算法的MATLAB实现的例子 非线性规划 非线性规划模型 定义：目标函数或约束条件中包含非线性函数 一般形式： 与线性规划区别：线性规划的最优解只能在可行域的边界达到，而非线性规划的最优解可能在可行域上的任意一点。 matlab中非线性规划的数学模型标准型： 其中部分变量与线下规划相同;为非线性向量函数。 例子 例子求解 代码实现： 1234567891011121314%目标函数function f=fun1(x);f=sum(x.^2)+8;%定义非线性约束条件function [g,h]=fun2(x);g=[-x(1)^2+x(2)-x(3)^2x(1)+x(2)^2+x(3)^3-20]; %非线性不等式约束h=[-x(1)-x(2)^2+2x(2)+2*x(3)^2-3]; %非线性等式约束%主程序，利用函数fmincon[x,y]=fmincon('fun1',rand(3,1),[],[],[],[],zeros(3,1),[],'fun2') 无约束问题 符号解 编写的matlab程序如下 12345678910111213141516171819202122clc, clearsyms x yf=x^3-y^3+3*x^2+3*y^2-9*x;%目标函数df=jacobian(f); %求一阶偏导数d2f=jacobian(df); %求Hessian阵（二阶导数阵）[xx,yy]=solve(df) %求驻点xx=double(xx);yy=double(yy); %转化成双精度浮点型数据，下面判断特征值的正负，必须是数值型的数据for i=1:length(xx) a=subs(d2f,{x,y},{xx(i),yy(i)}); b=eig(a); %求矩阵的特征值 f=subs(f,{x,y},{xx(i),yy(i)}); f=double(f); if all(b&gt;0) fprintf('(%f,%f)是极小值点，对应的极小值为%f\\n',xx(i),yy(i),f); elseif all(b&lt;0) fprintf('(%f,%f)是极大值点，对应的极大值为%f\\n',xx(i),yy(i),f); elseif any(b&gt;0) &amp; any(b&lt;0) fprintf('(%f,%f)不是极值点\\n',xx(i),yy(i)); else fprintf(无法判断(%f,%f)是否是极值点\\n',xx(i),yy(i)); endend 数值解 例子1 求解多元函数：的极值 1234567clc, clearf=@(x) x(1)^3-x(2)^3+3*x(1)^2+3*x(2)^2-9*x(1); %定义匿名函数g=@(x) -f(x);[xy1,z1]=fminunc(f, rand(2,1)) %求极小值点[xy2,z2]=fminsearch(g,rand(2,1)); %求极大值点，只能求出初值附近的一个极小值点xy2, z2=-z2 极小值点:,极小值;极大值点,极大值: 例子2 求函数的极小值 使用函数梯度，编写M函数fun3.m如下： 1234567function[f,g]=fun3(x);f=100*(x(2)-x(1)^2)^2+(1-x(1))^2;g=[-400*x(1)*(x(2)-x(1)^2)^2-2*(1-x(1));200*(x(2)-x(1)^2)];%g返回的是梯度向量%编写主程序文件：options=optimset('GrandObj','on');[x,y]=fminunc('fun3',rand(1,2),options) 求极值时，利用二阶导数（利用Hessian求解，加入优化参数） 123456789%目标函数的Hessian阵function [f,df,d2f]=fun4(x);f=100*(x(2)-x(1)^2)^2+(1-x(1))^2;df=[-400*x(1)*(x(2)-x(1)^2)-2*(1-x(1));200*(x(2)-x(1)^2)];d2f=[-400*x(2)+1200*x(1)^2+2,-400*x(1) -400*x(1),200];%编写主程序文件options = optimset('GradObj','on','Hessian','on');[x,y]=fminunc('fun4',rand(1,2),options) 函数的零点和方程组的解 求多项式 Matlab程序如下： 1234clc, clearxishu=[1 -1 2 -3];%多项式是用向量定义的，系数从高次幂到低次幂排列x0=roots(xishu) 求得多项式的全部零点为-0.1378和1.2757。 使用符号求解如下 123syms xx0=solve(x^3-x^2+2*x-3) %求函数零点的符号解x0=vpa(x0,5) %化成小数格式的数据 数值解 12y=@(x) x^3-x^2+2*x-3;x=fsolve(y,rand) %只能求给定初始值附近的一个零点 约束极值问题 二次规划 若某非线性规划的目标函数为自变量的二次函数，约束条件又全是线性的，就称其为二次规划。 例子： 12345h=[4,-4;-4,8];%实对称矩阵f=[-6;-3];a=[1,1;4,1];b=[3;9];[x,value]=quadprog(h,f,a,b,[],[],zeros(2,1)) 求得,,。 罚函数法 序列无约束最小化技术：将非线性规划问题转化为无约束极值问题。 利用问题中的约束函数作出适当的罚函数，由此构造出带参数的增广目标函数，把问题转化为无约束非线性规划问题。 对如上问题，取一个充分大的数，构造函数 例子 Matlab程序 1234567function g=test3(x);M=50000;f=x(1)^2+x(2)^2+8;g=f-M*min(min(x),0)-M*min(x(1)^2-x(2),0)+M*(-x(1)-x(2)^2+2)^2;%求增广目标函数的极小值[x,y]=fminsearch('test3',rand(2,1)) 飞行管理问题 参考博客【数学建模学习④】飞行管理问题 参考博客 ··············································································· matlab——整数规划 数学建模—整数规划（笔记） 蒙特卡洛算法的MATLAB实现 第三章：非线性规划 【数学建模学习④】飞行管理问题","link":"/2022/01/10/jm1/"},{"title":"笔记|统计学习方法：感知机模型","text":"12345678(由于我的markdown文件是使用VSC写的，在此顺便记录一下markdown语法)Tips：需要在Git中使用 npm install hexo-math --save 命令来安装数学公式环境VSC中预览：先按住ctrl+k,松开后按vctrl B 粗体ctrl l 斜体ctrl shift ] == #ctrl m 标记数学公式环境** 感知机（perception）是一个二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1与-1二值 1.感知机模型 假设输入空间（特征空间）是 ，输出空间是。输入 表示实例的特征向量，对应输出空间（特征空间）的点；输出 表示实例的类别,由输入空间到输出空间的如下函数 称为感知机。其中,和b为感知机模型参数, 叫作权值（weight）或权值向量（weight vector）,叫作偏置（bias）,表示和x的内积。sign是符号函数，即： 感知机是一种线性分类模型，属于判别模型。感知机的假设空间的定义是在特征空间中的所有线性分类模型或者线性分类器，即函数集合。 2.感知机的几何解释 线性方程 对应特征空间中的一个超平面，是超平面的法向量，b是超平面的截距，则这条线将超平面分离成正负两类。 感知机学习，由训练数据集 其中， ,,就得到了感知机模型，即求得模型参数，b。 3.感知机的学习策略 数据集的线性可分 对于给定的数据集 其中， , 能将数据集的正负实例点完全正确地划分到超平面的两侧。可如此划分则称为线性可分数据集，否则称为数据集不可分。 4.损失函数 由点到平面距离公式可得,一个错误分类的点到超平面的距离为： 对于分类错误的点，一定有： 则所有错误分类的点到超平面的总距离： 则感知机的损失函数为： 所有样本都分类正确时，损失函数为0，错误越少，损失函数越小，分类错误样本离超平面距离越近，则损失函数越小。因此感知机的学习目标就是最小化该损失函数。 5.梯度下降法 通俗的讲话，梯度就是导数和偏导数，梯度下降法的思想是：梯度方向是目标函数值下降最快的方向，因此沿着梯度下降的方向优化能最快寻找到目标函数的极小值。 参数、b的更新可以表示为： 在感知机中采用随机梯度下降法，即每次随机选择一个分类错误的样本计算，进行和b的更新，即： 其过程直观理解为：当一个样本被当前超平面划分到分类错误一类时，利用此样本调整超平面的参数，使超平面向靠近该样本的方向移动，则该样本距离超平面的距离减小，从而降低损失函数，直到超平面移动至使该样本被正确划分为止。 6.例子 7.python代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import numpy as np;import pandas as pd;import matplotlib.pyplot as plt; df=pd.read_excel(\"D:\\pythondata\\perceptrondata.xls\");#读取原题数据df=pd.DataFrame(df);x=df.values[:,:-1];#x代表所有行的第一列到倒数第二列的数据，即分类实例的所有特征向量y=df.values[:,-1];#y代表df的倒数第一列数据，即分类实例的所有类别a=x.shape;n=a[0];#n代表x特征向量的行数m=a[1];#m代表x特征向量的列数w=[];#创建空列表，w代表分离超平面中的法向量for i in range(m): w.append(0);#列表的长度与特征向量的特征个数相同#若直接命令w[0]=0,w[1]=0时，会报错，直接按照索引向列表内添加东西时，因为空的列表不能直接指定其位置。b=0;#b代表分离超平面的截距k=1;#用来判别是否找到最优的超平面。假设值为1，即未找到while (k==1): k=0; for i in range(n): t=np.dot(w,x[i]);#用来计算w*x if (y[i]*(t+b)&lt;=0): w=w+np.dot(y[i],x[i]); b=b+y[i]; k=1;print(\"分离超平面的法向量w={0},截距b={1}\".format(w,b));########可视化结果#def plot_and_scatter(df=None,w=0,b=0): xmin=df.values[:,:-1].min(); xmax=df.values[:,:-1].max(); xdiff=(xmax-xmin)*0.5; xx=np.linspace((xmin-xdiff),(xmax+xdiff),100); yy=-b-w[1]*xx; plt.figure(); plt.xlabel(\"X(1)\"); plt.ylabel(\"X(2)\");#设置坐标轴的文字标签 ax=plt.gca();# get current axis 获得坐标轴对象 ax.spines[\"right\"].set_color(\"none\"); ax.spines[\"top\"].set_color(\"none\"); # 将右边 上边的两条边颜色设置为空 其实就相当于抹掉这两条边 ax.xaxis.set_ticks_position(\"bottom\"); ax.yaxis.set_ticks_position(\"left\"); ax.spines[\"bottom\"].set_position((\"data\",0)); ax.spines[\"left\"].set_position((\"data\",0));#指定 data设置的bottom(也就是指定的x轴)绑定到y轴的0这个点上 plt.plot(xx,yy,\"r\"); color_list=[\"blue\",\"green\",\"black\",\"pink\",\"orange\"]; y=df.values[:,-1]; a=set(y); a=list(a); y_num=len(a); t=0; for j in range(y_num): tt=a[j]; y_index=[i for i,y in enumerate(y) if y==tt]; x_group1=df.values[y_index,0]; x_group2=df.values[y_index,1]; plt.scatter(x_group1,x_group2); t=t+1; plot_and_scatter(df,w,b);plt.show();","link":"/2022/01/05/mathrecord1/"},{"title":"入门网站归纳","text":"入门各类编程语言的网站 -菜鸟教程","link":"/2022/01/07/cainiao/"},{"title":"笔记|统计学习方法：k近邻算法","text":"k近邻算法(k-NN)是一种基本分类与回归方法，它有三个基本要素，本文将介绍k近邻算法的模型与kd树。 k近邻算法 给定一个训练数据集，对于新输入的实例，在训练数据集中找到与该实例最临近的k个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类。 数学实现 输入:训练数据集 其中， ,为实例的类别,;实例特征向量 输出：实例x所属的类y。 1.根据给定的距离度量，在训练集中找出与最近邻的个点，涵盖这个点的的领域记作。 2.在中根据分类决策规则(如多数表决)决定的类别。 式中为指示函数，即当时为，否则为0. 在时称为最近邻算法，k近邻法没有显式的学习过程。 模型建立 模型三要素为：距离度量，的大小和分类规则。 距离度量 闵可夫斯基距离(Minkowski Distance) 其中。 当时，是欧式距离。 当时，是曼哈顿距离。 的选择 的选择会对结果产生重大影响。 的值过小，极端情况下，测试实例只和最接近的一个样本有关，训练误差很小，但是如果这个样本恰好是噪点，预测就会出错，即产生了过拟合。 如果值过大，极端情况，则会产生欠拟合。 姑通常采用交叉验证法来选取合适的。 分类规则 近邻的分类决策通常是多数表决：由测试样本的个临近样本的多数类决定测试样本的类别。有如下规则： 给定测试样本，其最临近的个训练示例构成的集合，分类损失函数为型损失，如果涵盖区域的类别为，则分类误差率为： 要使得分类误差率最小，就是要使最大，所以多数表决规则等价于误分类绿最小。 实现：树 树算法有三步： 构造树 搜索近邻 预测 树的构建 选取为坐标轴，以训练集中的所有数据坐标中的中位数作为切分点，将超矩形区域切割成两个子区域。将该切分点作为根结点，由根结点生出深度为1的左右子结点，左节点对应坐标小于切分点，右结点对应坐标大于切分点。 对深度为的结点，选择为切分坐标轴，，以该结点区域中训练数据坐标的中位数作为切分点，将区域分为两个子区域，且生成深度为的左、右子结点。左节点对应坐标小于切分点，右结点对应坐标大于切分点 重复2，直到两个子区域没有数据时停止。 实例参考KNN算法和kd树详解（例子+图示） 的搜索 输入：已构造的树，目标点 输出：的最近邻 在kd树中找出包含目标点的叶结点；从根结点，递归地向下访问kd树，若目标点当前纬的坐标小于分切点，则移动到左子结点，直到子结点为叶子结点为止。 此叶子结点为\"当前最近点\"。 递归地向上回退，在每个结点都进行如下操作： 123456(a)如果该结点保存的实例点比当前最近点距离目标点更近，则以该实例点为“当前最近点”(b)当前最近点一定存在于该结点一个子结点对应的区域，检查该子结点的父结点的另一个子结点对应的区域是否有更近的点。具体的，检查另一子结点对应的区域是否与以目标点为球心，以目标点与“当前最近点”间的距离为半径的超球体相交。(c)如果相交，可能在另一个子结点对应的区域内存在距目标点更近的点，移动到另一个子结点。接着，递归地进行最近邻搜索；如果不相交，则向上退回。 当退回到根结点时，搜索结束。最后一个“当前最近点”即为的最近邻点。 实例参考KNN算法和kd树详解（例子+图示） Python代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import numpy as npclass Node: def __init__(self, data, lchild = None, rchild = None): self.data = data self.lchild = lchild self.rchild = rchildclass KdTree: def __init__(self): self.kdTree = None def create(self, dataSet, depth): #创建kd树，返回根结点 if (len(dataSet) &gt; 0): m, n = np.shape(dataSet) #求出样本行，列 midIndex = int(m / 2) #中间数的索引位置 axis = depth % n #判断以哪个轴划分数据 sortedDataSet = self.sort(dataSet, axis) #进行排序 node = Node(sortedDataSet[midIndex]) #将节点数据域设置为中位数，具体参考下书本 # print sortedDataSet[midIndex] leftDataSet = sortedDataSet[: midIndex] #将中位数的左边创建2改副本 rightDataSet = sortedDataSet[midIndex+1 :] print(leftDataSet) print(rightDataSet) node.lchild = self.create(leftDataSet, depth+1) #将中位数左边样本传入来递归创建树 node.rchild = self.create(rightDataSet, depth+1) return node else: return None def sort(self, dataSet, axis): #采用冒泡排序，利用aixs作为轴进行划分 sortDataSet = dataSet[:] #由于不能破坏原样本，此处建立一个副本 m, n = np.shape(sortDataSet) for i in range(m): for j in range(0, m - i - 1): if (sortDataSet[j][axis] &gt; sortDataSet[j+1][axis]): temp = sortDataSet[j] sortDataSet[j] = sortDataSet[j+1] sortDataSet[j+1] = temp print(sortDataSet) return sortDataSet def preOrder(self, node): if node != None: print(\"tttt-&gt;%s\" % node.data) self.preOrder(node.lchild) self.preOrder(node.rchild) # def search(self, tree, x): # node = tree # depth = 0 # while (node != None): # print node.data # n = len(x) #特征数 # axis = depth % n # if x[axis] &lt; node.data[axis]: # node = node.lchild # else: # node = node.rchild # depth += 1 def search(self, tree, x): self.nearestPoint = None #保存最近的点 self.nearestValue = 0 #保存最近的值 def travel(node, depth = 0): #递归搜索 if node != None: #递归终止条件 n = len(x) #特征数 axis = depth % n #计算轴 if x[axis] &lt; node.data[axis]: #如果数据小于结点，则往左结点找 travel(node.lchild, depth+1) else: travel(node.rchild, depth+1) #以下是递归完毕后，往父结点方向回朔 distNodeAndX = self.dist(x, node.data) #目标和节点的距离判断 if (self.nearestPoint == None): #确定当前点，更新最近的点和最近的值 self.nearestPoint = node.data self.nearestValue = distNodeAndX elif (self.nearestValue &gt; distNodeAndX): self.nearestPoint = node.data self.nearestValue = distNodeAndX print(node.data, depth, self.nearestValue, node.data[axis], x[axis]) if (abs(x[axis] - node.data[axis]) &lt;= self.nearestValue): #确定是否需要去子节点的区域去找（圆的判断） if x[axis] &lt; node.data[axis]: travel(node.rchild, depth+1) else: travel(node.lchild, depth + 1) travel(tree) return self.nearestPoint def dist(self, x1, x2): #欧式距离的计算 return ((np.array(x1) - np.array(x2)) ** 2).sum() ** 0.5##运行示例：#初始值设定dataSet = [[2, 3], [5, 4], [9, 6], [4, 7], [8, 1], [7, 2]]x = [5, 3]#调用函数kdtree = KdTree()tree = kdtree.create(dataSet, 0)kdtree.preOrder(tree)#输出结果print(kdtree.search(tree, x))","link":"/2022/01/06/mathrecord2/"},{"title":"笔记|统计学习方法：朴素贝叶斯","text":"联合概率 朴素贝叶斯是生成模型，由训练数据学习联合分布概率,求得后验概率为:。联合概率分布为： 概率估计方法是极大似然法，或者贝叶斯估计。 基本假设：条件独立性 确定x的类别 1. 计算先验概率以及条件概率 ， 2. 对于给定的实例,计算 3. 确定实例的类 确定x的类的例子： 利用贝叶斯定理与联合概率进行分类预测 贝叶斯定理： 将输入x分到后验概率最大的类y 后验概率最大等价于0-1损失函数时的期望风险最小化 贝叶斯估计 条件概率： 的取值有个 称作极大似然估计 称作拉普拉斯平滑 先验概率： 贝叶斯估计例子","link":"/2022/01/07/mathrecord3/"},{"title":"笔记|统计学习方法：决策树(一)","text":"决策树的基本概念 决策树就是一棵树。 叶结点对应于决策结果，其他每个结点则对应于一个属性测试； 每个结点包含的样本集合根据属性测试的结果被划分到子结点中； 根结点包含样本全集，从根结点到每个叶子结点的路径对应了一个判定测试序列。 示例： 决策树学习的关键在于如何选择最优的划分属性，所谓的最优划分属性，对于二元分类而言，就是尽量使划分的样本属于同一类别，即“纯度”最高的属性。那么如何来度量特征（features）的纯度，这时候就要用到“经验熵（information entropy）”。 经验熵 先来看看信息熵的定义：假如当前样本集D中第k类样本所占的比例为为类别的总数（对于二元分类来说,i=2）。则样本集的信息熵为： 的值越小，则D的纯度越高。 信息增益算法 输入：训练数据集和特征； 输出：特征对训练数据的信息增益 信息增益表示了得知特征X的信息而使得类Y的信息的不确定性性减少的程度。 经验条件熵 计算特征对数据集的经验条件熵 的意思是在某一特征下的特征样本中满足训练目标的个数 指的是该特征的样本容量 信息增益为： 信息增益=经验熵-经验条件熵 一般而言，信息增益越大，则表示使用特征 对数据集划分所获得的“纯度提升”越大。所以信息增益可以用于决策树划分属性的选择，其实就是选择信息增益最大的属性，ID3算法就是采用的信息增益来划分属性。 信息增益比： 信息增益比为信息增益与训练数据集关于特征的值熵值之比： 其中： 是特征取值的个数。 ID3算法决策树生成 输入：数据集，特征集,阈值 输出：决策树 先对特征集求信息增益，选取最大的作为根结点的特征。 看特征集对训练数据集D的划分成几个子集（例如该特征集会划分 是或否具有该特征） 若某个子集只具有同一个类的的样本点，成为一个叶节点 否则，对子集从剩下的特征中选择新的特征，求得信息增益 如此递归操作，直到所有的特征分类，生成决策树。 Python代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108from math import logimport operatordef calcShannonent(dataSet): #计算数据的熵（entropy） numEntries = len(dataSet) #数据条数 labelCounts = {} for featVec in dataSet: currentLabel = featVec[-1] #每行数据的最后一个字（类别） if currentLabel not in labelCounts.keys(): labelCounts[currentLabel] = 0 labelCounts[currentLabel] += 1 #统计又多少个类以及每个类的数量 shannonEnt = 0 for key in labelCounts: prob = float(labelCounts[key])/numEntries #计算单个类的熵值 shannonEnt -= prob*log(prob, 2) #累加每个类的熵值 return shannonEntdef createDataSet_temp(): #创造实例数据 labels = ['头发', '声音'] with open('TreeGrowth_ID3.txt', 'r', encoding='UTF-8') as f: #改了一个文件读写 dataSet = [[] for i in range(9)] value = ['长', '短', '粗', '细', '男', '女'] num_line = 0 for line in f: # re.split(r'(\\s{8}\\[\\')|(\\', \\')|(\\'\\],)|(\\'\\])', line) #尝试使用正则划分，失败了 for i in line: if (i in value): dataSet[num_line].append(i) num_line += 1 del(dataSet[0]) ''' dataSet = [ ['长', '粗', '男'], ['短', '粗', '男'], ['短', '粗', '男'], ['长', '细', '女'], ['短', '细', '女'], ['短', '粗', '女'], ['长', '粗', '女'], ['长', '粗', '女'] ] ''' return dataSet, labelsdef splitDataSet(dataSet, axis, value): #按某个特征分类后的数据 retDataSet = [] for featVec in dataSet: if featVec[axis] == value: #axis表示指定属性在label中的标号，value是该分类中属性的目标值 reducedFeatVec = featVec[:axis] #对于每一条记录，在分完类后，都要把之前使用过的属性删除 reducedFeatVec.extend(featVec[axis+1:]) retDataSet.append(reducedFeatVec) return retDataSetdef chooseBestFeatureToSplit(dataSet): #选择最优的分类特征 numFeatures = len(dataSet[0])-1 #特征的个数 baseEntropy = calcShannonent(dataSet) #原始熵 bestInfoGain = 0 bestFeature = -1 for i in range(numFeatures): #循环每一个特征 featList = [example[i] for example in dataSet]#读取每一条记录取出其中第i个属性的值，并新建一个列表 uniqueVals = set(featList) #set()创建一个无序不重复元素集，可进行关系测试，删除重复元素，进行交差并集运算 newEntropy = 0 for value in uniqueVals: #对于第i个属性值列表中每一个值 subDataSet = splitDataSet(dataSet, i, value) #subDataSet是去掉了第i个属性值是value的列表 prob = len(subDataSet)/float(len(dataSet)) newEntropy += prob*calcShannonent(subDataSet) #按特征分类后的熵 infoGain = baseEntropy - newEntropy #计算信息增益 if(infoGain &gt; bestInfoGain): #若按某特征划分后，熵值减少的最大，则次特征为最优分类特征 bestInfoGain = infoGain bestFeature = i return bestFeaturedef majorityCnt(classList): #多数表决排序，如：最后分类为2男1女则判断为男 classCount = {} for vote in classList: if vote not in classCount.keys(): classCount[vote] = 0 classCount[vote] += 1 sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True) #将items返回的可遍历键值对数组，根据键值对的第二个域(值)，进行降序排列 return sortedClassCount[0][0] #返回数量最大的类名def createTree(dataSet,labels): classList = [example[-1] for example in dataSet]#类别：男或女 对于记录集中的每一条都取其最后一个值形成列表 if classList.count(classList[0]==len(classList)):#~~~~~~~~~~~~~~~~~~~当该记录集只有一个特征时 return classList[0] if len(dataSet[0])==1: #当最后一个属性分类完成，记录的长为一 return majorityCnt((classList)) #直接返回数量最多的值 bestFeat = chooseBestFeatureToSplit(dataSet) #找到最优特征的标号 if(bestFeat == -1): #自己添加的，不然报错 return classList[0] bestFeatLabel = labels[bestFeat] #找到最优特征 myTree = {bestFeatLabel:{}} #分类结果以字典形式保存，得到树的当前层 del(labels[bestFeat]) #删除labels中当前最优特征 featValues = [example[bestFeat] for example in dataSet]#将记录集中当前最优特征的值形成列表 uniqueVals = set(featValues) #对值集去重 for value in uniqueVals: #对于每个值 subLabels = labels[:] #subLabels为去除当前最优特征的特征集 myTree[bestFeatLabel][value]=createTree(splitDataSet(dataSet,bestFeat,value),subLabels) #使用递归，创造下一层树，~~~~~~~~~~~~~~~~~~~~~~ return myTreeif __name__=='__main__': dataSet, labels = createDataSet_temp() #创造示例数据 print(createTree(dataSet, labels)) #输出决策树模型结果 参考博客： - 决策树（decision tree）(一)——构造决策树方法 - 决策树原理实例（python代码实现）","link":"/2022/01/07/mathrecord4/"},{"title":"笔记|统计学习方法：决策树(二)","text":"决策树的剪枝 由于决策树的生成算法是递归实现的，所以对已知数据的分类十分准确，但对未知数据的预测就不那么准确，就产生了过拟合的现象。 所以就产生了一种将已生成的树进行简化的过程，称为：“剪枝”。 剪枝算法定义 决策树的剪枝往往通过极小化决策树整体的损失函数来实现。 设 - 树的叶子结点个数为 - 是树的叶子结点 - 该叶子结点有个样本点，其中类的样本点有个 - 为叶子结点上的经验熵 - 为参数 则损失函数定义为： 其中经验熵为： 将记作,代入： 这时有： 式中： - 表示模型对训练数据的预测误差（模型与训练数据的拟合程度） - 表示模型复杂度 - 参数控制着两者之间的联系，参数较大的促使选择较为简单的模型，参数较小则选择复杂的模型，只考虑训练数据的拟合程度，不考虑模型复杂度。 ##### 剪枝 剪枝就是当确定时，选择损失函数最小的模型，即损失函数最小的子树。 决策树的生成只考虑了通过提高信息增益对训练数据的更好拟合，而剪枝通过优化损失函数还考虑减小模型复杂度。 树剪枝算法使用 输入：生成算法产生的整个树，参数； 输出：修剪后的子树 计算每个节点的经验熵 递归得从树的叶子结点向上回缩 设一组叶节点回缩到其父结点之前与之后的整体树分别为与，其对应的损失函数为与 。如果 返回第二步，直到不能继续位置，得到损失函数最小的子树。 Python代码实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import mathimport numpy as np # 创建数据集 备注 李航《统计学习方法》中表5.1 贷款申请数据数据def createDataLH(): data = np.array([['青年', '否', '否', '一般']]) data = np.append(data, [['青年', '否', '否', '好']], axis = 0) data = np.append(data, [['青年', '是', '否', '好'] , ['青年', '是', '是', '一般'] , ['青年', '否', '否', '一般'] , ['中年', '否', '否', '一般'] , ['中年', '否', '否', '好'] , ['中年', '是', '是', '好'] , ['中年', '否', '是', '非常好'] , ['中年', '否', '是', '非常好'] , ['老年', '否', '是', '非常好'] , ['老年', '否', '是', '好'] , ['老年', '是', '否', '好'] , ['老年', '是', '否', '非常好'] , ['老年', '否', '否', '一般'] ], axis = 0) label = np.array(['否', '否', '是', '是', '否', '否', '否', '是', '是', '是', '是', '是', '是', '是', '否']) name = np.array(['年龄', '有工作', '有房子', '信贷情况']) return data, label, name# 创建西瓜书数据集2.0def createDataXG20(): data = np.array([['青绿', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'] , ['乌黑', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑'] , ['乌黑', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'] , ['青绿', '蜷缩', '沉闷', '清晰', '凹陷', '硬滑'] , ['浅白', '蜷缩', '浊响', '清晰', '凹陷', '硬滑'] , ['青绿', '稍蜷', '浊响', '清晰', '稍凹', '软粘'] , ['乌黑', '稍蜷', '浊响', '稍糊', '稍凹', '软粘'] , ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '硬滑'] , ['乌黑', '稍蜷', '沉闷', '稍糊', '稍凹', '硬滑'] , ['青绿', '硬挺', '清脆', '清晰', '平坦', '软粘'] , ['浅白', '硬挺', '清脆', '模糊', '平坦', '硬滑'] , ['浅白', '蜷缩', '浊响', '模糊', '平坦', '软粘'] , ['青绿', '稍蜷', '浊响', '稍糊', '凹陷', '硬滑'] , ['浅白', '稍蜷', '沉闷', '稍糊', '凹陷', '硬滑'] , ['乌黑', '稍蜷', '浊响', '清晰', '稍凹', '软粘'] , ['浅白', '蜷缩', '浊响', '模糊', '平坦', '硬滑'] , ['青绿', '蜷缩', '沉闷', '稍糊', '稍凹', '硬滑']]) label = np.array(['是', '是', '是', '是', '是', '是', '是', '是', '否', '否', '否', '否', '否', '否', '否', '否', '否']) name = np.array(['色泽', '根蒂', '敲声', '纹理', '脐部', '触感']) return data, label, namedef splitXgData20(xgData, xgLabel): xgDataTrain = xgData[[0, 1, 2, 5, 6, 9, 13, 14, 15, 16],:] xgDataTest = xgData[[3, 4, 7, 8, 10, 11, 12],:] xgLabelTrain = xgLabel[[0, 1, 2, 5, 6, 9, 13, 14, 15, 16]] xgLabelTest = xgLabel[[3, 4, 7, 8, 10, 11, 12]] return xgDataTrain, xgLabelTrain, xgDataTest, xgLabelTest# 定义一个常用函数 用来求numpy array中数值等于某值的元素数量equalNums = lambda x,y: 0 if x is None else x[x==y].size# 定义计算信息熵的函数def singleEntropy(x): \"\"\"计算一个输入序列的信息熵\"\"\" # 转换为 numpy 矩阵 x = np.asarray(x) # 取所有不同值 xValues = set(x) # 计算熵值 entropy = 0 for xValue in xValues: p = equalNums(x, xValue) / x.size entropy -= p * math.log(p, 2) return entropy # 定义计算条件信息熵的函数def conditionnalEntropy(feature, y): \"\"\"计算 某特征feature 条件下y的信息熵\"\"\" # 转换为numpy feature = np.asarray(feature) y = np.asarray(y) # 取特征的不同值 featureValues = set(feature) # 计算熵值 entropy = 0 for feat in featureValues: # 解释：feature == feat 是得到取feature中所有元素值等于feat的元素的索引（类似这样理解） # y[feature == feat] 是取y中 feature元素值等于feat的元素索引的 y的元素的子集 p = equalNums(feature, feat) / feature.size entropy += p * singleEntropy(y[feature == feat]) return entropy # 定义信息增益def infoGain(feature, y): return singleEntropy(y) - conditionnalEntropy(feature, y)# 定义信息增益率def infoGainRatio(feature, y): return 0 if singleEntropy(feature) == 0 else infoGain(feature, y) / singleEntropy(feature)# 使用李航数据测试函数 p62lhData, lhLabel, lhName = createDataLH()print(\"书中H(D)为0.971，函数结果：\" + str(round(singleEntropy(lhLabel), 3))) print(\"书中g(D, A1)为0.083，函数结果：\" + str(round(infoGain(lhData[:,0] ,lhLabel), 3))) print(\"书中g(D, A2)为0.324，函数结果：\" + str(round(infoGain(lhData[:,1] ,lhLabel), 3))) print(\"书中g(D, A3)为0.420，函数结果：\" + str(round(infoGain(lhData[:,2] ,lhLabel), 3))) print(\"书中g(D, A4)为0.363，函数结果：\" + str(round(infoGain(lhData[:,3] ,lhLabel), 3))) # 测试正常，与书中结果一致 运行结果： 12345书中H(D)为0.971，函数结果：0.971书中g(D, A1)为0.083，函数结果：0.083书中g(D, A2)为0.324，函数结果：0.324书中g(D, A3)为0.420，函数结果：0.42书中g(D, A4)为0.363，函数结果：0.363 预剪枝： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768# 创建预剪枝决策树def createTreePrePruning(dataTrain, labelTrain, dataTest, labelTest, names, method = 'id3'): \"\"\" 预剪枝 需要使用测试数据对每次的划分进行评估 策略说明：原本如果某节点划分前后的测试结果没有提升，根据奥卡姆剃刀原则将不进行划分（即执行剪枝），但考虑到这种策略容易造成欠拟合， 且不能排除后续划分有进一步提升的可能，因此，没有提升仍保留划分，即不剪枝 另外：周志华的书上评估的是某一个节点划分前后对该层所有数据综合评估，如评估对脐部 凹陷下色泽是否划分， 书上取的色泽划分前的精度是71.4%(5/7)，划分后的精度是57.1%(4/7)，都是脐部下三个特征（凹陷，稍凹，平坦）所有的数据的精度，计算也不易 而我觉得实际计算时，只对当前节点下的数据划分前后进行评估即可，如脐部凹陷时有三个测试样本， 三个样本色泽划分前的精度是2/3=66.7%，色泽划分后的精度是1/3=33.3%，因此判断不划分 \"\"\" trainData = np.asarray(dataTrain) labelTrain = np.asarray(labelTrain) testData = np.asarray(dataTest) labelTest = np.asarray(labelTest) names = np.asarray(names) # 如果结果为单一结果 if len(set(labelTrain)) == 1: return labelTrain[0] # 如果没有待分类特征 elif trainData.size == 0: return voteLabel(labelTrain) # 其他情况则选取特征 bestFeat, bestEnt = bestFeature(dataTrain, labelTrain, method = method) # 取特征名称 bestFeatName = names[bestFeat] # 从特征名称列表删除已取得特征名称 names = np.delete(names, [bestFeat]) # 根据最优特征进行分割 dataTrainSet, labelTrainSet = splitFeatureData(dataTrain, labelTrain, bestFeat) # 预剪枝评估 # 划分前的分类标签 labelTrainLabelPre = voteLabel(labelTrain) labelTrainRatioPre = equalNums(labelTrain, labelTrainLabelPre) / labelTrain.size # 划分后的精度计算 if dataTest is not None: dataTestSet, labelTestSet = splitFeatureData(dataTest, labelTest, bestFeat) # 划分前的测试标签正确比例 labelTestRatioPre = equalNums(labelTest, labelTrainLabelPre) / labelTest.size # 划分后 每个特征值的分类标签正确的数量 labelTrainEqNumPost = 0 for val in labelTrainSet.keys(): labelTrainEqNumPost += equalNums(labelTestSet.get(val), voteLabel(labelTrainSet.get(val))) + 0.0 # 划分后 正确的比例 labelTestRatioPost = labelTrainEqNumPost / labelTest.size # 如果没有评估数据 但划分前的精度等于最小值0.5 则继续划分 if dataTest is None and labelTrainRatioPre == 0.5: decisionTree = {bestFeatName: {}} for featValue in dataTrainSet.keys(): decisionTree[bestFeatName][featValue] = createTreePrePruning(dataTrainSet.get(featValue), labelTrainSet.get(featValue) , None, None, names, method) elif dataTest is None: return labelTrainLabelPre # 如果划分后的精度相比划分前的精度下降, 则直接作为叶子节点返回 elif labelTestRatioPost &lt; labelTestRatioPre: return labelTrainLabelPre else : # 根据选取的特征名称创建树节点 decisionTree = {bestFeatName: {}} # 对最优特征的每个特征值所分的数据子集进行计算 for featValue in dataTrainSet.keys(): decisionTree[bestFeatName][featValue] = createTreePrePruning(dataTrainSet.get(featValue), labelTrainSet.get(featValue) , dataTestSet.get(featValue), labelTestSet.get(featValue) , names, method) return decisionTree 预剪枝测试： 12345678910111213# 将西瓜数据2.0分割为测试集和训练集xgDataTrain, xgLabelTrain, xgDataTest, xgLabelTest = splitXgData20(xgData, xgLabel)# 生成不剪枝的树xgTreeTrain = createTree(xgDataTrain, xgLabelTrain, xgName, method = 'id3')# 生成预剪枝的树xgTreePrePruning = createTreePrePruning(xgDataTrain, xgLabelTrain, xgDataTest, xgLabelTest, xgName, method = 'id3')# 画剪枝前的树print(\"剪枝前的树\")createPlot(xgTreeTrain)# 画剪枝后的树print(\"剪枝后的树\")createPlot(xgTreePrePruning) 后剪枝 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105# 创建决策树 带预划分标签def createTreeWithLabel(data, labels, names, method = 'id3'): data = np.asarray(data) labels = np.asarray(labels) names = np.asarray(names) # 如果不划分的标签为 votedLabel = voteLabel(labels) # 如果结果为单一结果 if len(set(labels)) == 1: return votedLabel # 如果没有待分类特征 elif data.size == 0: return votedLabel # 其他情况则选取特征 bestFeat, bestEnt = bestFeature(data, labels, method = method) # 取特征名称 bestFeatName = names[bestFeat] # 从特征名称列表删除已取得特征名称 names = np.delete(names, [bestFeat]) # 根据选取的特征名称创建树节点 划分前的标签votedPreDivisionLabel=_vpdl decisionTree = {bestFeatName: {\"_vpdl\": votedLabel}} # 根据最优特征进行分割 dataSet, labelSet = splitFeatureData(data, labels, bestFeat) # 对最优特征的每个特征值所分的数据子集进行计算 for featValue in dataSet.keys(): decisionTree[bestFeatName][featValue] = createTreeWithLabel(dataSet.get(featValue), labelSet.get(featValue), names, method) return decisionTree # 将带预划分标签的tree转化为常规的tree# 函数中进行的copy操作，原因见有道笔记 【YL20190621】关于Python中字典存储修改的思考def convertTree(labeledTree): labeledTreeNew = labeledTree.copy() nodeName = list(labeledTree.keys())[0] labeledTreeNew[nodeName] = labeledTree[nodeName].copy() for val in list(labeledTree[nodeName].keys()): if val == \"_vpdl\": labeledTreeNew[nodeName].pop(val) elif type(labeledTree[nodeName][val]) == dict: labeledTreeNew[nodeName][val] = convertTree(labeledTree[nodeName][val]) return labeledTreeNew# 后剪枝 训练完成后决策节点进行替换评估 这里可以直接对xgTreeTrain进行操作def treePostPruning(labeledTree, dataTest, labelTest, names): newTree = labeledTree.copy() dataTest = np.asarray(dataTest) labelTest = np.asarray(labelTest) names = np.asarray(names) # 取决策节点的名称 即特征的名称 featName = list(labeledTree.keys())[0] # print(\"\\n当前节点：\" + featName) # 取特征的列 featCol = np.argwhere(names==featName)[0][0] names = np.delete(names, [featCol]) # print(\"当前节点划分的数据维度：\" + str(names)) # print(\"当前节点划分的数据：\" ) # print(dataTest) # print(labelTest) # 该特征下所有值的字典 newTree[featName] = labeledTree[featName].copy() featValueDict = newTree[featName] featPreLabel = featValueDict.pop(\"_vpdl\") # print(\"当前节点预划分标签：\" + featPreLabel) # 是否为子树的标记 subTreeFlag = 0 # 分割测试数据 如果有数据 则进行测试或递归调用 np的array我不知道怎么判断是否None, 用is None是错的 dataFlag = 1 if sum(dataTest.shape) &gt; 0 else 0 if dataFlag == 1: # print(\"当前节点有划分数据！\") dataTestSet, labelTestSet = splitFeatureData(dataTest, labelTest, featCol) for featValue in featValueDict.keys(): # print(\"当前节点属性 {0} 的子节点：{1}\".format(featValue ,str(featValueDict[featValue]))) if dataFlag == 1 and type(featValueDict[featValue]) == dict: subTreeFlag = 1 # 如果是子树则递归 newTree[featName][featValue] = treePostPruning(featValueDict[featValue], dataTestSet.get(featValue), labelTestSet.get(featValue), names) # 如果递归后为叶子 则后续进行评估 if type(featValueDict[featValue]) != dict: subTreeFlag = 0 # 如果没有数据 则转换子树 if dataFlag == 0 and type(featValueDict[featValue]) == dict: subTreeFlag = 1 # print(\"当前节点无划分数据！直接转换树：\"+str(featValueDict[featValue])) newTree[featName][featValue] = convertTree(featValueDict[featValue]) # print(\"转换结果：\" + str(convertTree(featValueDict[featValue]))) # 如果全为叶子节点， 评估需要划分前的标签，这里思考两种方法， # 一是，不改变原来的训练函数，评估时使用训练数据对划分前的节点标签重新打标 # 二是，改进训练函数，在训练的同时为每个节点增加划分前的标签，这样可以保证评估时只使用测试数据，避免再次使用大量的训练数据 # 这里考虑第二种方法 写新的函数 createTreeWithLabel，当然也可以修改createTree来添加参数实现 if subTreeFlag == 0: ratioPreDivision = equalNums(labelTest, featPreLabel) / labelTest.size equalNum = 0 for val in labelTestSet.keys(): equalNum += equalNums(labelTestSet[val], featValueDict[val]) ratioAfterDivision = equalNum / labelTest.size # print(\"当前节点预划分标签的准确率：\" + str(ratioPreDivision)) # print(\"当前节点划分后的准确率：\" + str(ratioAfterDivision)) # 如果划分后的测试数据准确率低于划分前的，则划分无效，进行剪枝，即使节点等于预划分标签 # 注意这里取的是小于，如果有需要 也可以取 小于等于 if ratioAfterDivision &lt; ratioPreDivision: newTree = featPreLabel return newTree 测试： 12345678910111213141516# 书中的树结构 p81 p83xgTreeBeforePostPruning = {\"脐部\": {\"_vpdl\": \"是\" , '凹陷': {'色泽':{\"_vpdl\": \"是\", '青绿': '是', '乌黑': '是', '浅白': '否'}} , '稍凹': {'根蒂':{\"_vpdl\": \"是\" , '稍蜷': {'色泽': {\"_vpdl\": \"是\" , '青绿': '是' , '乌黑': {'纹理': {\"_vpdl\": \"是\" , '稍糊': '是', '清晰': '否', '模糊': '是'}} , '浅白': '是'}} , '蜷缩': '否' , '硬挺': '是'}} , '平坦': '否'}}xgTreePostPruning = treePostPruning(xgTreeBeforePostPruning, xgDataTest, xgLabelTest, xgName)createPlot(convertTree(xgTreeBeforePostPruning))createPlot(xgTreePostPruning) 代码参考博客： 决策树python源码实现（含预剪枝和后剪枝）","link":"/2022/01/09/mathrecord5/"},{"title":"笔记|统计学习方法：CART算法","text":"待写","link":"/2022/01/09/mathrecord6/"},{"title":"有关抛射角度对铅球射程的影响的研究","text":"假设——以水平面为参考系 假设一定高度的人以一定的角度抛出一铅球求该铅球的最远射程 设运动员出手高度为h 出手角度为 出手速度为v 铅球达到最高点时间 从最高点下落到水平面的时间为 总时间 水平方向路程 忽略空气阻力 不考虑展臂动作带来的误差 求解过程 1.将初速度分解： 2.铅球从开始抛出到最高点时间： 3.铅球最高点处到抛出位置的垂直高度： 4.铅球从最高点落到水平面的时间： 5.铅球水平方向经过的路程： 联立以上方程求得与出手速度、出手角度、出手高度的函数关系式： 此时的情况是对出手高度没有要求。 给定出手高度，对于不同的出手速度，确定最佳的角度 对求导： 使得，则 化简： 姑在给定高度，对于不同速度，最佳角度 结论 铅球抛出点与落地点等高时，最佳抛射角度为。，不等高时小于。，具体的最佳抛射角度与抛出点和落地点的高度差以及抛出时的速度有关。理论值大约在。~。. matlab实现","link":"/2022/01/06/wzhppydshecheng/"},{"title":"jm2","text":"","link":"/2022/01/11/jm2/"}],"tags":[{"name":"java","slug":"java","link":"/tags/java/"},{"name":"数学建模","slug":"数学建模","link":"/tags/%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1/"},{"name":"math","slug":"math","link":"/tags/math/"}],"categories":[{"name":"java","slug":"java","link":"/categories/java/"}]}